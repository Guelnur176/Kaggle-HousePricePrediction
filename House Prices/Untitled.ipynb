{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n",
      "(1459, 80)\n",
      "(1445, 66)\n",
      "(1459, 65)\n",
      "Id                 int64\n",
      "MSSubClass        object\n",
      "LotFrontage      float64\n",
      "LotArea            int64\n",
      "Street            object\n",
      "Alley             object\n",
      "LotShape          object\n",
      "LandContour       object\n",
      "LotConfig         object\n",
      "LandSlope         object\n",
      "Neighborhood      object\n",
      "Condition1        object\n",
      "BldgType          object\n",
      "HouseStyle        object\n",
      "OverallQual        int64\n",
      "OverallCond       object\n",
      "YearBuilt          int64\n",
      "YearRemodAdd       int64\n",
      "RoofStyle         object\n",
      "MasVnrType        object\n",
      "ExterQual         object\n",
      "ExterCond         object\n",
      "Foundation        object\n",
      "BsmtQual          object\n",
      "BsmtCond          object\n",
      "BsmtExposure      object\n",
      "BsmtFinType1      object\n",
      "BsmtFinSF1       float64\n",
      "BsmtFinType2      object\n",
      "BsmtFinSF2       float64\n",
      "                  ...   \n",
      "2ndFlrSF           int64\n",
      "LowQualFinSF       int64\n",
      "GrLivArea          int64\n",
      "BsmtFullBath       int64\n",
      "BsmtHalfBath       int64\n",
      "FullBath           int64\n",
      "HalfBath           int64\n",
      "BedroomAbvGr       int64\n",
      "KitchenAbvGr      object\n",
      "KitchenQual       object\n",
      "TotRmsAbvGrd       int64\n",
      "Fireplaces         int64\n",
      "FireplaceQu       object\n",
      "GarageType        object\n",
      "GarageFinish      object\n",
      "GarageCars         int64\n",
      "GarageArea         int64\n",
      "GarageCond        object\n",
      "PavedDrive        object\n",
      "WoodDeckSF         int64\n",
      "OpenPorchSF        int64\n",
      "EnclosedPorch      int64\n",
      "3SsnPorch          int64\n",
      "ScreenPorch        int64\n",
      "PoolArea           int64\n",
      "Fence             object\n",
      "MoSold            object\n",
      "YrSold            object\n",
      "SaleCondition     object\n",
      "SalePrice          int64\n",
      "dtype: object\n",
      "Null values treino \n",
      " Index([], dtype='object')\n",
      "Null values test \n",
      " Index([], dtype='object')\n",
      "New shape train: (1445, 249)\n",
      "Indice da coluna SalePrice no novo dataset 235\n",
      "New shape test: (1459, 248)\n",
      "Colunas que existem apenas teste :  Index([], dtype='object')\n",
      "Colunas que existem apenas treino :  Index(['SalePrice'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def remove_categorical_columns(df):\n",
    "    df.drop('MSZoning',axis=1,inplace=True)\n",
    "    df.drop('Street',axis=1,inplace=True)\n",
    "    df.drop('Alley',axis=1,inplace=True)\n",
    "    df.drop('LotShape',axis=1,inplace=True)\n",
    "    df.drop('LandContour',axis=1,inplace=True)\n",
    "    df.drop('Utilities',axis=1,inplace=True)\n",
    "    df.drop('LotConfig',axis=1,inplace=True)\n",
    "    df.drop('LandSlope',axis=1,inplace=True)\n",
    "    df.drop('Neighborhood',axis=1,inplace=True)\n",
    "    df.drop('Condition1',axis=1,inplace=True)\n",
    "    df.drop('Condition2',axis=1,inplace=True)\n",
    "    df.drop('BldgType',axis=1,inplace=True)\n",
    "    df.drop('HouseStyle',axis=1,inplace=True)\n",
    "    df.drop('RoofStyle',axis=1,inplace=True)\n",
    "    df.drop('RoofMatl',axis=1,inplace=True)\n",
    "    df.drop('Exterior1st',axis=1,inplace=True)\n",
    "    df.drop('Exterior2nd',axis=1,inplace=True)\n",
    "    df.drop('MasVnrType',axis=1,inplace=True)\n",
    "    df.drop('ExterQual',axis=1,inplace=True)\n",
    "    df.drop('ExterCond',axis=1,inplace=True)\n",
    "    df.drop('Foundation',axis=1,inplace=True)\n",
    "    df.drop('BsmtQual',axis=1,inplace=True)\n",
    "    df.drop('BsmtCond',axis=1,inplace=True)\n",
    "    df.drop('BsmtExposure',axis=1,inplace=True)\n",
    "    df.drop('BsmtFinType1',axis=1,inplace=True)\n",
    "    df.drop('BsmtFinType2',axis=1,inplace=True)\n",
    "    df.drop('Heating',axis=1,inplace=True)\n",
    "    df.drop('HeatingQC',axis=1,inplace=True)\n",
    "    df.drop('CentralAir',axis=1,inplace=True)\n",
    "    df.drop('Electrical',axis=1,inplace=True)\n",
    "    df.drop('KitchenQual',axis=1,inplace=True)\n",
    "    df.drop('Functional',axis=1,inplace=True)\n",
    "    df.drop('FireplaceQu',axis=1,inplace=True)\n",
    "    df.drop('GarageType',axis=1,inplace=True)\n",
    "    df.drop('GarageFinish',axis=1,inplace=True)\n",
    "    df.drop('GarageQual',axis=1,inplace=True)\n",
    "    df.drop('GarageCond',axis=1,inplace=True)\n",
    "    df.drop('PavedDrive',axis=1,inplace=True)\n",
    "    df.drop('PoolQC',axis=1,inplace=True)\n",
    "    df.drop('Fence',axis=1,inplace=True)\n",
    "    df.drop('MiscFeature',axis=1,inplace=True)\n",
    "    df.drop('SaleType',axis=1,inplace=True)\n",
    "    df.drop('SaleCondition',axis=1,inplace=True)\n",
    "\n",
    "def input_missing_value(df):\n",
    "    \n",
    "    # MSSubClass as str\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype(\"str\")\n",
    "    #After converting this column to String, it will be handled as categorical\n",
    "    #There is one value in the test set that there isn't in the training. It is 150\n",
    "    #It will be result in one column for this categorical value that doesn't exist in the training set\n",
    "    #It can't happen\n",
    "    #There is only one value 150 in the row 1358 in the test set\n",
    "    #We also can't remove any single row from the test set as we will need make predictions for all rows \n",
    "    #Let's just pass the value 150 to 40 , as this value exists in booth sets and is is the less common \n",
    "    df['MSSubClass'][df.MSSubClass=='150']='40'\n",
    "    \n",
    "    # Converting OverallCond to str\n",
    "    df.OverallCond = df.OverallCond.astype(\"str\")\n",
    "    \n",
    "    # KitchenAbvGr to categorical\n",
    "    df['KitchenAbvGr'] = df['KitchenAbvGr'].astype(\"str\")\n",
    "    df.drop(df[df.KitchenAbvGr=='3'].index,inplace=True) # apenas no treino\n",
    "    \n",
    "    # Year and Month to categorical\n",
    "    df['YrSold'] = df['YrSold'].astype(\"str\")\n",
    "    df['MoSold'] = df['MoSold'].astype(\"str\")    \n",
    "    \n",
    "    #LotFrontage - insert the mean \n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=1)\n",
    "    #print(np.shape(df['LotFrontage']))\n",
    "    df['LotFrontage'] = imp.fit_transform(df['LotFrontage']).transpose()    \n",
    "   \n",
    "    #Alley\n",
    "    df.Alley.fillna(inplace=True,value='No')\n",
    "\n",
    "    #MasVnrType - remove the records where the value is NA \n",
    "    #print(\"Number of lines where MasVnrType has Nan value\", len(df[df['MasVnrType'].isnull()]))\n",
    "    #df.dropna(axis=0,subset=['MasVnrType'],inplace=True)\n",
    "    #df.drop('MasVnrType',axis=1,inplace=True)\n",
    "    # MasVnrType NA in all. filling with most popular values\n",
    "    df.MasVnrType.fillna(value=df['MasVnrType'].mode()[0],inplace=True)\n",
    "    \n",
    "    #MasVnrArea - remove the hole column\n",
    "    df.drop('MasVnrArea',axis=1,inplace=True)\n",
    "    \n",
    "    #Condition2 - remove the hole column Possui quantidade de tipos diferentes na base de treino e teste e apenas \n",
    "    #um dos tipos é relevante    \n",
    "    df.drop('Condition2',axis=1,inplace=True)\n",
    "    \n",
    "    #RoofMatl - remove the hole column Possui quantidade de tipos diferentes na base de treino e teste e apenas \n",
    "    #um dos tipos é relevante    \n",
    "    df.drop('RoofMatl',axis=1,inplace=True)\n",
    "    \n",
    "\n",
    "    #MSZoning   - tem NA apenas na base de teste. Como nao posso remover linhas removo a coluna   \n",
    "    #df.dropna(axis=0,subset=['MSZoning'],inplace=True)\n",
    "    df.drop('MSZoning',axis=1,inplace=True)\n",
    "    #df.MSZoning.fillna(df['MSZoning'].mode()[0])\n",
    "\n",
    "    \n",
    "    #BsmtQual\n",
    "    df.BsmtQual.fillna(inplace=True,value='No')\n",
    "    \n",
    "    #HouseStyle - Esse valor so existe na base de treino. Ao inves de remover toda coluna removo somente as linhas \n",
    "    df.drop(df[df.HouseStyle=='2.5Fin'].index,inplace=True)\n",
    "    \n",
    "    #BsmtCond\n",
    "    df.BsmtCond.fillna(inplace=True,value='No')\n",
    "\n",
    "    #BsmtExposure\n",
    "    df.BsmtExposure.fillna(inplace=True,value='No')\n",
    "\n",
    "    #BsmtFinType1\n",
    "    df.BsmtFinType1.fillna(inplace=True,value='No')\n",
    "\n",
    "    #BsmtFinType2\n",
    "    df.BsmtFinType2.fillna(inplace=True,value='No')\n",
    "\n",
    "    #Electrical - remove the records where the value is NA\n",
    "    #print(\"Number of lines where Electrical has Nan value\",len(df[df['Electrical'].isnull()]))\n",
    "    df.dropna(axis=0,subset=['Electrical'],inplace=True) # apenas no treino \n",
    "    df.drop(df[df.Electrical=='Mix'].index,inplace=True) # apenas no treino\n",
    "    #print(\"Number of lines where Electrical has Nan value\",len(df[df['Electrical'].isnull()]))\n",
    "\n",
    "    #FireplaceQu\n",
    "    df.FireplaceQu.fillna(inplace=True,value='No')\n",
    "    \n",
    "\n",
    "    #GarageType\n",
    "    df.GarageType.fillna(inplace=True,value='No')\n",
    "\n",
    "    #GarageYrBlt - remove the hole column\n",
    "    df.drop('GarageYrBlt',axis=1,inplace=True)\n",
    "\n",
    "    #GarageFinish\n",
    "    df.GarageFinish.fillna(inplace=True,value='No')\n",
    "\n",
    "    #GarageQual - A base de teste nao tem um dos tipos presente na base de treino. Assim a base de treino terá uma \n",
    "    #feature para esse tipo e a de teste não. Alem disso, apenas um tipo é pertinente\n",
    "    #Achei melhor entao excluir essa coluna    \n",
    "    df.drop('GarageQual',axis=1,inplace=True)\n",
    "    #df.drop(df[df.GarageQual=='Ex'].index,inplace=True)\n",
    "    \n",
    "    #GarageCond\n",
    "    df.GarageCond.fillna(inplace=True,value='No')\n",
    "\n",
    "    #PoolQC\n",
    "    #df.PoolQC.fillna(inplace=True,value='No')\n",
    "    df.drop('PoolQC',axis=1,inplace=True)\n",
    "    \n",
    "    #Fence\n",
    "    df.Fence.fillna(inplace=True,value='No')\n",
    "\n",
    "    #MiscFeature\n",
    "    #df.MiscFeature.fillna(inplace=True,value='No')\n",
    "    df.drop('MiscFeature',axis=1,inplace=True)\n",
    "\n",
    "    #MiscVal\n",
    "    df.drop('MiscVal',axis=1,inplace=True)\n",
    "    \n",
    "    #SaleType\n",
    "    df.drop('SaleType',axis=1,inplace=True)\n",
    "    \n",
    "    #Exterior1st- nao posso remover linhas do teste\n",
    "    #df.dropna(axis=0,subset=['Exterior1st'],inplace=True)     \n",
    "    #df.drop(df[df.Exterior1st=='Stone'].index,inplace=True)\n",
    "    #df.drop(df[df.Exterior1st=='ImStucc'].index,inplace=True)\n",
    "    #df.drop(df[df.Exterior1st=='CBlock'].index,inplace=True)\n",
    "    df.drop('Exterior1st',axis=1,inplace=True)\n",
    "    \n",
    "    #Exterior2nd\n",
    "    #df.dropna(axis=0,subset=['Exterior2nd'],inplace=True)\n",
    "    #df.Exterior2nd.fillna(inplace=True,value= 'Other')\n",
    "    #df.drop(df[df.Exterior2nd=='Other'].index,inplace=True)\n",
    "    #df.drop(df[df.Exterior2nd=='CBlock'].index,inplace=True)\n",
    "    df.drop('Exterior2nd',axis=1,inplace=True)\n",
    "    \n",
    "    #Heating -- esses tipos existem apenas na base de treino\n",
    "    df.drop(df[df.Heating=='OthW'].index,inplace=True)\n",
    "    df.drop(df[df.Heating=='Floor'].index,inplace=True)\n",
    "    \n",
    "    #KitchenQual\n",
    "    #df.dropna(axis=0,subset=['KitchenQual'],inplace=True)\n",
    "    df.KitchenQual.fillna(inplace=True,value='Fa') #- Apenas a base de teste tem NA e como nao posso remover registro\n",
    "    #dessa base setei o valor menos comum\n",
    "    \n",
    "    #Functional\n",
    "    #df.dropna(axis=0,subset=['Functional'],inplace=True)\n",
    "    df.drop('Functional',axis=1,inplace=True)\n",
    "    \n",
    "    #Utilities\n",
    "    df.drop('Utilities',axis=1,inplace=True)\n",
    "    \n",
    "    #BsmtFinSF1\n",
    "    #df.dropna(axis=0,subset=['BsmtFinSF1'],inplace=True)\n",
    "    df['BsmtFinSF1'] = imp.fit_transform(df['BsmtFinSF1']).transpose()    \n",
    "    \n",
    "    #BsmtFinSF2\n",
    "    #df.dropna(axis=0,subset=['BsmtFinSF2'],inplace=True)\n",
    "    df['BsmtFinSF2'] = imp.fit_transform(df['BsmtFinSF2']).transpose()    \n",
    "    \n",
    "    #BsmtUnfSF\n",
    "    #df.dropna(axis=0,subset=['BsmtUnfSF'],inplace=True)\n",
    "    df.drop('BsmtUnfSF',axis=1,inplace=True)\n",
    "    \n",
    "    #TotalBsmtSF\n",
    "    #df.dropna(axis=0,subset=['TotalBsmtSF'],inplace=True)\n",
    "    #df['TotalBsmtSF'] = imp.fit_transform(df['TotalBsmtSF']).transpose()    \n",
    "    df.TotalBsmtSF.fillna(value=0,inplace=True)\n",
    "    \n",
    "    #BsmtFullBath - apenas na base de teste tem NA.Nao posso remover a linha\n",
    "    df.BsmtFullBath.fillna(inplace=True,value=0)\n",
    "    \n",
    "    #BsmtHalfBath- apenas na base de teste tem NA.Nao posso remover a linha\n",
    "    df.BsmtHalfBath.fillna(inplace=True,value=0)\n",
    "    \n",
    "    #GarageCars\n",
    "    #df.dropna(axis=0,subset=['GarageCars'],inplace=True)\n",
    "    df.GarageCars.fillna(value=0,inplace=True)\n",
    "    \n",
    "    #GarageArea\n",
    "    #df.dropna(axis=0,subset=['GarageArea'],inplace=True)\n",
    "    df.GarageArea.fillna(value=0,inplace=True)\n",
    "    \n",
    "df = pd.read_csv(\"train.csv\",na_values=['?','NA'],delimiter=',',delim_whitespace=False)\n",
    "df_test = pd.read_csv(\"test.csv\",na_values=['?','NA'],delimiter=',',delim_whitespace=False)\n",
    "\n",
    "print(df.shape)\n",
    "print(df_test.shape)\n",
    "#print(df.head())\n",
    "#print(df.describe())\n",
    "#print(df.dtypes)\n",
    "#df = df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "########################Dealing with missing values\n",
    "\n",
    "#missing data\n",
    "# total = df.isnull().sum().sort_values(ascending=False)\n",
    "# percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "# print(missing_data.head(20))\n",
    "\n",
    "# \n",
    "#               Total   Percent\n",
    "# PoolQC         1453  0.995205\n",
    "# MiscFeature    1406  0.963014\n",
    "# Alley          1369  0.937671\n",
    "# Fence          1179  0.807534\n",
    "# FireplaceQu     690  0.472603\n",
    "# LotFrontage     259  0.177397\n",
    "# GarageCond       81  0.055479\n",
    "# GarageType       81  0.055479\n",
    "# GarageYrBlt      81  0.055479\n",
    "# GarageFinish     81  0.055479\n",
    "# GarageQual       81  0.055479\n",
    "# BsmtExposure     38  0.026027\n",
    "# BsmtFinType2     38  0.026027\n",
    "# BsmtFinType1     37  0.025342\n",
    "# BsmtCond         37  0.025342\n",
    "# BsmtQual         37  0.025342\n",
    "# MasVnrArea        8  0.005479\n",
    "# MasVnrType        8  0.005479\n",
    "# Electrical        1  0.000685\n",
    "# Utilities         0  0.000000\n",
    "\n",
    "\n",
    "\n",
    "#print(df.columns[df.isnull().any()])\n",
    "#'LotFrontage', 'Alley', 'MasVnrType', 'MasVnrArea', 'BsmtQual',\n",
    "#       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "#       'Electrical', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n",
    "#       'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence',\n",
    "#       'MiscFeature'\n",
    "input_missing_value(df)\n",
    "\n",
    "\n",
    "#print(df_test.columns[df_test.isnull().any()])\n",
    "#Index(['MSZoning', 'LotFrontage', 'Alley', 'Utilities', 'Exterior1st',\n",
    "#       'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond',\n",
    "#       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n",
    "#      'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n",
    "#       'BsmtHalfBath', 'KitchenQual', 'Functional', 'FireplaceQu',\n",
    "#      'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea',\n",
    "#       'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature',\n",
    "#       'SaleType'],\n",
    "\n",
    "input_missing_value(df_test)\n",
    "\n",
    "\n",
    "#Valores numericos que continham NA sao detectados como String. Assim, depois que removemos o NA temos que setar corretamente \n",
    "#o tipo \n",
    "df_test.BsmtFullBath = df_test.BsmtFullBath.astype(\"int64\")\n",
    "df_test.BsmtHalfBath = df_test.BsmtHalfBath.astype(\"int64\")\n",
    "df_test.GarageCars = df_test.GarageCars.astype(\"int64\")\n",
    "df_test.GarageArea = df_test.GarageArea.astype(\"int64\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df_test.shape)\n",
    "print(df.dtypes)\n",
    "print(\"Null values treino \\n\", df.columns[df.isnull().any()])\n",
    "print(\"Null values test \\n\", df_test.columns[df_test.isnull().any()])\n",
    "\n",
    "########################End dealing with missing values\n",
    "\n",
    "\n",
    "# The OneHotEncoder converts features represented as numeric codes (so they are values that can't be ordered)\n",
    "#to their binary representation\n",
    "#enc = preprocessing.OneHotEncoder() \n",
    "#df = enc.fit_transform(df)\n",
    "\n",
    "\n",
    "########################Tratando campos nominais\n",
    "\n",
    "vec = DictVectorizer()\n",
    "aux = np.asmatrix(vec.fit_transform(df.transpose().to_dict().values()).toarray())\n",
    "\n",
    "data_train = pd.DataFrame(aux,columns=vec.feature_names_)\n",
    "#data_train = pd.get_dummies(df)\n",
    "\n",
    "\n",
    "data_train.to_csv('train_no_categorical.csv')\n",
    "\n",
    "print(\"New shape train:\" , np.shape(data_train))\n",
    "print(\"Indice da coluna SalePrice no novo dataset\" , data_train.columns.get_loc('SalePrice'))\n",
    "\n",
    "################################################# Base de teste\n",
    "\n",
    "vec = DictVectorizer()\n",
    "aux_test = vec.fit_transform(df_test.transpose().to_dict().values()).toarray()\n",
    "data_test = pd.DataFrame(aux_test,columns=vec.feature_names_)\n",
    "#data_test = pd.get_dummies(df_test)\n",
    " \n",
    "print(\"New shape test:\" , np.shape(data_test))\n",
    "\n",
    "data_test.to_csv('test_no_categorical.csv')\n",
    "\n",
    "\n",
    "print(\"Colunas que existem apenas teste : \" , data_test.columns.difference(data_train.columns))\n",
    "print(\"Colunas que existem apenas treino : \" , data_train.columns.difference(data_test.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count      1445.000000\n",
      "mean     181043.215225\n",
      "std       79195.218195\n",
      "min       34900.000000\n",
      "25%      130000.000000\n",
      "50%      163000.000000\n",
      "75%      214000.000000\n",
      "max      755000.000000\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAAPUCAYAAABMx1tcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa2fce822e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAAFoCAYAAADzZ0kIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiSpEoYOSZJUCUOHJEmqhKFDkiRV4v8DcZ61JHR3uyoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa320626da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhMAAAFoCAYAAADtrnm7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkooYJiRJUpH/D2/J0Sp5ix+jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3203cd6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFoCAYAAADUycjgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiQVMTRIkqQihgZJklTE0CBJkor8P9FcfwI/n2/8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3a611af98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg8AAAFoCAYAAADQPBjdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCRJRQwPkiSpyP8Hq1xKkV+zRYwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3202f1e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg8AAAFoCAYAAADQPBjdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3200a67b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "print(data_train.SalePrice.describe())\n",
    "\n",
    "saleprice_scaled = preprocessing.StandardScaler().fit_transform((data_train['SalePrice'][:,np.newaxis]));\n",
    "fig = plt.figure(1, figsize=(6, 12))\n",
    "#ax = fig.add_subplot(111)\n",
    "#ax.boxplot(saleprice_scaled)\n",
    "plt.boxplot(saleprice_scaled,showfliers=True)\n",
    "\n",
    "plt.figure()\n",
    "x = plt.hist(data_train['SalePrice'],bins=50)\n",
    "\n",
    "plt.figure()\n",
    "saleprice_log = np.log(data_train['SalePrice'])\n",
    "x = plt.hist(saleprice_log,bins=50)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "x = plt.hist(data_train['LotArea'],bins=50)\n",
    "\n",
    "plt.figure()\n",
    "saleprice_log = np.log(data_train['LotArea'])\n",
    "x = plt.hist(saleprice_log,bins=50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "x = plt.hist(data_train['GarageCars'],bins=50)\n",
    "\n",
    "# plt.figure()\n",
    "# saleprice_log = np.log(data_train['GarageArea'])\n",
    "# x = plt.hist(saleprice_log,bins=50)\n",
    "\n",
    "\n",
    "#data_train['SalePrice'] = np.log1p(data_train['SalePrice'])\n",
    "data_train['OverallQual'] = np.log1p(data_train['OverallQual'])\n",
    "data_train['LotArea'] = np.log1p(data_train['LotArea'])\n",
    "\n",
    "\n",
    "data_test['OverallQual'] = np.log1p(data_test['OverallQual'])\n",
    "data_test['LotArea'] = np.log1p(data_test['LotArea'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df,name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name,x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tentativa de selecionar melhores features \n",
      "\n",
      "\n",
      "As features selecionadas com Tree-based feature selection foram: \n",
      "\n",
      "['ExterQual=TA' 'GarageCars' 'OverallQual' 'GrLivArea' 'BsmtQual=Ex'\n",
      " 'FireplaceQu=No' 'FullBath' 'Neighborhood=NoRidge' 'TotalBsmtSF'\n",
      " 'KitchenQual=Ex' '1stFlrSF' 'TotRmsAbvGrd' 'YearRemodAdd' 'BsmtFinSF1'\n",
      " '2ndFlrSF' 'MSSubClass=60' 'LotArea' 'GarageArea' 'BsmtExposure=Gd'\n",
      " 'BsmtFullBath' 'BsmtQual=Gd' 'KitchenQual=TA' 'ExterQual=Fa'\n",
      " 'BedroomAbvGr' 'BldgType=1Fam' 'LotFrontage' 'YearBuilt']\n",
      "[[ 0.25326976  0.15653858  0.11906353  0.05953481  0.05669846  0.02899387\n",
      "   0.02430099  0.01950651  0.01584306  0.01531264  0.01377303  0.01118266\n",
      "   0.00942146  0.00895623  0.00836027  0.00793164  0.00711999  0.00592519\n",
      "   0.00564279  0.00560564  0.00516632  0.00507838  0.00463865  0.00458616\n",
      "   0.00452164  0.00424911  0.00357861]]\n",
      "\n",
      " New shape train apos Tree-based feature selection: (1445, 26)\n",
      "\n",
      " Fim tentativa selecionar melhores features \n",
      "\n",
      "\n",
      " New shape test apos Tree-based feature selection: (1459, 26)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Tentativa de selecionar melhores features \\n\")\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "#Removing features with low variance\n",
    "#print(\"Original shape: {}\".format(np.shape(df.iloc[:,0:-1])))\n",
    "#sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "#features = sel.fit_transform(df.iloc[:,0:-1])\n",
    "#print(\"Shape apos Removing features with low variance {}\".format(np.shape(features))) #nenhuma foi selecionada \n",
    "#print(\"\\n\")\n",
    "\n",
    "#Tree-based feature selection\n",
    "y_train = (data_train['SalePrice'])\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "\n",
    "print()\n",
    "\n",
    "clf = ExtraTreesRegressor(n_estimators=20)\n",
    "clf = clf.fit(x_train,y_train)\n",
    "data = np.zeros((1,x_train.shape[1])) \n",
    "data = pd.DataFrame(data, columns=x_train.columns)\n",
    "data.iloc[0] = clf.feature_importances_\n",
    "data = data.T.sort_values(df.index[0], ascending=False).T\n",
    "\n",
    "\n",
    "print(\"As features selecionadas com Tree-based feature selection foram: \\n\")\n",
    "yyy = np.asarray((data.columns[0:27]))\n",
    "xxx = np.asarray((data.iloc[:,0:27]))\n",
    "print(yyy)\n",
    "print(xxx)\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "aux = model.transform(x_train)\n",
    "\n",
    "print(\"\\n New shape train apos Tree-based feature selection: {}\".format(aux.shape))\n",
    "\n",
    "print(\"\\n Fim tentativa selecionar melhores features \\n\")\n",
    "\n",
    "\n",
    "data_train_less_features = pd.concat([pd.DataFrame(aux),pd.DataFrame(y_train)],axis=1)\n",
    "data_train_less_features.to_csv('data_train_less_features.csv')\n",
    "\n",
    "\n",
    "aux = model.transform((data_test))\n",
    "data_test_less_features = pd.DataFrame(aux)\n",
    "print(\"\\n New shape test apos Tree-based feature selection: {}\".format(aux.shape))\n",
    "data_test_less_features.to_csv('data_test_less_features.csv')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn   import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression \n",
      "Fold #1\n",
      "Fold score (RMSE): 86326540431590368.00\n",
      "Accuracy: -1559335520869089326137344.000\n",
      "Fold #2\n",
      "Fold score (RMSE): 33221.64\n",
      "Accuracy: 0.803\n",
      "Fold #3\n",
      "Fold score (RMSE): 409287583619938368.00\n",
      "Accuracy: -20164434937263641927352320.000\n",
      "Fold #4\n",
      "Fold score (RMSE): 382950344654433088.00\n",
      "Accuracy: -23636707329943163665121280.000\n",
      "Fold #5\n",
      "Fold score (RMSE): 38140580376246368.00\n",
      "Accuracy: -233904345574870563684352.000\n",
      "\n",
      " Average RMSE: 2.541945192291608e+17\n",
      "\n",
      "\n",
      "\n",
      "SGDRegressor \n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 867241.28\n",
      "Accuracy: -156.373\n",
      "Fold #2\n",
      "Fold score (RMSE): 178233.78\n",
      "Accuracy: -4.667\n",
      "Fold #3\n",
      "Fold score (RMSE): 685615.51\n",
      "Accuracy: -55.584\n",
      "Fold #4\n",
      "Fold score (RMSE): 350640.80\n",
      "Accuracy: -18.816\n",
      "Fold #5\n",
      "Fold score (RMSE): 68744.72\n",
      "Accuracy: 0.240\n",
      "\n",
      " Average RMSE: 525664.9391346639\n",
      "\n",
      "\n",
      "\n",
      "Ridge \n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 27838.56\n",
      "Accuracy: 0.838\n",
      "Fold #2\n",
      "Fold score (RMSE): 32545.73\n",
      "Accuracy: 0.811\n",
      "Fold #3\n",
      "Fold score (RMSE): 33542.53\n",
      "Accuracy: 0.865\n",
      "Fold #4\n",
      "Fold score (RMSE): 32048.54\n",
      "Accuracy: 0.834\n",
      "Fold #5\n",
      "Fold score (RMSE): 42194.68\n",
      "Accuracy: 0.714\n",
      "\n",
      " Average RMSE: 33961.18564108616\n",
      "\n",
      "\n",
      "\n",
      "Lasso \n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 26676.24\n",
      "Accuracy: 0.851\n",
      "Fold #2\n",
      "Fold score (RMSE): 31803.63\n",
      "Accuracy: 0.820\n",
      "Fold #3\n",
      "Fold score (RMSE): 33044.01\n",
      "Accuracy: 0.869\n",
      "Fold #4\n",
      "Fold score (RMSE): 30176.61\n",
      "Accuracy: 0.853\n",
      "Fold #5\n",
      "Fold score (RMSE): 40484.11\n",
      "Accuracy: 0.736\n",
      "\n",
      " Average RMSE: 32755.426347219047\n",
      "\n",
      "\n",
      "\n",
      "Elastic Net \n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 57040.92\n",
      "Accuracy: 0.319\n",
      "Fold #2\n",
      "Fold score (RMSE): 62968.71\n",
      "Accuracy: 0.293\n",
      "Fold #3\n",
      "Fold score (RMSE): 78686.04\n",
      "Accuracy: 0.255\n",
      "Fold #4\n",
      "Fold score (RMSE): 67289.79\n",
      "Accuracy: 0.270\n",
      "Fold #5\n",
      "Fold score (RMSE): 66142.34\n",
      "Accuracy: 0.297\n",
      "\n",
      " Average RMSE: 66802.60739260416\n",
      "\n",
      "\n",
      " Less Features\n",
      "Linear Regression \n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 33889.64\n",
      "Accuracy: 0.862\n",
      "Fold #2\n",
      "Fold score (RMSE): 26060.33\n",
      "Accuracy: 0.857\n",
      "Fold #3\n",
      "Fold score (RMSE): 53456.13\n",
      "Accuracy: 0.347\n",
      "Fold #4\n",
      "Fold score (RMSE): 35875.25\n",
      "Accuracy: 0.837\n",
      "Fold #5\n",
      "Fold score (RMSE): 29259.05\n",
      "Accuracy: 0.854\n",
      "\n",
      " Average RMSE: 36954.92262138444\n",
      "\n",
      "\n",
      "\n",
      "SGDRegressor \n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 35796.94\n",
      "Accuracy: 0.846\n",
      "Fold #2\n",
      "Fold score (RMSE): 26112.13\n",
      "Accuracy: 0.857\n",
      "Fold #3\n",
      "Fold score (RMSE): 55719.98\n",
      "Accuracy: 0.291\n",
      "Fold #4\n",
      "Fold score (RMSE): 35103.42\n",
      "Accuracy: 0.844\n",
      "Fold #5\n",
      "Fold score (RMSE): 29614.80\n",
      "Accuracy: 0.851\n",
      "\n",
      " Average RMSE: 37887.37628136676\n",
      "\n",
      "\n",
      "\n",
      "Ridge \n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 33953.22\n",
      "Accuracy: 0.861\n",
      "Fold #2\n",
      "Fold score (RMSE): 26053.86\n",
      "Accuracy: 0.857\n",
      "Fold #3\n",
      "Fold score (RMSE): 53186.66\n",
      "Accuracy: 0.354\n",
      "Fold #4\n",
      "Fold score (RMSE): 35870.40\n",
      "Accuracy: 0.837\n",
      "Fold #5\n",
      "Fold score (RMSE): 29221.15\n",
      "Accuracy: 0.855\n",
      "\n",
      " Average RMSE: 36880.90669480122\n"
     ]
    }
   ],
   "source": [
    "#Starting making predictors\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, RidgeCV, Lasso, LassoCV, ElasticNetCV\n",
    "\n",
    "#Caso 1 - Linear Regression \n",
    "print(\"Linear Regression \")\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "data_train = data_train.reindex(np.random.permutation(data_train.index))\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "#Normalization\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "classifier = LinearRegression()\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"SGDRegressor \\n\\n\")\n",
    "\n",
    "\n",
    "classifier = SGDRegressor()\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Ridge \\n\\n\")\n",
    "\n",
    "\n",
    "classifier = RidgeCV()\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Lasso \\n\\n\")\n",
    "\n",
    "\n",
    "classifier = LassoCV()\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Elastic Net \\n\\n\")\n",
    "\n",
    "\n",
    "classifier = ElasticNetCV()\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "\n",
    "\n",
    "###########Less features\n",
    "\n",
    "print(\"\\n\\n Less Features\")\n",
    "print(\"Linear Regression \\n\")\n",
    "#Normalization\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "classifierLinearRegression = LinearRegression()\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "    pred = []    \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifierLinearRegression = classifierLinearRegression.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifierLinearRegression.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifierLinearRegression.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "  \n",
    "#Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "# Write the cross-validated prediction\n",
    "pred = []\n",
    "pred = np.array(pred)\n",
    "pred = classifierLinearRegression.predict(scaler.transform(data_test_less_features))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_LinearRegression.csv', columns=['SalePrice'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"SGDRegressor \\n\\n\")\n",
    "\n",
    "classifier = SGDRegressor()\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Ridge \\n\\n\")\n",
    "\n",
    "\n",
    "classifier = RidgeCV()\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "Fold #1\n"
     ]
    }
   ],
   "source": [
    "#Caso 3 - SVM\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import NuSVR\n",
    "\n",
    "print(\"SVM\")\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "data_train = data_train.reindex(np.random.permutation(data_train.index))\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#Normalization\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "x_train = np.ascontiguousarray(x_train)\n",
    "\n",
    "\n",
    "classifier = NuSVR(kernel='linear', C=1e4) #34761.27693615821\n",
    "kf = KFold(5)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "\n",
    "# The mean squared error\n",
    "#pred = classifier.predict(x_test_scaled)\n",
    "#score = metrics.mean_squared_error(y_test, pred)\n",
    "#print(\"Final, out of sample score (RMSE): {}\".format(score))    \n",
    "\n",
    "# Evaluate success using accuracy\n",
    "#print(\"Final Accuracy: %.3f\" % classifier.score(X=x_test_scaled,y=y_test))\n",
    "\n",
    "###########Less features\n",
    "print(\"\\n\\n Less features \\n\")\n",
    "#Normalization\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "x_train = np.ascontiguousarray(x_train)\n",
    "\n",
    "\n",
    "#classifier = SVR(kernel='rbf', C=1e3, gamma=0.1) #66483.84692815947\n",
    "classifierSVR = SVR(kernel='linear', C=1e4) #34761.27693615821\n",
    "#classifier = SVR(kernel='poly', C=1e3, degree=3) #86747.4465877091\n",
    "#classifier = NuSVR(C=1e3) #57249.1589623674\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifierSVR = classifierSVR.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifierSVR.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifierSVR.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "    \n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "# Write the cross-validated prediction\n",
    "pred = []\n",
    "pred = np.array(pred)\n",
    "pred = classifierSVR.predict(scaler.transform(data_test_less_features))\n",
    "\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "\n",
    "\n",
    "result.to_csv('pred_SVR.csv', columns=['SalePrice'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN MLPRegressor\n",
      "Fold #1\n",
      "Fold score (RMSE): 34904.19\n",
      "Accuracy: 0.758\n",
      "Fold #2\n",
      "Fold score (RMSE): 51262.59\n",
      "Accuracy: 0.571\n",
      "Fold #3\n",
      "Fold score (RMSE): 39369.48\n",
      "Accuracy: 0.767\n",
      "Fold #4\n",
      "Fold score (RMSE): 51077.20\n",
      "Accuracy: 0.631\n",
      "Fold #5\n",
      "Fold score (RMSE): 41717.08\n",
      "Accuracy: 0.725\n",
      "\n",
      " Average RMSE: 44148.17497000926\n",
      "\n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 26675.85\n",
      "Accuracy: 0.865\n",
      "Fold #2\n",
      "Fold score (RMSE): 37557.61\n",
      "Accuracy: 0.784\n",
      "Fold #3\n",
      "Fold score (RMSE): 37136.83\n",
      "Accuracy: 0.818\n",
      "Fold #4\n",
      "Fold score (RMSE): 25357.48\n",
      "Accuracy: 0.875\n",
      "Fold #5\n",
      "Fold score (RMSE): 51302.98\n",
      "Accuracy: 0.609\n",
      "\n",
      " Average RMSE: 36813.90485415818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Shuffle\n",
    "print(\"NN MLPRegressor\")\n",
    "np.random.seed(42)\n",
    "data_train = data_train.reindex(np.random.permutation(data_train.index))\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "#Normalization\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "classifier = MLPRegressor(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10,4,2), random_state=1)\n",
    "kf = KFold(5)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "# The mean squared error\n",
    "#pred = classifier.predict(x_test_scaled)\n",
    "#score = metrics.mean_squared_error(y_test, pred)\n",
    "#print(\"Final, out of sample score (RMSE): {}\".format(score))    \n",
    "\n",
    "# Evaluate success using accuracy\n",
    "#print(\"Final Accuracy: %.3f\" % classifier.score(X=x_test_scaled,y=y_test))\n",
    "\n",
    "###########Less features\n",
    "print(\"\\n\\n\")\n",
    "#Normalization\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "classifier = MLPRegressor(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(8,2), random_state=1)\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests\n",
      "Mean squared error: 29597.05109582514\n",
      "Accuracy: 0.852\n",
      "\n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 28951.730162219337\n",
      "Accuracy: 0.886\n",
      "Fold #2\n",
      "Fold score (RMSE): 25924.39997455174\n",
      "Accuracy: 0.897\n",
      "Fold #3\n",
      "Fold score (RMSE): 32718.1452740622\n",
      "Accuracy: 0.820\n",
      "Fold #4\n",
      "Fold score (RMSE): 30757.611548626795\n",
      "Accuracy: 0.822\n",
      "Fold #5\n",
      "Fold score (RMSE): 27059.08361496614\n",
      "Accuracy: 0.880\n",
      "\n",
      " Average RMSE: 29185.540590707198\n",
      "\n",
      " oob score : 0.8655219352623467\n",
      "\n",
      "\n",
      " Less features \n",
      "\n",
      "\n",
      "Mean squared error: 29779.96055902021\n",
      "Accuracy: 0.856\n",
      "\n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 36254.35231265968\n",
      "Accuracy: 0.750\n",
      "Fold #2\n",
      "Fold score (RMSE): 39473.89479260442\n",
      "Accuracy: 0.762\n",
      "Fold #3\n",
      "Fold score (RMSE): 41208.986515079916\n",
      "Accuracy: 0.776\n",
      "Fold #4\n",
      "Fold score (RMSE): 36231.22653321236\n",
      "Accuracy: 0.745\n",
      "Fold #5\n",
      "Fold score (RMSE): 39425.132424309006\n",
      "Accuracy: 0.769\n",
      "\n",
      " Average RMSE: 38568.873056088494\n",
      "\n",
      " oob score : 0.7591719377211419\n"
     ]
    }
   ],
   "source": [
    "##Random Forests\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Shuffle\n",
    "print(\"Random Forests\")\n",
    "np.random.seed(42)\n",
    "data_train = data_train.reindex(np.random.permutation(data_train.index))\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_train.drop('SalePrice',axis=1), data_train['SalePrice'], \n",
    "                                                    test_size=0.20, random_state=42)\n",
    "\n",
    "classifier = RandomForestRegressor(n_estimators=20,oob_score=True)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# The mean squared error\n",
    "pred = classifier.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "print(\"Mean squared error: {}\".format(score))\n",
    "# Evaluate success using accuracy\n",
    "print(\"Accuracy: %.3f\" % classifier.score(X=x_test,y=y_test))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "classifier = RandomForestRegressor(n_estimators=1000,oob_score=True)\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "print(\"\\n oob score : {}\".format(classifier.oob_score_))    \n",
    "\n",
    "# Write the cross-validated prediction\n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifier.predict(scaler.transform(data_test))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_RF_full_features.csv', columns=['SalePrice'])\n",
    "\n",
    "###########Less features\n",
    "print(\"\\n\\n Less features \\n\\n\")\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_train_less_features.drop('SalePrice',axis=1), \n",
    "                                    data_train_less_features['SalePrice'], test_size=0.20, random_state=42)\n",
    "\n",
    "classifier = RandomForestRegressor(n_estimators=20,oob_score=True)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# The mean squared error\n",
    "pred = classifier.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "print(\"Mean squared error: {}\".format(score))\n",
    "# Evaluate success using accuracy\n",
    "print(\"Accuracy: %.3f\" % classifier.score(X=x_test,y=y_test))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=7)    \n",
    "\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "\n",
    "classifierRandomForestRegressor = RandomForestRegressor(n_estimators=1000,oob_score=True,max_depth=3)\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifierRandomForestRegressor = classifierRandomForestRegressor.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifierRandomForestRegressor.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifierRandomForestRegressor.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "print(\"\\n oob score : {}\".format(classifierRandomForestRegressor.oob_score_))    \n",
    "\n",
    "\n",
    "# Write the cross-validated prediction\n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifierRandomForestRegressor.predict(scaler.transform(data_test_less_features))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_RF_less_features.csv', columns=['SalePrice'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 33557.18177960725\n",
      "Accuracy: 0.789\n",
      "\n",
      "\n",
      "\n",
      "Mean squared error: 39375.655916518524\n",
      "Accuracy: 0.761\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "data_train_less_features = data_train_less_features.reindex(np.random.permutation(data_train_less_features.index))\n",
    "data_train_less_features.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train,y_train,test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "classifierGBR = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, \n",
    "                                               min_samples_leaf=25, min_samples_split=20, loss='huber').fit(x_train, y_train)\n",
    "\n",
    "# The mean squared error\n",
    "pred = classifierGBR.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "print(\"Mean squared error: {}\".format(score))\n",
    "# Evaluate success using accuracy\n",
    "print(\"Accuracy: %.3f\" % classifierGBR.score(X=x_test,y=y_test))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "data_train_less_features = data_train_less_features.reindex(np.random.permutation(data_train_less_features.index))\n",
    "data_train_less_features.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train,y_train,test_size=0.20, random_state=43)\n",
    "\n",
    "classifierGBR.fit(x_train, y_train)\n",
    "\n",
    "# The mean squared error\n",
    "pred = classifierGBR.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "print(\"Mean squared error: {}\".format(score))\n",
    "# Evaluate success using accuracy\n",
    "print(\"Accuracy: %.3f\" % classifierGBR.score(X=x_test,y=y_test))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "#save the contest result \n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifierGBR.predict(scaler.transform(data_test_less_features))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_GBR.csv', columns=['SalePrice'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 0.12.1\n",
      "Fold #1\n",
      "Fold score (RMSE): 47318.10\n",
      "Fold #2\n",
      "Fold score (RMSE): 28433.82\n",
      "Fold #3\n",
      "Fold score (RMSE): 37041.23\n",
      "Fold #4\n",
      "Fold score (RMSE): 24512.86\n",
      "Fold #5\n",
      "Fold score (RMSE): 28395.67\n",
      "\n",
      " Average RMSE: 34137.16945021057\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensor Flow Version: {}\".format(tf.__version__))\n",
    "import tensorflow.contrib.learn as learn\n",
    "import shutil \n",
    "import os\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df,target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    \n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.int32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "\n",
    "    # Get a new directory to hold checkpoints from a neural network.  This allows the neural network to be\n",
    "# loaded later.  If the erase param is set to true, the contents of the directory will be cleared.\n",
    "def get_model_dir(name,erase=False):\n",
    "    base_path = os.path.join(\".\",\"dnn\")\n",
    "    model_dir = os.path.join(base_path,name)\n",
    "    os.makedirs(model_dir,exist_ok=True)\n",
    "    if erase and len(model_dir)>4 and os.path.isdir(model_dir):\n",
    "        shutil.rmtree(model_dir,ignore_errors=True) # be careful, this deletes everything below the specified path\n",
    "    return model_dir\n",
    "\n",
    "\n",
    "#Normalization\n",
    "# y_train = ((data_train['SalePrice']))\n",
    "# x_train = (data_train.drop('SalePrice',axis=1))\n",
    "# scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "# x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "\n",
    "#Normalization\n",
    "#x_train,y_train = to_xy(data_train_less_features,\"SalePrice\")\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "#Choose an optimizer\n",
    "#opt=tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "#opt=tf.train.MomentumOptimizer(learning_rate=0.001,momentum=0.9)\n",
    "\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x_train.shape[1])]\n",
    "classifierDNN = learn.DNNRegressor(hidden_units=[20, 10, 10, 5, 2], \n",
    "                                   feature_columns=feature_columns,\n",
    "                                   model_dir=get_model_dir(\"dnn\",True)\n",
    "                                   #optimizer=opt\n",
    "                                  )\n",
    "\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    # Early stopping\n",
    "    validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    x_test_fold,\n",
    "    y_test_fold,\n",
    "    every_n_steps=500,\n",
    "    #metrics=validation_metrics,\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=400)\n",
    "        \n",
    "    classifierDNN.fit(x_train_fold, y_train_fold, monitors=[validation_monitor] ,steps=1000)\n",
    "    pred = (list(classifierDNN.predict(x_test_fold, as_iterable=True)))\n",
    "    \n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "print(\"\\n\\n\")\n",
    "\n",
    "#Save the contest result\n",
    "pred = (list(classifierDNN.predict(scaler.transform(data_test_less_features),as_iterable=True)))\n",
    "#pred = list(classifierDNN.predict(scaler.transform(data_test),as_iterable=True))\n",
    "\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_DNN.csv', columns=['SalePrice'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[135705.78, 151312.81, 175873.78, 179217.95, 183730.81, 176933.25, 171580.22, 163244.17, 202774.53, 127362.87, 204563.72, 102630.38, 103376.4, 135792.81, 114710.8, 354446.75, 243943.67, 339375.16, 340696.56, 414247.0, 291875.84, 212966.0, 184279.14, 164335.38, 203950.41, 203481.0, 369254.06, 246585.91, 189256.22, 239331.78, 198016.73, 109966.16, 208014.39, 313440.31, 301488.31, 244281.75, 172711.81, 155905.73, 155400.0, 146315.95, 150985.66, 144712.42, 266215.38, 223809.31, 217690.06, 170060.41, 243389.42, 202791.91, 159048.77, 143772.62, 153568.06, 159095.88, 129604.34, 162916.41, 206623.91, 161427.84, 168968.11, 136417.33, 226005.89, 132963.59, 138726.03, 174181.53, 106007.48, 113769.03, 125263.48, 125588.12, 114434.8, 133103.5, 127734.84, 178743.06, 132464.64, 106379.93, 124957.05, 124871.69, 154609.59, 115983.21, 90665.094, 165095.33, 235909.09, 116884.62, 142810.89, 139905.34, 191080.75, 85648.25, 106850.31, 140782.31, 135248.12, 114453.91, 119902.91, 129252.49, 131540.27, 144873.75, 128968.26, 117779.38, 163588.47, 97747.68, 110521.24, 102144.3, 106122.26, 137361.16, 117757.27, 124954.65, 119321.99, 164985.62, 150738.64, 252016.92, 94154.539, 214321.53, 121051.7, 132666.31, 113867.05, 153579.53, 262203.09, 120412.07, 244208.16, 239646.91, 187896.06, 140023.66, 153882.72, 197639.72, 133333.69, 119919.88, 300541.06, 221443.2, 145783.44, 82271.672, 95410.305, 150292.69, 100903.77, 127703.18, 91448.727, 108381.02, 121897.86, 164223.22, 133816.08, 212262.36, 162883.5, 193002.69, 198314.09, 177096.66, 70281.727, 132196.89, 86310.883, 304923.25, 251809.12, 161961.58, 174116.2, 230075.66, 189545.02, 159859.23, 147063.47, 172789.25, 147715.06, 127708.48, 91215.016, 89526.367, 100004.35, 125066.34, 132041.09, 215206.16, 157227.3, 121657.95, 284808.34, 201001.78, 134452.12, 164861.72, 183831.33, 297723.62, 173397.25, 313904.56, 189288.0, 224216.3, 172090.94, 170919.72, 179048.95, 154124.58, 176878.48, 204560.28, 174889.58, 252039.03, 188910.78, 252603.62, 218110.22, 221128.89, 180494.12, 150663.7, 161357.39, 126663.79, 130333.21, 118412.49, 123978.0, 104697.48, 103613.52, 127167.83, 127303.09, 131864.86, 128198.65, 131799.42, 129035.2, 141898.27, 401662.69, 354433.56, 348882.69, 427254.53, 336726.88, 350974.97, 363918.25, 345549.44, 342343.09, 349077.84, 254141.84, 382360.47, 305284.28, 248570.44, 194908.09, 188425.77, 222523.69, 398465.75, 386232.88, 319249.75, 310699.94, 350908.75, 181591.23, 191968.44, 175239.91, 171211.5, 188247.12, 196726.56, 195724.34, 198857.97, 187863.17, 279656.16, 181579.02, 175380.66, 159940.55, 267001.16, 156377.59, 333011.91, 315502.03, 269429.97, 289351.0, 248949.75, 258154.72, 281554.0, 259761.72, 414594.5, 225328.0, 197760.44, 268941.88, 217475.84, 260155.05, 247039.88, 301236.81, 215363.81, 203126.14, 179958.27, 171944.14, 125734.65, 236513.62, 256941.92, 179260.25, 121138.79, 160604.91, 214378.25, 235834.34, 178802.81, 159691.81, 183136.53, 173761.67, 177627.78, 113015.03, 128318.05, 129268.45, 125069.3, 123829.07, 111037.02, 316756.97, 277589.31, 260767.0, 187072.16, 173677.33, 153394.81, 165466.34, 273283.59, 192876.59, 224598.31, 208739.19, 234354.28, 141283.89, 150957.5, 221618.22, 123048.02, 139916.67, 203167.66, 171386.94, 115496.59, 111386.69, 152584.83, 139376.25, 152274.83, 153187.16, 190880.97, 174971.06, 115159.46, 168242.28, 169151.67, 221210.72, 118884.73, 177810.06, 146511.69, 126711.77, 128538.63, 134325.75, 135565.62, 128208.38, 114559.95, 108149.01, 139169.59, 103799.82, 179516.11, 123799.99, 91579.914, 163751.22, 85622.109, 72779.086, 117119.82, 169215.02, 45770.297, 107229.27, 98754.086, 191824.09, 143846.39, 148512.09, 150637.97, 130061.15, 141136.41, 137039.03, 112797.84, 105878.8, 113839.2, 127390.27, 133139.05, 156056.06, 148287.36, 143132.09, 120658.63, 145556.03, 123242.57, 107306.2, 140140.95, 78066.82, 103100.68, 116219.4, 91373.844, 59374.996, 85836.336, 99300.008, 130605.11, 163263.19, 81014.648, 93973.914, 150054.88, 67893.508, 127834.35, 123478.8, 100327.06, 114283.45, 120625.7, 137982.16, 133180.84, 147345.05, 118785.06, 130047.61, 119223.16, 151327.69, 127455.16, 98090.742, 121926.27, 93005.43, 153926.17, 148271.09, 95963.266, 138091.3, 152342.11, 136532.36, 160238.81, 158333.28, 48659.254, 102909.57, 122107.23, 139409.47, 108332.21, 118892.46, 166318.23, 177079.45, 209599.67, 190034.19, 148734.14, 112187.29, 159847.22, 111897.28, 353701.06, 349742.69, 349788.56, 346573.25, 376972.78, 243433.47, 298921.28, 212423.95, 239976.69, 321609.09, 176935.69, 217855.91, 143441.22, 203127.98, 190501.62, 220794.36, 208390.81, 137513.28, 134126.78, 257882.47, 248570.97, 198237.56, 209123.31, 268571.59, 332920.78, 221417.03, 280931.5, 181736.78, 124538.97, 141857.2, 102189.07, 136869.69, 117250.1, 150697.92, 137853.31, 135804.88, 126798.59, 150014.89, 142607.23, 167717.3, 142489.52, 222927.78, 130214.23, 160930.62, 154718.33, 183129.97, 109617.05, 122993.16, 114992.6, 175780.67, 303009.0, 148748.08, 85186.398, 326755.38, 61085.309, 255145.84, 141701.69, 164644.19, 152729.59, 394730.38, 345659.47, 217279.62, 228439.31, 210216.5, 373666.66, 129589.1, 159726.62, 121351.01, 128004.93, 121123.52, 151461.5, 171052.53, 174778.97, 170291.06, 196958.62, 180564.64, 170634.25, 238895.67, 178569.22, 177010.8, 191871.28, 231796.72, 400118.5, 387415.31, 188418.66, 346601.69, 179996.72, 245393.06, 172631.56, 261524.55, 224381.77, 171386.72, 196870.22, 145844.59, 285909.19, 151322.16, 281884.88, 138701.23, 115619.51, 118894.57, 107293.02, 111812.16, 114040.02, 128443.33, 135207.36, 273185.44, 398332.69, 386643.59, 376188.22, 406705.38, 354827.81, 258312.22, 346358.69, 396868.69, 272366.19, 300044.22, 332574.25, 301374.22, 208200.84, 334414.12, 224867.28, 209451.53, 166445.33, 241820.47, 234514.53, 208797.3, 174951.19, 200413.16, 219798.3, 215396.22, 204901.0, 180564.19, 242703.34, 187456.94, 314311.62, 495638.44, 294348.28, 375925.06, 317004.22, 297804.81, 282950.5, 265332.91, 260037.97, 262062.75, 207395.91, 258980.72, 201074.47, 187651.27, 198867.84, 134474.56, 183176.91, 179820.88, 182772.7, 201817.97, 191975.38, 194812.39, 124724.48, 122651.78, 122925.32, 117028.98, 187705.44, 134644.53, 295946.97, 339011.56, 169186.36, 144590.0, 162772.19, 150237.19, 265529.81, 236150.78, 279487.06, 264789.06, 157285.61, 205879.48, 181554.94, 186762.09, 276641.44, 233664.19, 306178.03, 315147.88, 192927.88, 161150.5, 176391.53, 199439.0, 128445.51, 156129.72, 123459.05, 135179.97, 176925.97, 90150.227, 123316.95, 142595.69, 83785.164, 135160.08, 136312.84, 122202.07, 205299.84, 143344.5, 165124.47, 177636.72, 134595.55, 109463.88, 130893.15, 122321.1, 174026.7, 128287.88, 135273.22, 84905.32, 101524.32, 88668.422, 142413.81, 144556.03, 180680.81, 143558.14, 116892.65, 129049.73, 123647.82, 123775.68, 109798.98, 123199.2, 119553.48, 120378.88, 107891.55, 116438.39, 119968.46, 122901.29, 122019.84, 89204.797, 122183.26, 112396.01, 113608.67, 133029.44, 123619.09, 153304.83, 89166.664, 102126.36, 146028.62, 67274.844, 93237.773, 140600.3, 121154.24, 109873.44, 139977.89, 101441.55, 72095.414, 226227.25, 112054.48, 114180.12, 113337.82, 118000.22, 127103.3, 119979.59, 118876.61, 148832.16, 121250.15, 159250.09, 122097.87, 101663.64, 118027.13, 81995.031, 97316.242, 97800.18, 168889.28, 129234.76, 155499.48, 178153.44, 114720.84, 96326.617, 124250.98, 132353.55, 115436.2, 125103.7, 117356.76, 112993.38, 71663.852, 105288.8, 134420.88, 155526.62, 145290.78, 152405.39, 122288.43, 133717.41, 142293.62, 139876.72, 172447.14, 149100.12, 121962.25, 149017.03, 241196.84, 128431.9, 169689.97, 150483.78, 117391.21, 139638.59, 269179.28, 232266.72, 231291.16, 227683.12, 191963.03, 252977.62, 308226.97, 364877.19, 273631.72, 200844.78, 142767.66, 216769.52, 206491.09, 201127.38, 218246.55, 147847.28, 127870.33, 146309.12, 223864.69, 269493.19, 346690.91, 242664.16, 219562.7, 146149.62, 234008.16, 194117.38, 225876.94, 195533.62, 129818.54, 130511.96, 144628.27, 148923.03, 156607.81, 345214.69, 72303.297, 71815.742, 99474.539, 109483.04, 87734.07, 117707.25, 72442.914, 112163.68, 138863.25, 175501.78, 137292.22, 136002.56, 178105.25, 158261.28, 158220.28, 106601.85, 138290.66, 193559.66, 242958.47, 202520.12, 109284.17, 122390.85, 119784.37, 97417.57, 121445.4, 107951.12, 136076.56, 45812.59, 81642.977, 98726.523, 84409.414, 325107.75, 313948.06, 300763.53, 224615.16, 127199.42, 169441.14, 191076.16, 314724.66, 238702.3, 137136.78, 215673.19, 176048.95, 198104.69, 270360.97, 226350.95, 261604.78, 302173.62, 231880.53, 119624.54, 164246.92, 150862.8, 136480.31, 103195.02, 111576.91, 97152.461, 138910.47, 127786.18, 124644.12, 133938.17, 138647.06, 131736.33, 172337.05, 144012.2, 161126.19, 189619.34, 187295.61, 238842.09, 166576.45, 167919.28, 164093.52, 190309.41, 221756.66, 394426.69, 356194.75, 188780.78, 344139.62, 321705.28, 405279.62, 156127.38, 197554.42, 222331.84, 205205.28, 168022.97, 174023.31, 153493.44, 200949.66, 181894.45, 140556.88, 120604.71, 113463.82, 162019.98, 197332.08, 107273.84, 99193.32, 133314.0, 123356.07, 380771.44, 274907.88, 316710.06, 363138.84, 361341.41, 405820.56, 404773.56, 376543.91, 360213.69, 246933.97, 367411.94, 372408.47, 326657.72, 286377.69, 333049.94, 251186.03, 224883.97, 234719.75, 171967.08, 174898.38, 192389.0, 211752.22, 317191.75, 207818.25, 200500.2, 208170.94, 179224.22, 194550.94, 176421.42, 206723.94, 191674.64, 173959.47, 184164.11, 183895.5, 263541.69, 180024.38, 214341.25, 203094.72, 213198.81, 196608.75, 215809.98, 227163.2, 185069.19, 176030.77, 314176.47, 434757.31, 319245.88, 305275.91, 365467.91, 305336.97, 194451.38, 273900.31, 232251.31, 349324.62, 229508.59, 227067.3, 224415.19, 226407.31, 231131.62, 205410.14, 192219.05, 223279.09, 193412.94, 331917.53, 260200.22, 254147.84, 249513.28, 122986.62, 140707.89, 167523.25, 195956.66, 194322.72, 148468.38, 111053.96, 136747.39, 255875.44, 139719.86, 175356.14, 216666.88, 180014.53, 198945.88, 229942.78, 203878.03, 140198.89, 152116.03, 178266.78, 249128.84, 255578.34, 243441.88, 266663.06, 326386.38, 129449.51, 206409.44, 151982.06, 164381.44, 199344.38, 195308.05, 226161.44, 148728.55, 136181.47, 134070.62, 89203.961, 120975.35, 144990.17, 127243.96, 107001.56, 157946.03, 139539.14, 210693.75, 145932.75, 217491.69, 133256.09, 61130.25, 63096.566, 111415.84, 122413.87, 137155.75, 123276.73, 157068.5, 151421.42, 119001.02, 124610.13, 129129.77, 194161.06, 120999.26, 153397.08, 127947.01, 143242.77, 122344.62, 122517.21, 118909.1, 118934.67, 111128.16, 136411.33, 143954.11, 119948.52, 102442.51, 142952.42, 280433.5, 143718.53, 114381.02, 171809.12, 113922.6, 140390.17, 130378.51, 139500.72, 135848.41, 134380.95, 156874.95, 88951.82, 104675.74, 127476.05, 102355.52, 119464.13, 116022.46, 103045.21, 125508.62, 129528.79, 88583.328, 110374.85, 189454.92, 150165.42, 102696.12, 148860.97, 117792.32, 224968.81, 86781.164, 108040.13, 116166.63, 124814.95, 121392.27, 143905.41, 87504.938, 139536.53, 99372.031, 125248.58, 105100.07, 170179.69, 119656.16, 110539.65, 164661.16, 86602.852, 101230.77, 197364.59, 193019.91, 179128.62, 113304.59, 93538.062, 227366.27, 119099.99, 128211.66, 145037.72, 120726.27, 162212.36, 122429.18, 136655.94, 115379.93, 116551.9, 108815.8, 150654.84, 241579.62, 164711.62, 173402.25, 145555.0, 70859.562, 168805.91, 146535.31, 143213.47, 110848.63, 243539.34, 132465.53, 114470.2, 93641.453, 108622.54, 126904.78, 154988.14, 88383.445, 199347.03, 218395.62, 254077.28, 268156.25, 256464.78, 225515.72, 225847.88, 184090.47, 227299.62, 207943.91, 264659.72, 151872.0, 179257.55, 130642.06, 145080.55, 233715.19, 205791.8, 195455.06, 216453.53, 136509.44, 114181.77, 118413.91, 142811.27, 124939.16, 125409.23, 143632.22, 118872.12, 246008.81, 223347.38, 196900.19, 263685.47, 269086.34, 218911.22, 253193.97, 189821.53, 196780.47, 183475.33, 191118.66, 159637.36, 148125.61, 115341.21, 141781.84, 136108.67, 132789.81, 143426.72, 170696.0, 509410.97, 138892.41, 120304.31, 69134.133, 97370.508, 122255.4, 107069.38, 113257.12, 143842.89, 124557.45, 130100.84, 125006.2, 134490.28, 142893.03, 181027.2, 132478.94, 121015.16, 119727.76, 201661.86, 194789.81, 108518.11, 170286.81, 162546.5, 212150.5, 268988.22, 121939.68, 114155.16, 137367.97, 85452.539, 60374.809, 139565.17, 136683.28, 127790.99, 256162.84, 164313.56, 198281.22, 218934.16, 196831.16, 134113.41, 150178.31, 198727.42, 222949.16, 206010.02, 267009.34, 171338.73, 229366.31, 287108.47, 211015.19, 317199.16, 383380.25, 122120.29, 112561.23, 96810.742, 99795.422, 94391.555, 93859.961, 138202.41, 205738.09, 211038.06, 140899.81, 108539.29, 154787.8, 167626.58, 130345.04, 129318.34, 140173.44, 144920.02, 189270.88, 198723.69, 191072.89, 171617.28, 181843.69, 183866.59, 240550.91, 255992.94, 260709.19, 157079.52, 152647.2, 430845.75, 437883.81, 351742.56, 383833.0, 406486.91, 335716.56, 412629.31, 154647.72, 190781.22, 242943.64, 255279.72, 204145.78, 149165.78, 106948.04, 196270.89, 107133.15, 117910.01, 103770.45, 103037.59, 106913.66, 130160.57, 136323.53, 130552.0, 125584.23, 373373.44, 258331.22, 253967.94, 364607.81, 326054.47, 330909.44, 316488.56, 332580.22, 350123.41, 346638.53, 377922.88, 266709.75, 311120.53, 331180.88, 267140.25, 176459.14, 189673.56, 177525.2, 275341.0, 188217.59, 187205.94, 198953.16, 207219.58, 187607.66, 195411.38, 197707.06, 274668.09, 294650.91, 301511.12, 513963.53, 320642.06, 583200.19, 342101.62, 320857.0, 239254.86, 292649.41, 215206.38, 200294.55, 411690.12, 188363.09, 127199.27, 198208.3, 131151.12, 193788.34, 188389.66, 199357.34, 188479.16, 182807.55, 158875.62, 167522.34, 118875.02, 128935.12, 138937.05, 126235.93, 115836.07, 114753.18, 132963.22, 109450.12, 130315.76, 338397.0, 436288.97, 160889.12, 151299.2, 152223.0, 149360.41, 187159.45, 218354.03, 152482.7, 156785.66, 139683.28, 162557.84, 158224.5, 131797.48, 132122.64, 139937.0, 186540.25, 157928.41, 145992.47, 134122.95, 129937.79, 115784.2, 163801.17, 147803.38, 132211.98, 144775.5, 126322.39, 127274.69, 172642.67, 126768.82, 143078.73, 152311.73, 135266.61, 154710.11, 153283.39, 145634.7, 145699.69, 113037.42, 132761.25, 120719.61, 140053.33, 232876.5, 165698.44, 243571.84, 119143.12, 98077.945, 96314.242, 96509.195, 144930.28, 138347.06, 145093.42, 148581.89, 200450.09, 162883.02, 264171.03, 123321.01, 65197.621, 112410.84, 131142.64, 140620.88, 118004.02, 116662.23, 153653.77, 123438.1, 123128.8, 144920.91, 131729.5, 118113.68, 107170.66, 114939.54, 94767.398, 96407.891, 89474.016, 119825.87, 107968.55, 79429.953, 118926.96, 78584.438, 175435.52, 94539.711, 117997.63, 65369.734, 165796.78, 77747.672, 106098.34, 102236.35, 213784.62, 90500.344, 105383.37, 89611.43, 115664.1, 133544.78, 152622.55, 151286.64, 103684.13, 93926.492, 140253.53, 148926.89, 130318.6, 131657.08, 147850.31, 149641.03, 169610.16, 169507.19, 116232.71, 220706.06, 139339.98, 129673.09, 138026.58, 143111.66, 125917.73, 196444.44, 312915.12, 176161.53, 142224.91, 129399.74, 132568.59, 254266.17, 214357.72, 223503.34, 191399.41, 246363.75, 328510.31, 229211.09, 244204.38, 197009.73, 166896.73, 140550.88, 171864.06, 190957.31, 203772.33, 221547.34, 155537.09, 147824.12, 113520.3, 208872.69, 194358.44, 220824.12, 213310.72, 270744.56, 227711.94, 235834.31, 227229.84, 146884.25, 212205.81, 202324.75, 195349.56, 209822.94, 129457.95, 137222.2, 148309.33, 205991.66, 127511.96, 255033.55, 141857.2, 163025.7, 91946.094, 115711.1, 113802.4, 125456.32, 101805.66, 68233.148, 108684.57, 120448.15, 128722.96, 169090.44, 114088.99, 157410.66, 139312.05, 106474.9, 157401.58, 150531.41, 159746.03, 213732.56, 173479.09, 192730.14, 107230.68, 120262.71, 77839.93, 83954.445, 125099.55, 64528.27, 86066.195, 68319.461, 332808.09, 306367.09, 226697.78, 147441.19, 240942.97, 149427.48, 243624.92, 194827.44, 324795.0, 332758.09, 88825.883, 227079.22, 101374.15, 134471.84, 151623.97, 84757.82, 96759.93, 141497.27, 101622.71, 88694.609, 89597.586, 96948.461, 159829.89, 115597.61, 230775.23]\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "# estimators = []\n",
    "# estimators.append(('linear',classifierLinearRegression))\n",
    "# estimators.append(('svr',classifierSVR))\n",
    "# estimators.append(('rf',classifierRandomForestRegressor))\n",
    "\n",
    "# # # create the ensemble model\n",
    "# ensemble = VotingClassifier(estimators,voting='hard')\n",
    "# ensemble = ensemble.fit(x_train,y_train)\n",
    "# pred = []\n",
    "# pred = np.array(pred,dtype='float64')\n",
    "# pred = ensemble.predict(scaler.transform(data_test_less_features))\n",
    "# result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "# result.to_csv('pred_EnsembleVoting.csv', columns=['SalePrice'])\n",
    "\n",
    "\n",
    "#Emsemble via stacking\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "stregr = StackingRegressor(regressors=[classifierLinearRegression,classifierSVR, classifierRandomForestRegressor,classifierGBR], \n",
    "                           meta_regressor=classifierLinearRegression)\n",
    "stregr.fit(x_train,y_train)\n",
    "pred = []\n",
    "pred = np.array(pred)\n",
    "pred = stregr.predict(scaler.transform(data_test_less_features))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_EnsembleStacker.csv', columns=['SalePrice'])\n",
    "\n",
    "#Emsemble via avereging\n",
    "dnn=[]\n",
    "dnn = list(classifierDNN.predict(scaler.transform(data_test_less_features),as_iterable=True))\n",
    "print(dnn)\n",
    "final_labels = (\n",
    "                (classifierLinearRegression.predict(scaler.transform(data_test_less_features))) + \n",
    "                (classifierSVR.predict(scaler.transform(data_test_less_features))) +\n",
    "                (classifierRandomForestRegressor.predict(scaler.transform(data_test_less_features)))+\n",
    "                (classifierGBR.predict(scaler.transform(data_test_less_features))) +\n",
    "                dnn\n",
    "               \n",
    "               ) / 5\n",
    "\n",
    "## Saving to CSV\n",
    "pd.DataFrame({'Id': range(1461,2920), 'SalePrice': final_labels}).to_csv('pred_EnsembleAvereging.csv', index =False)  \n",
    "\n",
    "print(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
