{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n",
      "(1459, 80)\n",
      "(1445, 66)\n",
      "(1459, 65)\n",
      "Id                 int64\n",
      "MSSubClass        object\n",
      "LotFrontage      float64\n",
      "LotArea            int64\n",
      "Street            object\n",
      "Alley             object\n",
      "LotShape          object\n",
      "LandContour       object\n",
      "LotConfig         object\n",
      "LandSlope         object\n",
      "Neighborhood      object\n",
      "Condition1        object\n",
      "BldgType          object\n",
      "HouseStyle        object\n",
      "OverallQual        int64\n",
      "OverallCond       object\n",
      "YearBuilt          int64\n",
      "YearRemodAdd       int64\n",
      "RoofStyle         object\n",
      "MasVnrType        object\n",
      "ExterQual         object\n",
      "ExterCond         object\n",
      "Foundation        object\n",
      "BsmtQual          object\n",
      "BsmtCond          object\n",
      "BsmtExposure      object\n",
      "BsmtFinType1      object\n",
      "BsmtFinSF1       float64\n",
      "BsmtFinType2      object\n",
      "BsmtFinSF2       float64\n",
      "                  ...   \n",
      "2ndFlrSF           int64\n",
      "LowQualFinSF       int64\n",
      "GrLivArea          int64\n",
      "BsmtFullBath       int64\n",
      "BsmtHalfBath       int64\n",
      "FullBath           int64\n",
      "HalfBath           int64\n",
      "BedroomAbvGr       int64\n",
      "KitchenAbvGr      object\n",
      "KitchenQual       object\n",
      "TotRmsAbvGrd       int64\n",
      "Fireplaces         int64\n",
      "FireplaceQu       object\n",
      "GarageType        object\n",
      "GarageFinish      object\n",
      "GarageCars         int64\n",
      "GarageArea         int64\n",
      "GarageCond        object\n",
      "PavedDrive        object\n",
      "WoodDeckSF         int64\n",
      "OpenPorchSF        int64\n",
      "EnclosedPorch      int64\n",
      "3SsnPorch          int64\n",
      "ScreenPorch        int64\n",
      "PoolArea           int64\n",
      "Fence             object\n",
      "MoSold            object\n",
      "YrSold            object\n",
      "SaleCondition     object\n",
      "SalePrice          int64\n",
      "dtype: object\n",
      "Null values treino \n",
      " Index([], dtype='object')\n",
      "Null values test \n",
      " Index([], dtype='object')\n",
      "New shape train: (1445, 249)\n",
      "Indice da coluna SalePrice no novo dataset 235\n",
      "New shape test: (1459, 248)\n",
      "Colunas que existem apenas teste :  Index([], dtype='object')\n",
      "Colunas que existem apenas treino :  Index(['SalePrice'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def remove_categorical_columns(df):\n",
    "    df.drop('MSZoning',axis=1,inplace=True)\n",
    "    df.drop('Street',axis=1,inplace=True)\n",
    "    df.drop('Alley',axis=1,inplace=True)\n",
    "    df.drop('LotShape',axis=1,inplace=True)\n",
    "    df.drop('LandContour',axis=1,inplace=True)\n",
    "    df.drop('Utilities',axis=1,inplace=True)\n",
    "    df.drop('LotConfig',axis=1,inplace=True)\n",
    "    df.drop('LandSlope',axis=1,inplace=True)\n",
    "    df.drop('Neighborhood',axis=1,inplace=True)\n",
    "    df.drop('Condition1',axis=1,inplace=True)\n",
    "    df.drop('Condition2',axis=1,inplace=True)\n",
    "    df.drop('BldgType',axis=1,inplace=True)\n",
    "    df.drop('HouseStyle',axis=1,inplace=True)\n",
    "    df.drop('RoofStyle',axis=1,inplace=True)\n",
    "    df.drop('RoofMatl',axis=1,inplace=True)\n",
    "    df.drop('Exterior1st',axis=1,inplace=True)\n",
    "    df.drop('Exterior2nd',axis=1,inplace=True)\n",
    "    df.drop('MasVnrType',axis=1,inplace=True)\n",
    "    df.drop('ExterQual',axis=1,inplace=True)\n",
    "    df.drop('ExterCond',axis=1,inplace=True)\n",
    "    df.drop('Foundation',axis=1,inplace=True)\n",
    "    df.drop('BsmtQual',axis=1,inplace=True)\n",
    "    df.drop('BsmtCond',axis=1,inplace=True)\n",
    "    df.drop('BsmtExposure',axis=1,inplace=True)\n",
    "    df.drop('BsmtFinType1',axis=1,inplace=True)\n",
    "    df.drop('BsmtFinType2',axis=1,inplace=True)\n",
    "    df.drop('Heating',axis=1,inplace=True)\n",
    "    df.drop('HeatingQC',axis=1,inplace=True)\n",
    "    df.drop('CentralAir',axis=1,inplace=True)\n",
    "    df.drop('Electrical',axis=1,inplace=True)\n",
    "    df.drop('KitchenQual',axis=1,inplace=True)\n",
    "    df.drop('Functional',axis=1,inplace=True)\n",
    "    df.drop('FireplaceQu',axis=1,inplace=True)\n",
    "    df.drop('GarageType',axis=1,inplace=True)\n",
    "    df.drop('GarageFinish',axis=1,inplace=True)\n",
    "    df.drop('GarageQual',axis=1,inplace=True)\n",
    "    df.drop('GarageCond',axis=1,inplace=True)\n",
    "    df.drop('PavedDrive',axis=1,inplace=True)\n",
    "    df.drop('PoolQC',axis=1,inplace=True)\n",
    "    df.drop('Fence',axis=1,inplace=True)\n",
    "    df.drop('MiscFeature',axis=1,inplace=True)\n",
    "    df.drop('SaleType',axis=1,inplace=True)\n",
    "    df.drop('SaleCondition',axis=1,inplace=True)\n",
    "\n",
    "def input_missing_value(df):\n",
    "    \n",
    "    # MSSubClass as str\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype(\"str\")\n",
    "    #After converting this column to String, it will be handled as categorical\n",
    "    #There is one value in the test set that there isn't in the training. It is 150\n",
    "    #It will be result in one column for this categorical value that doesn't exist in the training set\n",
    "    #It can't happen\n",
    "    #There is only one value 150 in the row 1358 in the test set\n",
    "    #We also can't remove any single row from the test set as we will need make predictions for all rows \n",
    "    #Let's just pass the value 150 to 40 , as this value exists in booth sets and is is the less common \n",
    "    df['MSSubClass'][df.MSSubClass=='150']='40'\n",
    "    \n",
    "    # Converting OverallCond to str\n",
    "    df.OverallCond = df.OverallCond.astype(\"str\")\n",
    "    \n",
    "    # KitchenAbvGr to categorical\n",
    "    df['KitchenAbvGr'] = df['KitchenAbvGr'].astype(\"str\")\n",
    "    df.drop(df[df.KitchenAbvGr=='3'].index,inplace=True) # apenas no treino\n",
    "    \n",
    "    # Year and Month to categorical\n",
    "    df['YrSold'] = df['YrSold'].astype(\"str\")\n",
    "    df['MoSold'] = df['MoSold'].astype(\"str\")    \n",
    "    \n",
    "    #LotFrontage - insert the mean \n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=1)\n",
    "    #print(np.shape(df['LotFrontage']))\n",
    "    df['LotFrontage'] = imp.fit_transform(df['LotFrontage']).transpose()    \n",
    "   \n",
    "    #Alley\n",
    "    df.Alley.fillna(inplace=True,value='No')\n",
    "\n",
    "    #MasVnrType - remove the records where the value is NA \n",
    "    #print(\"Number of lines where MasVnrType has Nan value\", len(df[df['MasVnrType'].isnull()]))\n",
    "    #df.dropna(axis=0,subset=['MasVnrType'],inplace=True)\n",
    "    #df.drop('MasVnrType',axis=1,inplace=True)\n",
    "    # MasVnrType NA in all. filling with most popular values\n",
    "    df.MasVnrType.fillna(value=df['MasVnrType'].mode()[0],inplace=True)\n",
    "    \n",
    "    #MasVnrArea - remove the hole column\n",
    "    df.drop('MasVnrArea',axis=1,inplace=True)\n",
    "    \n",
    "    #Condition2 - remove the hole column Possui quantidade de tipos diferentes na base de treino e teste e apenas \n",
    "    #um dos tipos é relevante    \n",
    "    df.drop('Condition2',axis=1,inplace=True)\n",
    "    \n",
    "    #RoofMatl - remove the hole column Possui quantidade de tipos diferentes na base de treino e teste e apenas \n",
    "    #um dos tipos é relevante    \n",
    "    df.drop('RoofMatl',axis=1,inplace=True)\n",
    "    \n",
    "\n",
    "    #MSZoning   - tem NA apenas na base de teste. Como nao posso remover linhas removo a coluna   \n",
    "    #df.dropna(axis=0,subset=['MSZoning'],inplace=True)\n",
    "    df.drop('MSZoning',axis=1,inplace=True)\n",
    "    #df.MSZoning.fillna(df['MSZoning'].mode()[0])\n",
    "\n",
    "    \n",
    "    #BsmtQual\n",
    "    df.BsmtQual.fillna(inplace=True,value='No')\n",
    "    \n",
    "    #HouseStyle - Esse valor so existe na base de treino. Ao inves de remover toda coluna removo somente as linhas \n",
    "    df.drop(df[df.HouseStyle=='2.5Fin'].index,inplace=True)\n",
    "    \n",
    "    #BsmtCond\n",
    "    df.BsmtCond.fillna(inplace=True,value='No')\n",
    "\n",
    "    #BsmtExposure\n",
    "    df.BsmtExposure.fillna(inplace=True,value='No')\n",
    "\n",
    "    #BsmtFinType1\n",
    "    df.BsmtFinType1.fillna(inplace=True,value='No')\n",
    "\n",
    "    #BsmtFinType2\n",
    "    df.BsmtFinType2.fillna(inplace=True,value='No')\n",
    "\n",
    "    #Electrical - remove the records where the value is NA\n",
    "    #print(\"Number of lines where Electrical has Nan value\",len(df[df['Electrical'].isnull()]))\n",
    "    df.dropna(axis=0,subset=['Electrical'],inplace=True) # apenas no treino \n",
    "    df.drop(df[df.Electrical=='Mix'].index,inplace=True) # apenas no treino\n",
    "    #print(\"Number of lines where Electrical has Nan value\",len(df[df['Electrical'].isnull()]))\n",
    "\n",
    "    #FireplaceQu\n",
    "    df.FireplaceQu.fillna(inplace=True,value='No')\n",
    "    \n",
    "\n",
    "    #GarageType\n",
    "    df.GarageType.fillna(inplace=True,value='No')\n",
    "\n",
    "    #GarageYrBlt - remove the hole column\n",
    "    df.drop('GarageYrBlt',axis=1,inplace=True)\n",
    "\n",
    "    #GarageFinish\n",
    "    df.GarageFinish.fillna(inplace=True,value='No')\n",
    "\n",
    "    #GarageQual - A base de teste nao tem um dos tipos presente na base de treino. Assim a base de treino terá uma \n",
    "    #feature para esse tipo e a de teste não. Alem disso, apenas um tipo é pertinente\n",
    "    #Achei melhor entao excluir essa coluna    \n",
    "    df.drop('GarageQual',axis=1,inplace=True)\n",
    "    #df.drop(df[df.GarageQual=='Ex'].index,inplace=True)\n",
    "    \n",
    "    #GarageCond\n",
    "    df.GarageCond.fillna(inplace=True,value='No')\n",
    "\n",
    "    #PoolQC\n",
    "    #df.PoolQC.fillna(inplace=True,value='No')\n",
    "    df.drop('PoolQC',axis=1,inplace=True)\n",
    "    \n",
    "    #Fence\n",
    "    df.Fence.fillna(inplace=True,value='No')\n",
    "\n",
    "    #MiscFeature\n",
    "    #df.MiscFeature.fillna(inplace=True,value='No')\n",
    "    df.drop('MiscFeature',axis=1,inplace=True)\n",
    "\n",
    "    #MiscVal\n",
    "    df.drop('MiscVal',axis=1,inplace=True)\n",
    "    \n",
    "    #SaleType\n",
    "    df.drop('SaleType',axis=1,inplace=True)\n",
    "    \n",
    "    #Exterior1st- nao posso remover linhas do teste\n",
    "    #df.dropna(axis=0,subset=['Exterior1st'],inplace=True)     \n",
    "    #df.drop(df[df.Exterior1st=='Stone'].index,inplace=True)\n",
    "    #df.drop(df[df.Exterior1st=='ImStucc'].index,inplace=True)\n",
    "    #df.drop(df[df.Exterior1st=='CBlock'].index,inplace=True)\n",
    "    df.drop('Exterior1st',axis=1,inplace=True)\n",
    "    \n",
    "    #Exterior2nd\n",
    "    #df.dropna(axis=0,subset=['Exterior2nd'],inplace=True)\n",
    "    #df.Exterior2nd.fillna(inplace=True,value= 'Other')\n",
    "    #df.drop(df[df.Exterior2nd=='Other'].index,inplace=True)\n",
    "    #df.drop(df[df.Exterior2nd=='CBlock'].index,inplace=True)\n",
    "    df.drop('Exterior2nd',axis=1,inplace=True)\n",
    "    \n",
    "    #Heating -- esses tipos existem apenas na base de treino\n",
    "    df.drop(df[df.Heating=='OthW'].index,inplace=True)\n",
    "    df.drop(df[df.Heating=='Floor'].index,inplace=True)\n",
    "    \n",
    "    #KitchenQual\n",
    "    #df.dropna(axis=0,subset=['KitchenQual'],inplace=True)\n",
    "    df.KitchenQual.fillna(inplace=True,value='Fa') #- Apenas a base de teste tem NA e como nao posso remover registro\n",
    "    #dessa base setei o valor menos comum\n",
    "    \n",
    "    #Functional\n",
    "    #df.dropna(axis=0,subset=['Functional'],inplace=True)\n",
    "    df.drop('Functional',axis=1,inplace=True)\n",
    "    \n",
    "    #Utilities\n",
    "    df.drop('Utilities',axis=1,inplace=True)\n",
    "    \n",
    "    #BsmtFinSF1\n",
    "    #df.dropna(axis=0,subset=['BsmtFinSF1'],inplace=True)\n",
    "    df['BsmtFinSF1'] = imp.fit_transform(df['BsmtFinSF1']).transpose()    \n",
    "    \n",
    "    #BsmtFinSF2\n",
    "    #df.dropna(axis=0,subset=['BsmtFinSF2'],inplace=True)\n",
    "    df['BsmtFinSF2'] = imp.fit_transform(df['BsmtFinSF2']).transpose()    \n",
    "    \n",
    "    #BsmtUnfSF\n",
    "    #df.dropna(axis=0,subset=['BsmtUnfSF'],inplace=True)\n",
    "    df.drop('BsmtUnfSF',axis=1,inplace=True)\n",
    "    \n",
    "    #TotalBsmtSF\n",
    "    #df.dropna(axis=0,subset=['TotalBsmtSF'],inplace=True)\n",
    "    #df['TotalBsmtSF'] = imp.fit_transform(df['TotalBsmtSF']).transpose()    \n",
    "    df.TotalBsmtSF.fillna(value=0,inplace=True)\n",
    "    \n",
    "    #BsmtFullBath - apenas na base de teste tem NA.Nao posso remover a linha\n",
    "    df.BsmtFullBath.fillna(inplace=True,value=0)\n",
    "    \n",
    "    #BsmtHalfBath- apenas na base de teste tem NA.Nao posso remover a linha\n",
    "    df.BsmtHalfBath.fillna(inplace=True,value=0)\n",
    "    \n",
    "    #GarageCars\n",
    "    #df.dropna(axis=0,subset=['GarageCars'],inplace=True)\n",
    "    df.GarageCars.fillna(value=0,inplace=True)\n",
    "    \n",
    "    #GarageArea\n",
    "    #df.dropna(axis=0,subset=['GarageArea'],inplace=True)\n",
    "    df.GarageArea.fillna(value=0,inplace=True)\n",
    "    \n",
    "df = pd.read_csv(\"train.csv\",na_values=['?','NA'],delimiter=',',delim_whitespace=False)\n",
    "df_test = pd.read_csv(\"test.csv\",na_values=['?','NA'],delimiter=',',delim_whitespace=False)\n",
    "\n",
    "print(df.shape)\n",
    "print(df_test.shape)\n",
    "#print(df.head())\n",
    "#print(df.describe())\n",
    "#print(df.dtypes)\n",
    "#df = df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "########################Dealing with missing values\n",
    "\n",
    "#missing data\n",
    "# total = df.isnull().sum().sort_values(ascending=False)\n",
    "# percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "# print(missing_data.head(20))\n",
    "\n",
    "# \n",
    "#               Total   Percent\n",
    "# PoolQC         1453  0.995205\n",
    "# MiscFeature    1406  0.963014\n",
    "# Alley          1369  0.937671\n",
    "# Fence          1179  0.807534\n",
    "# FireplaceQu     690  0.472603\n",
    "# LotFrontage     259  0.177397\n",
    "# GarageCond       81  0.055479\n",
    "# GarageType       81  0.055479\n",
    "# GarageYrBlt      81  0.055479\n",
    "# GarageFinish     81  0.055479\n",
    "# GarageQual       81  0.055479\n",
    "# BsmtExposure     38  0.026027\n",
    "# BsmtFinType2     38  0.026027\n",
    "# BsmtFinType1     37  0.025342\n",
    "# BsmtCond         37  0.025342\n",
    "# BsmtQual         37  0.025342\n",
    "# MasVnrArea        8  0.005479\n",
    "# MasVnrType        8  0.005479\n",
    "# Electrical        1  0.000685\n",
    "# Utilities         0  0.000000\n",
    "\n",
    "\n",
    "\n",
    "#print(df.columns[df.isnull().any()])\n",
    "#'LotFrontage', 'Alley', 'MasVnrType', 'MasVnrArea', 'BsmtQual',\n",
    "#       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "#       'Electrical', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n",
    "#       'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence',\n",
    "#       'MiscFeature'\n",
    "input_missing_value(df)\n",
    "\n",
    "\n",
    "#print(df_test.columns[df_test.isnull().any()])\n",
    "#Index(['MSZoning', 'LotFrontage', 'Alley', 'Utilities', 'Exterior1st',\n",
    "#       'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond',\n",
    "#       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n",
    "#      'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n",
    "#       'BsmtHalfBath', 'KitchenQual', 'Functional', 'FireplaceQu',\n",
    "#      'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea',\n",
    "#       'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature',\n",
    "#       'SaleType'],\n",
    "\n",
    "input_missing_value(df_test)\n",
    "\n",
    "\n",
    "#Valores numericos que continham NA sao detectados como String. Assim, depois que removemos o NA temos que setar corretamente \n",
    "#o tipo \n",
    "df_test.BsmtFullBath = df_test.BsmtFullBath.astype(\"int64\")\n",
    "df_test.BsmtHalfBath = df_test.BsmtHalfBath.astype(\"int64\")\n",
    "df_test.GarageCars = df_test.GarageCars.astype(\"int64\")\n",
    "df_test.GarageArea = df_test.GarageArea.astype(\"int64\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df_test.shape)\n",
    "print(df.dtypes)\n",
    "print(\"Null values treino \\n\", df.columns[df.isnull().any()])\n",
    "print(\"Null values test \\n\", df_test.columns[df_test.isnull().any()])\n",
    "\n",
    "########################End dealing with missing values\n",
    "\n",
    "\n",
    "# The OneHotEncoder converts features represented as numeric codes (so they are values that can't be ordered)\n",
    "#to their binary representation\n",
    "#enc = preprocessing.OneHotEncoder() \n",
    "#df = enc.fit_transform(df)\n",
    "\n",
    "\n",
    "########################Tratando campos nominais\n",
    "\n",
    "vec = DictVectorizer()\n",
    "aux = np.asmatrix(vec.fit_transform(df.transpose().to_dict().values()).toarray())\n",
    "\n",
    "data_train = pd.DataFrame(aux,columns=vec.feature_names_)\n",
    "#data_train = pd.get_dummies(df)\n",
    "\n",
    "\n",
    "data_train.to_csv('train_no_categorical.csv')\n",
    "\n",
    "print(\"New shape train:\" , np.shape(data_train))\n",
    "print(\"Indice da coluna SalePrice no novo dataset\" , data_train.columns.get_loc('SalePrice'))\n",
    "\n",
    "################################################# Base de teste\n",
    "\n",
    "vec = DictVectorizer()\n",
    "aux_test = vec.fit_transform(df_test.transpose().to_dict().values()).toarray()\n",
    "data_test = pd.DataFrame(aux_test,columns=vec.feature_names_)\n",
    "#data_test = pd.get_dummies(df_test)\n",
    " \n",
    "print(\"New shape test:\" , np.shape(data_test))\n",
    "\n",
    "data_test.to_csv('test_no_categorical.csv')\n",
    "\n",
    "\n",
    "print(\"Colunas que existem apenas teste : \" , data_test.columns.difference(data_train.columns))\n",
    "print(\"Colunas que existem apenas treino : \" , data_train.columns.difference(data_test.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count      1445.000000\n",
      "mean     181043.215225\n",
      "std       79195.218195\n",
      "min       34900.000000\n",
      "25%      130000.000000\n",
      "50%      163000.000000\n",
      "75%      214000.000000\n",
      "max      755000.000000\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAAPUCAYAAABMx1tcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f545c250860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAAFoCAYAAADzZ0kIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiSpEoYOSZJUCUOHJEmqhKFDkiRV4v8DcZ61JHR3uyoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f545c0f59b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhMAAAFoCAYAAADtrnm7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkooYJiRJUpH/D2/J0Sp5ix+jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5428f79128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAAFoCAYAAADzZ0kIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJElSEf8PXu1sVKaiyEEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5428daa2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFoCAYAAADUycjgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5428e5d908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg8AAAFoCAYAAADQPBjdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5467468c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "print(data_train.SalePrice.describe())\n",
    "\n",
    "saleprice_scaled = preprocessing.StandardScaler().fit_transform((data_train['SalePrice'][:,np.newaxis]));\n",
    "fig = plt.figure(1, figsize=(6, 12))\n",
    "#ax = fig.add_subplot(111)\n",
    "#ax.boxplot(saleprice_scaled)\n",
    "plt.boxplot(saleprice_scaled)\n",
    "\n",
    "plt.figure()\n",
    "x = plt.hist(data_train['SalePrice'],bins=50)\n",
    "\n",
    "plt.figure()\n",
    "saleprice_log = np.log(data_train['SalePrice'])\n",
    "x = plt.hist(saleprice_log,bins=50)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "x = plt.hist(data_train['LotArea'],bins=50)\n",
    "\n",
    "plt.figure()\n",
    "saleprice_log = np.log(data_train['LotArea'])\n",
    "x = plt.hist(saleprice_log,bins=50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "x = plt.hist(data_train['GarageCars'],bins=50)\n",
    "\n",
    "# plt.figure()\n",
    "# saleprice_log = np.log(data_train['GarageArea'])\n",
    "# x = plt.hist(saleprice_log,bins=50)\n",
    "\n",
    "\n",
    "#data_train['SalePrice'] = np.log(data_train['SalePrice'])\n",
    "data_train['OverallQual'] = np.log(data_train['OverallQual'])\n",
    "data_train['LotArea'] = np.log(data_train['LotArea'])\n",
    "\n",
    "\n",
    "data_test['OverallQual'] = np.log(data_test['OverallQual'])\n",
    "data_test['LotArea'] = np.log(data_test['LotArea'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df,name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name,x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tentativa de selecionar melhores features \n",
      "\n",
      "\n",
      "As features selecionadas com Tree-based feature selection foram: \n",
      "\n",
      "['ExterQual=TA' 'OverallQual' 'GarageCars' 'BsmtQual=Ex' 'GrLivArea'\n",
      " 'TotalBsmtSF' 'Neighborhood=NoRidge' 'FireplaceQu=No' '1stFlrSF'\n",
      " '2ndFlrSF' 'BsmtFinSF1' 'TotRmsAbvGrd' 'FullBath' 'LotArea'\n",
      " 'MSSubClass=60' 'BedroomAbvGr' 'YearRemodAdd' 'GarageType=Attchd'\n",
      " 'YearBuilt' 'BsmtQual=Gd' 'Fireplaces' 'GarageArea' 'BsmtExposure=Gd'\n",
      " 'KitchenQual=TA' 'KitchenQual=Ex' 'ExterQual=Fa' 'BsmtFullBath']\n",
      "[[ 0.24506076  0.17339574  0.10220639  0.08477404  0.06769452  0.02122161\n",
      "   0.02114328  0.01799789  0.01759197  0.01331804  0.01144122  0.01097446\n",
      "   0.00868255  0.00749924  0.00676658  0.00647457  0.0064394   0.00586092\n",
      "   0.00583216  0.00502513  0.00501331  0.0049933   0.00462131  0.00456835\n",
      "   0.00449937  0.0044832   0.00376397]]\n",
      "\n",
      " New shape train apos Tree-based feature selection: (1445, 26)\n",
      "\n",
      " Fim tentativa selecionar melhores features \n",
      "\n",
      "\n",
      " New shape test apos Tree-based feature selection: (1459, 26)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Tentativa de selecionar melhores features \\n\")\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "#Removing features with low variance\n",
    "#print(\"Original shape: {}\".format(np.shape(df.iloc[:,0:-1])))\n",
    "#sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "#features = sel.fit_transform(df.iloc[:,0:-1])\n",
    "#print(\"Shape apos Removing features with low variance {}\".format(np.shape(features))) #nenhuma foi selecionada \n",
    "#print(\"\\n\")\n",
    "\n",
    "#Tree-based feature selection\n",
    "y_train = (data_train['SalePrice'])\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "\n",
    "print()\n",
    "\n",
    "clf = ExtraTreesRegressor(n_estimators=20)\n",
    "clf = clf.fit(x_train,y_train)\n",
    "data = np.zeros((1,x_train.shape[1])) \n",
    "data = pd.DataFrame(data, columns=x_train.columns)\n",
    "data.iloc[0] = clf.feature_importances_\n",
    "data = data.T.sort_values(df.index[0], ascending=False).T\n",
    "\n",
    "\n",
    "print(\"As features selecionadas com Tree-based feature selection foram: \\n\")\n",
    "yyy = np.asarray((data.columns[0:27]))\n",
    "xxx = np.asarray((data.iloc[:,0:27]))\n",
    "print(yyy)\n",
    "print(xxx)\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "aux = model.transform(x_train)\n",
    "\n",
    "print(\"\\n New shape train apos Tree-based feature selection: {}\".format(aux.shape))\n",
    "\n",
    "print(\"\\n Fim tentativa selecionar melhores features \\n\")\n",
    "\n",
    "\n",
    "data_train_less_features = pd.concat([pd.DataFrame(aux),pd.DataFrame(y_train)],axis=1)\n",
    "data_train_less_features.to_csv('data_train_less_features.csv')\n",
    "\n",
    "\n",
    "aux = model.transform((data_test))\n",
    "data_test_less_features = pd.DataFrame(aux)\n",
    "print(\"\\n New shape test apos Tree-based feature selection: {}\".format(aux.shape))\n",
    "data_test_less_features.to_csv('data_test_less_features.csv')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn   import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caso 1 - Linear Regression \n",
      "Fold #1\n",
      "Fold score (RMSE): 622243187960918912.00\n",
      "Accuracy: -62765513129602993115627520.000\n",
      "Fold #2\n",
      "Fold score (RMSE): 905009122196841856.00\n",
      "Accuracy: -131289954335369637232902144.000\n",
      "Fold #3\n",
      "Fold score (RMSE): 117636420432799248.00\n",
      "Accuracy: -1935378154193937026252800.000\n",
      "Fold #4\n",
      "Fold score (RMSE): 201586936866963200.00\n",
      "Accuracy: -7499709332072572561391616.000\n",
      "Fold #5\n",
      "Fold score (RMSE): 24891.00\n",
      "Accuracy: 0.901\n",
      "\n",
      " Average RMSE: 5.021361800848278e+17\n",
      "\n",
      "\n",
      "\n",
      "SGDRegressor \n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 46234.39\n",
      "Accuracy: 0.653\n",
      "Fold #2\n",
      "Fold score (RMSE): 3193689.37\n",
      "Accuracy: -1633.974\n",
      "Fold #3\n",
      "Fold score (RMSE): 1041515.60\n",
      "Accuracy: -150.710\n",
      "Fold #4\n",
      "Fold score (RMSE): 958658.73\n",
      "Accuracy: -168.609\n",
      "Fold #5\n",
      "Fold score (RMSE): 307935.01\n",
      "Accuracy: -14.108\n",
      "\n",
      " Average RMSE: 1568463.8772219159\n",
      "\n",
      "\n",
      " Less Features\n",
      "Fold #1\n",
      "Fold score (RMSE): 25195.78\n",
      "Accuracy: 0.879\n",
      "Fold #2\n",
      "Fold score (RMSE): 34522.20\n",
      "Accuracy: 0.818\n",
      "Fold #3\n",
      "Fold score (RMSE): 34675.22\n",
      "Accuracy: 0.841\n",
      "Fold #4\n",
      "Fold score (RMSE): 27179.47\n",
      "Accuracy: 0.856\n",
      "Fold #5\n",
      "Fold score (RMSE): 45643.30\n",
      "Accuracy: 0.690\n",
      "\n",
      " Average RMSE: 34208.23123874215\n",
      "\n",
      "\n",
      "\n",
      "SGDRegressor \n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 25378.30\n",
      "Accuracy: 0.878\n",
      "Fold #2\n",
      "Fold score (RMSE): 36372.82\n",
      "Accuracy: 0.798\n",
      "Fold #3\n",
      "Fold score (RMSE): 33540.48\n",
      "Accuracy: 0.852\n",
      "Fold #4\n",
      "Fold score (RMSE): 28021.28\n",
      "Accuracy: 0.847\n",
      "Fold #5\n",
      "Fold score (RMSE): 44504.18\n",
      "Accuracy: 0.705\n",
      "\n",
      " Average RMSE: 34228.11254807786\n"
     ]
    }
   ],
   "source": [
    "#Starting making predictors\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "\n",
    "#Caso 1 - Linear Regression \n",
    "print(\"Caso 1 - Linear Regression \")\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "data_train = data_train.reindex(np.random.permutation(data_train.index))\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "#Normalization\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "classifier = LinearRegression()\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "# The mean squared error\n",
    "#pred = classifier.predict(x_test_scaled)\n",
    "#score = metrics.mean_squared_error(y_test, pred)\n",
    "#print(\"Final, out of sample score (RMSE): {}\".format(score))    \n",
    "\n",
    "# Evaluate success using accuracy\n",
    "#print(\"Final Accuracy: %.3f\" % classifier.score(X=x_test_scaled,y=y_test))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"SGDRegressor \\n\\n\")\n",
    "\n",
    "\n",
    "classifier = SGDRegressor()\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "###########Less features\n",
    "\n",
    "print(\"\\n\\n Less Features\")\n",
    "\n",
    "#Normalization\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "classifierLinearRegression = LinearRegression()\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "    pred = []    \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifierLinearRegression = classifierLinearRegression.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifierLinearRegression.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifierLinearRegression.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "  \n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "# Write the cross-validated prediction\n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifierLinearRegression.predict(scaler.transform(data_test_less_features))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_LinearRegression.csv', columns=['SalePrice'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"SGDRegressor \\n\\n\")\n",
    "\n",
    "classifier = SGDRegressor()\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "Fold #1\n",
      "Fold score (RMSE): 39525.09\n",
      "Accuracy: 0.707\n",
      "Fold #2\n",
      "Fold score (RMSE): 23699.40\n",
      "Accuracy: 0.878\n",
      "Fold #3\n",
      "Fold score (RMSE): 39725.28\n",
      "Accuracy: 0.820\n",
      "Fold #4\n",
      "Fold score (RMSE): 23568.49\n",
      "Accuracy: 0.910\n",
      "Fold #5\n",
      "Fold score (RMSE): 25820.08\n",
      "Accuracy: 0.894\n",
      "\n",
      " Average RMSE: 31381.979449950468\n",
      "\n",
      "\n",
      " Less features \n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 24490.56\n",
      "Accuracy: 0.886\n",
      "Fold #2\n",
      "Fold score (RMSE): 33605.32\n",
      "Accuracy: 0.827\n",
      "Fold #3\n",
      "Fold score (RMSE): 34913.01\n",
      "Accuracy: 0.839\n",
      "Fold #4\n",
      "Fold score (RMSE): 25681.50\n",
      "Accuracy: 0.872\n",
      "Fold #5\n",
      "Fold score (RMSE): 44985.00\n",
      "Accuracy: 0.699\n",
      "\n",
      " Average RMSE: 33559.53943694814\n"
     ]
    }
   ],
   "source": [
    "#Caso 3 - SVM\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import NuSVR\n",
    "\n",
    "print(\"SVM\")\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "data_train = data_train.reindex(np.random.permutation(data_train.index))\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#Normalization\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "x_train = np.ascontiguousarray(x_train)\n",
    "\n",
    "\n",
    "classifier = NuSVR(kernel='linear', C=1e3) #34761.27693615821\n",
    "kf = KFold(5)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "\n",
    "# The mean squared error\n",
    "#pred = classifier.predict(x_test_scaled)\n",
    "#score = metrics.mean_squared_error(y_test, pred)\n",
    "#print(\"Final, out of sample score (RMSE): {}\".format(score))    \n",
    "\n",
    "# Evaluate success using accuracy\n",
    "#print(\"Final Accuracy: %.3f\" % classifier.score(X=x_test_scaled,y=y_test))\n",
    "\n",
    "###########Less features\n",
    "print(\"\\n\\n Less features \\n\")\n",
    "#Normalization\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "x_train = np.ascontiguousarray(x_train)\n",
    "\n",
    "\n",
    "#classifier = SVR(kernel='rbf', C=1e3, gamma=0.1) #66483.84692815947\n",
    "classifierSVR = SVR(kernel='linear', C=1e3) #34761.27693615821\n",
    "#classifier = SVR(kernel='poly', C=1e3, degree=3) #86747.4465877091\n",
    "#classifier = NuSVR(C=1e3) #57249.1589623674\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifierSVR = classifierSVR.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifierSVR.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifierSVR.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "    \n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "# Write the cross-validated prediction\n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifierSVR.predict(scaler.transform(data_test_less_features))\n",
    "\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "\n",
    "\n",
    "result.to_csv('pred_SVR.csv', columns=['SalePrice'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN MLPRegressor\n",
      "Fold #1\n",
      "Fold score (RMSE): 36219.85\n",
      "Accuracy: 0.740\n",
      "Fold #2\n",
      "Fold score (RMSE): 49890.26\n",
      "Accuracy: 0.594\n",
      "Fold #3\n",
      "Fold score (RMSE): 37256.48\n",
      "Accuracy: 0.792\n",
      "Fold #4\n",
      "Fold score (RMSE): 57485.03\n",
      "Accuracy: 0.533\n",
      "Fold #5\n",
      "Fold score (RMSE): 38793.61\n",
      "Accuracy: 0.762\n",
      "\n",
      " Average RMSE: 44717.857481245104\n",
      "\n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 27091.87\n",
      "Accuracy: 0.861\n",
      "Fold #2\n",
      "Fold score (RMSE): 39778.14\n",
      "Accuracy: 0.758\n",
      "Fold #3\n",
      "Fold score (RMSE): 38260.95\n",
      "Accuracy: 0.807\n",
      "Fold #4\n",
      "Fold score (RMSE): 25655.44\n",
      "Accuracy: 0.872\n",
      "Fold #5\n",
      "Fold score (RMSE): 48460.65\n",
      "Accuracy: 0.651\n",
      "\n",
      " Average RMSE: 36842.38250360394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Shuffle\n",
    "print(\"NN MLPRegressor\")\n",
    "np.random.seed(42)\n",
    "data_train = data_train.reindex(np.random.permutation(data_train.index))\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "#Normalization\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "classifier = MLPRegressor(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10,4,2), random_state=1)\n",
    "kf = KFold(5)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "# The mean squared error\n",
    "#pred = classifier.predict(x_test_scaled)\n",
    "#score = metrics.mean_squared_error(y_test, pred)\n",
    "#print(\"Final, out of sample score (RMSE): {}\".format(score))    \n",
    "\n",
    "# Evaluate success using accuracy\n",
    "#print(\"Final Accuracy: %.3f\" % classifier.score(X=x_test_scaled,y=y_test))\n",
    "\n",
    "###########Less features\n",
    "print(\"\\n\\n\")\n",
    "#Normalization\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "classifier = MLPRegressor(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(8,2), random_state=1)\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests\n",
      "Mean squared error: 29597.05109582514\n",
      "Accuracy: 0.852\n",
      "\n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 28951.730162219337\n",
      "Accuracy: 0.886\n",
      "Fold #2\n",
      "Fold score (RMSE): 25924.39997455174\n",
      "Accuracy: 0.897\n",
      "Fold #3\n",
      "Fold score (RMSE): 32718.1452740622\n",
      "Accuracy: 0.820\n",
      "Fold #4\n",
      "Fold score (RMSE): 30757.611548626795\n",
      "Accuracy: 0.822\n",
      "Fold #5\n",
      "Fold score (RMSE): 27059.08361496614\n",
      "Accuracy: 0.880\n",
      "\n",
      " Average RMSE: 29185.540590707198\n",
      "\n",
      " oob score : 0.8655222916876083\n",
      "\n",
      "\n",
      " Less features \n",
      "\n",
      "\n",
      "Mean squared error: 29747.78358186385\n",
      "Accuracy: 0.857\n",
      "\n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 36294.32835986868\n",
      "Accuracy: 0.750\n",
      "Fold #2\n",
      "Fold score (RMSE): 39355.108645266264\n",
      "Accuracy: 0.763\n",
      "Fold #3\n",
      "Fold score (RMSE): 41209.549168746766\n",
      "Accuracy: 0.776\n",
      "Fold #4\n",
      "Fold score (RMSE): 36140.77541894397\n",
      "Accuracy: 0.746\n",
      "Fold #5\n",
      "Fold score (RMSE): 39628.79999529705\n",
      "Accuracy: 0.766\n",
      "\n",
      " Average RMSE: 38577.006693666204\n",
      "\n",
      " oob score : 0.7588977163258779\n"
     ]
    }
   ],
   "source": [
    "##Random Forests\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Shuffle\n",
    "print(\"Random Forests\")\n",
    "np.random.seed(42)\n",
    "data_train = data_train.reindex(np.random.permutation(data_train.index))\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_train.drop('SalePrice',axis=1), data_train['SalePrice'], \n",
    "                                                    test_size=0.20, random_state=42)\n",
    "\n",
    "classifier = RandomForestRegressor(n_estimators=20,oob_score=True)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# The mean squared error\n",
    "pred = classifier.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "print(\"Mean squared error: {}\".format(score))\n",
    "# Evaluate success using accuracy\n",
    "print(\"Accuracy: %.3f\" % classifier.score(X=x_test,y=y_test))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "classifier = RandomForestRegressor(n_estimators=1000,oob_score=True)\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "print(\"\\n oob score : {}\".format(classifier.oob_score_))    \n",
    "\n",
    "# Write the cross-validated prediction\n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifier.predict(scaler.transform(data_test))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_RF_full_features.csv', columns=['SalePrice'])\n",
    "\n",
    "###########Less features\n",
    "print(\"\\n\\n Less features \\n\\n\")\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_train_less_features.drop('SalePrice',axis=1), \n",
    "                                    data_train_less_features['SalePrice'], test_size=0.20, random_state=42)\n",
    "\n",
    "classifier = RandomForestRegressor(n_estimators=20,oob_score=True)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# The mean squared error\n",
    "pred = classifier.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "print(\"Mean squared error: {}\".format(score))\n",
    "# Evaluate success using accuracy\n",
    "print(\"Accuracy: %.3f\" % classifier.score(X=x_test,y=y_test))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=7)    \n",
    "\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "\n",
    "classifierRandomForestRegressor = RandomForestRegressor(n_estimators=1000,oob_score=True,max_depth=3)\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifierRandomForestRegressor = classifierRandomForestRegressor.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifierRandomForestRegressor.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifierRandomForestRegressor.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "print(\"\\n oob score : {}\".format(classifierRandomForestRegressor.oob_score_))    \n",
    "\n",
    "\n",
    "# Write the cross-validated prediction\n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifierRandomForestRegressor.predict(scaler.transform(data_test_less_features))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_RF_less_features.csv', columns=['SalePrice'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 28955.769604822068\n",
      "Accuracy: 0.886\n",
      "\n",
      "\n",
      "\n",
      "Mean squared error: 31991.973659829757\n",
      "Accuracy: 0.859\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "data_train_less_features = data_train_less_features.reindex(np.random.permutation(data_train_less_features.index))\n",
    "data_train_less_features.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train,y_train,test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "classifierGBR = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, \n",
    "                                               min_samples_leaf=25, min_samples_split=20, loss='huber').fit(x_train, y_train)\n",
    "\n",
    "# The mean squared error\n",
    "pred = classifierGBR.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "print(\"Mean squared error: {}\".format(score))\n",
    "# Evaluate success using accuracy\n",
    "print(\"Accuracy: %.3f\" % classifierGBR.score(X=x_test,y=y_test))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "data_train_less_features = data_train_less_features.reindex(np.random.permutation(data_train_less_features.index))\n",
    "data_train_less_features.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train,y_train,test_size=0.20, random_state=43)\n",
    "\n",
    "classifierGBR.fit(x_train, y_train)\n",
    "\n",
    "# The mean squared error\n",
    "pred = classifierGBR.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "print(\"Mean squared error: {}\".format(score))\n",
    "# Evaluate success using accuracy\n",
    "print(\"Accuracy: %.3f\" % classifierGBR.score(X=x_test,y=y_test))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "#save the contest result \n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifierGBR.predict(scaler.transform(data_test_less_features))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_GBR.csv', columns=['SalePrice'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 0.12.1\n",
      "Fold #1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensor Flow Version: {}\".format(tf.__version__))\n",
    "import tensorflow.contrib.learn as learn\n",
    "import shutil \n",
    "import os\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df,target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    \n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.int32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "\n",
    "    # Get a new directory to hold checkpoints from a neural network.  This allows the neural network to be\n",
    "# loaded later.  If the erase param is set to true, the contents of the directory will be cleared.\n",
    "def get_model_dir(name,erase=False):\n",
    "    base_path = os.path.join(\".\",\"dnn\")\n",
    "    model_dir = os.path.join(base_path,name)\n",
    "    os.makedirs(model_dir,exist_ok=True)\n",
    "    if erase and len(model_dir)>4 and os.path.isdir(model_dir):\n",
    "        shutil.rmtree(model_dir,ignore_errors=True) # be careful, this deletes everything below the specified path\n",
    "    return model_dir\n",
    "\n",
    "\n",
    "#Normalization\n",
    "# y_train = ((data_train['SalePrice']))\n",
    "# x_train = (data_train.drop('SalePrice',axis=1))\n",
    "# scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "# x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "\n",
    "#Normalization\n",
    "#x_train,y_train = to_xy(data_train_less_features,\"SalePrice\")\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "#Choose an optimizer\n",
    "#opt=tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "#opt=tf.train.MomentumOptimizer(learning_rate=0.001,momentum=0.9)\n",
    "\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x_train.shape[1])]\n",
    "classifierDNN = learn.DNNRegressor(hidden_units=[20, 10, 10, 5, 2], \n",
    "                                   feature_columns=feature_columns,\n",
    "                                   model_dir=get_model_dir(\"dnn\",False)\n",
    "                                   #optimizer=opt\n",
    "                                  )\n",
    "\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    # Early stopping\n",
    "    validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    x_test_fold,\n",
    "    y_test_fold,\n",
    "    every_n_steps=500,\n",
    "    #metrics=validation_metrics,\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=400)\n",
    "        \n",
    "    classifierDNN.fit(x_train_fold, y_train_fold, monitors=[validation_monitor] ,steps=1000)\n",
    "    pred = (list(classifierDNN.predict(x_test_fold, as_iterable=True)))\n",
    "    \n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "print(\"\\n\\n\")\n",
    "\n",
    "#Save the contest result\n",
    "pred = (list(classifierDNN.predict(scaler.transform(data_test_less_features),as_iterable=True)))\n",
    "#pred = list(classifierDNN.predict(scaler.transform(data_test),as_iterable=True))\n",
    "\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_DNN.csv', columns=['SalePrice'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127030.88, 159091.22, 185627.73, 181137.52, 181691.22, 183384.64, 167210.89, 165582.08, 207554.75, 122512.02, 198860.83, 92482.484, 103221.2, 118889.57, 108065.27, 371084.06, 248491.05, 332935.31, 342665.12, 416984.62, 265149.44, 206076.66, 190148.66, 163981.98, 183198.45, 199972.34, 355562.62, 246362.47, 197607.17, 254594.66, 196051.08, 97785.227, 209876.28, 320849.44, 302630.81, 247956.94, 185394.09, 162348.09, 166161.09, 157754.39, 159744.03, 152083.44, 314337.75, 227530.33, 212824.17, 149386.2, 254563.75, 218604.91, 162848.94, 145724.03, 144071.44, 143939.75, 132062.62, 159592.77, 214284.66, 149636.36, 200532.09, 129054.54, 224548.33, 131270.44, 138347.34, 163789.02, 113360.63, 118244.18, 124617.3, 133122.84, 103377.3, 135651.5, 132354.83, 169532.69, 149677.69, 102245.46, 137387.77, 123300.04, 169125.28, 110148.54, 77304.664, 166549.38, 245214.22, 131767.91, 141533.97, 145341.12, 194554.5, 80812.648, 106017.57, 137004.19, 131585.95, 123589.28, 122164.54, 134930.92, 123205.66, 154622.19, 156333.95, 110221.7, 169604.84, 100239.09, 97472.242, 102925.16, 110805.04, 149112.52, 75800.656, 129454.71, 117491.24, 153423.31, 141540.52, 255773.27, 84803.219, 216717.44, 126777.02, 137871.05, 120244.38, 142709.75, 277960.75, 120002.26, 246434.45, 236175.53, 194945.56, 134477.81, 149961.0, 193805.53, 128339.34, 125132.76, 294030.12, 232172.91, 146786.34, 91164.055, 99521.508, 166508.42, 109234.65, 136186.12, 89215.539, 106275.98, 114466.49, 181194.81, 141914.59, 209194.62, 167359.97, 192421.56, 198867.22, 169960.17, 53349.566, 126059.05, 74367.062, 309805.88, 237966.91, 153231.88, 154428.38, 215109.19, 195386.91, 159400.95, 141403.5, 191406.64, 169348.25, 136040.31, 93105.758, 92234.969, 103741.45, 124983.52, 134991.81, 243319.41, 147452.41, 131903.28, 258510.94, 201644.86, 137296.53, 168839.88, 185172.84, 275968.78, 171166.19, 342663.56, 186337.02, 230309.66, 169275.0, 190483.61, 172571.39, 150207.38, 185921.47, 206509.53, 188161.5, 236467.97, 201055.47, 260203.06, 224032.17, 223244.53, 174665.38, 153764.84, 155223.66, 129430.05, 123580.82, 120034.73, 125201.14, 96098.758, 94607.789, 156105.41, 136087.91, 151884.16, 155732.95, 155008.38, 119538.73, 143445.62, 436237.47, 355102.66, 334305.44, 436988.44, 326734.5, 346814.78, 349844.62, 333999.56, 340292.44, 335759.94, 254422.66, 393338.88, 303975.44, 239701.89, 193440.48, 199329.78, 218610.0, 405004.5, 373913.44, 299733.12, 288749.47, 344878.38, 179269.75, 184762.92, 176871.8, 163551.47, 166551.91, 190644.48, 194012.3, 186012.22, 178589.39, 268387.78, 167444.53, 178125.86, 160097.94, 246880.0, 164108.5, 329991.53, 338315.41, 254309.34, 277536.88, 255904.47, 246110.0, 269570.25, 244923.56, 431638.12, 213734.53, 202396.33, 264337.84, 223022.48, 272625.81, 247277.97, 304198.69, 215352.69, 210595.92, 179455.41, 166491.55, 139086.62, 216810.75, 253151.62, 178811.23, 135426.75, 180003.84, 216511.34, 238169.34, 185487.45, 172522.95, 186443.62, 165192.48, 168549.88, 119322.17, 128507.56, 124728.51, 117856.23, 127392.38, 113908.78, 303551.03, 255807.12, 278429.56, 201214.88, 197639.75, 164083.72, 163356.75, 276053.5, 220057.11, 198380.94, 210752.75, 248949.67, 153314.69, 147592.61, 198314.66, 119240.52, 152365.91, 203619.69, 165485.33, 115491.16, 110839.54, 152689.16, 154507.31, 159579.38, 153331.7, 186440.22, 183981.23, 121804.8, 172588.05, 168688.25, 224766.47, 138897.72, 162994.94, 116930.73, 129654.88, 137435.73, 125470.95, 144179.78, 134208.48, 109013.71, 109782.44, 131459.73, 103708.84, 190473.81, 132486.56, 81004.227, 174334.41, 117683.3, 98157.789, 133513.95, 151065.42, 42861.973, 106608.45, 82382.852, 164604.47, 146485.16, 78773.852, 156649.31, 130998.63, 114779.54, 137217.41, 117608.12, 100223.01, 107087.49, 141157.56, 130768.49, 140091.42, 143245.09, 141810.39, 118330.74, 146985.47, 116215.06, 129840.3, 146048.89, 69790.305, 106927.18, 117126.52, 85338.5, 38756.0, 102618.21, 103478.8, 149055.64, 148782.61, 80291.781, 82269.312, 142493.33, 49583.926, 121731.7, 118046.52, 101335.35, 103263.78, 122444.46, 133553.42, 130289.05, 146637.59, 113971.92, 122747.15, 122684.54, 147044.31, 121490.54, 85871.359, 122759.62, 87772.977, 161530.53, 158327.89, 101569.43, 143940.31, 135980.5, 136327.5, 157255.59, 150620.41, 44968.18, 101320.21, 126725.02, 143095.72, 116362.53, 126073.93, 171889.91, 185099.23, 206452.75, 164035.86, 167640.92, 120829.4, 164717.16, 121041.89, 408452.5, 404581.16, 404608.12, 367399.81, 370951.12, 234131.83, 286932.0, 213926.03, 236550.62, 309931.06, 175647.61, 211425.52, 144557.75, 192829.31, 195841.12, 218570.06, 208345.53, 130831.69, 134529.95, 266621.06, 241260.69, 205606.75, 218085.09, 268622.22, 322017.03, 209874.47, 266104.81, 174389.53, 123824.05, 149133.16, 99555.773, 142441.38, 132667.78, 149498.41, 136585.12, 139027.61, 129307.59, 146206.55, 132510.98, 167641.44, 146117.08, 198091.38, 131164.08, 169032.27, 171462.14, 172577.14, 105926.71, 127034.81, 130506.1, 154368.22, 305771.03, 129113.54, 73227.414, 304360.62, 87309.336, 242946.02, 141475.75, 169781.48, 151368.3, 396152.44, 346965.66, 199268.69, 220973.81, 209002.53, 372588.81, 139178.55, 173285.05, 114916.1, 124039.04, 137924.69, 137378.31, 172770.09, 193631.36, 172752.62, 192475.77, 174214.45, 175425.47, 257023.0, 178294.17, 175997.34, 187169.75, 223050.25, 425198.34, 390710.47, 170013.12, 327505.19, 194210.66, 251641.0, 168709.62, 257569.78, 225217.91, 166598.64, 194374.03, 134733.58, 301828.0, 158215.23, 266177.5, 153444.3, 107046.54, 117071.14, 99327.469, 106981.04, 119665.59, 154551.95, 137709.41, 292412.0, 376014.94, 389116.06, 352044.0, 408135.47, 344474.72, 238637.16, 335132.25, 405547.97, 267956.31, 287407.06, 308490.88, 310429.06, 207780.5, 328885.53, 218595.42, 205579.75, 164529.16, 250133.81, 219349.25, 204605.69, 172795.14, 196721.84, 212589.86, 210067.77, 220167.5, 183135.31, 246334.97, 181541.8, 319118.72, 459486.69, 308013.69, 365747.5, 322771.44, 304024.91, 283582.12, 273790.16, 252535.59, 249691.81, 206323.36, 246921.72, 198350.22, 192091.22, 196297.86, 146154.3, 177911.14, 184586.44, 182287.53, 200371.95, 197735.41, 197333.64, 125544.14, 139600.14, 123372.49, 107663.35, 189990.25, 145337.33, 286131.12, 324942.75, 183183.91, 152484.19, 162221.53, 166566.58, 263436.72, 255037.92, 257277.69, 262119.78, 165620.47, 222268.38, 174908.98, 180753.12, 279118.16, 235713.28, 311935.88, 310273.0, 216498.66, 152356.16, 168952.53, 226519.86, 140945.59, 159433.28, 127522.01, 132613.89, 159403.58, 102515.2, 129346.12, 144553.89, 74971.688, 49365.258, 132065.09, 120022.24, 217967.84, 136026.39, 149914.89, 172055.81, 131081.73, 116444.51, 132906.78, 120664.59, 172749.94, 111710.94, 137887.91, 73792.477, 100483.95, 96229.695, 136167.25, 142233.84, 167354.94, 137769.72, 118077.73, 136723.92, 123581.57, 127044.71, 107221.7, 136728.03, 132025.31, 134260.02, 109005.35, 120259.66, 128779.91, 125513.34, 125355.12, 80754.773, 125952.27, 117433.35, 121128.66, 30680.66, 127731.09, 144709.73, 77781.68, 87602.727, 152413.48, 45450.012, 106466.2, 140609.91, 120614.78, 109035.02, 129909.26, 117063.49, 91550.062, 198195.69, 120191.13, 112488.34, 130069.84, 138325.03, 128978.54, 119855.6, 113641.23, 143502.09, 112496.91, 167361.77, 122067.05, 100168.07, 134178.81, 74663.219, 90558.039, 84346.672, 144428.05, 128990.88, 140106.44, 170975.53, 139341.58, 107659.55, 141669.11, 121063.13, 109714.6, 114100.02, 115627.01, 88044.539, 83653.789, 112167.73, 143334.83, 155610.25, 145046.31, 140172.19, 125909.71, 130861.52, 137142.52, 139368.73, 175113.89, 170545.84, 127654.05, 162353.66, 211849.53, 133679.84, 205316.22, 157540.12, 110173.09, 150319.88, 282845.19, 225654.33, 230853.53, 207480.34, 183641.98, 245706.3, 336375.59, 361864.53, 258987.42, 176227.2, 145676.72, 208179.61, 212158.94, 204354.08, 223240.47, 141864.69, 139436.66, 143079.19, 231418.91, 249565.7, 343198.59, 244001.69, 220837.16, 144779.66, 232048.53, 193208.12, 225375.81, 198102.22, 130748.8, 133560.5, 153013.16, 152627.77, 172896.0, 407541.75, 68704.414, 70337.773, 90234.352, 104253.55, 97129.961, 110074.71, 90206.859, 109383.01, 161018.08, 151900.05, 139577.97, 131630.52, 180549.83, 146996.66, 168247.83, 114837.88, 138259.84, 171491.12, 250493.25, 208354.72, 124671.27, 113041.85, 119468.07, 98925.336, 101999.07, 99222.609, 142053.88, 38914.293, 88924.961, 95239.523, 84207.414, 303342.88, 289889.62, 329301.88, 213886.03, 133155.53, 174519.31, 185544.5, 331368.31, 237818.91, 139019.47, 210784.06, 192214.22, 194178.22, 265081.34, 223726.02, 255525.78, 313853.5, 253767.09, 123083.64, 170487.45, 152510.7, 129369.71, 125539.88, 102260.59, 77432.922, 149764.53, 132509.33, 118616.56, 135562.62, 141776.95, 152396.52, 181548.67, 146527.17, 158668.05, 193024.03, 188404.02, 260782.5, 161011.28, 171624.7, 162665.38, 196819.44, 220563.38, 419366.31, 360114.62, 172127.25, 325598.44, 347410.38, 402328.97, 159671.16, 195870.3, 224689.44, 194208.75, 165921.22, 173160.16, 174273.92, 189520.48, 177354.88, 144837.97, 104836.33, 110952.23, 166786.66, 199739.69, 107330.77, 102542.95, 141382.23, 122858.64, 366082.97, 274231.47, 319890.88, 351214.91, 372178.19, 409767.69, 391075.94, 358925.62, 365380.56, 245634.0, 355244.91, 355210.91, 330386.88, 282092.06, 333309.0, 248899.5, 230408.25, 226645.67, 199000.09, 200944.03, 189558.19, 215769.38, 299797.94, 203783.19, 190855.47, 223870.25, 183224.92, 189603.72, 179837.56, 202872.72, 180371.34, 177397.95, 180875.27, 177609.09, 272902.66, 169632.81, 230036.47, 181895.75, 208904.53, 175162.61, 211529.0, 218197.94, 186555.81, 176200.52, 318091.69, 429902.03, 323135.28, 295932.06, 348478.25, 305618.44, 195770.02, 260131.25, 222467.44, 362622.75, 213635.06, 223540.06, 223858.88, 221542.88, 221464.59, 204319.64, 192628.14, 223418.47, 210840.59, 331045.38, 252558.94, 242923.34, 245349.94, 129114.98, 143720.78, 161582.84, 199031.25, 194899.66, 140896.53, 107833.45, 138658.5, 266302.06, 145831.88, 206479.06, 209118.16, 175395.31, 200372.53, 230675.09, 207572.62, 155771.47, 161610.47, 180592.92, 250438.16, 255517.12, 260218.19, 270642.88, 327066.75, 134926.19, 192228.78, 143414.05, 155001.72, 222499.02, 202733.09, 224513.03, 147708.72, 137973.61, 135944.69, 136718.97, 129194.71, 135655.0, 136763.88, 107898.55, 157380.28, 119397.73, 218274.78, 148609.58, 235644.53, 120015.2, 57085.355, 57917.395, 108440.16, 131428.55, 134955.92, 135025.25, 157964.67, 142031.58, 127261.07, 136084.97, 131499.59, 150051.12, 125044.59, 152248.67, 127835.9, 134581.0, 136342.81, 118225.13, 129906.04, 123881.09, 127974.12, 126276.61, 138378.55, 118450.92, 117062.1, 151075.31, 282932.06, 134018.56, 107752.05, 169132.12, 114585.71, 134794.81, 127793.26, 137695.5, 136388.78, 134109.56, 149931.25, 98623.82, 104046.68, 127352.81, 103543.04, 124330.47, 109006.6, 103408.15, 125263.2, 121026.48, 83527.945, 117056.5, 182747.22, 150602.25, 104320.93, 165634.94, 121929.3, 186861.94, 88460.078, 114426.23, 118138.79, 131215.88, 124518.43, 142656.64, 106545.3, 141628.19, 112545.65, 122593.6, 110575.23, 159626.69, 128541.88, 123133.1, 145780.02, 86798.125, 99962.508, 200091.34, 218609.77, 181726.2, 128906.73, 81764.914, 197387.73, 105996.55, 122656.98, 153450.53, 124465.19, 133187.42, 126518.15, 130459.1, 115015.4, 116783.9, 121221.02, 150274.86, 249822.5, 148805.84, 166131.48, 139172.88, 69476.945, 172000.05, 170105.86, 156118.03, 109842.12, 243396.28, 148688.56, 111563.23, 98445.633, 102228.95, 135139.69, 160236.55, 72123.359, 203346.59, 221788.75, 247295.75, 281639.75, 250242.31, 230325.97, 224371.67, 188351.23, 225961.88, 216870.38, 331572.88, 150543.12, 184829.44, 131119.95, 144249.34, 238666.52, 221880.14, 198018.34, 214936.44, 130264.52, 133400.05, 137836.66, 140396.42, 121103.76, 127706.43, 147489.67, 126811.03, 231768.44, 229105.75, 201560.69, 241192.08, 264235.22, 221101.69, 264629.47, 190738.12, 198274.59, 175008.67, 192372.2, 163251.23, 151453.45, 114327.21, 140966.08, 131115.75, 140450.47, 150109.88, 173267.3, 560163.25, 143529.5, 121579.77, 71021.289, 105828.86, 112196.18, 102173.26, 111489.27, 143731.58, 118881.66, 135930.19, 133486.14, 137938.16, 144580.56, 184836.03, 109589.84, 123521.94, 129113.96, 181234.58, 195101.02, 123720.46, 165926.38, 169506.81, 225686.98, 258916.38, 129879.41, 119728.4, 125002.41, 78842.938, 44872.879, 144944.91, 134003.09, 113758.74, 266382.94, 181811.38, 189685.59, 238296.91, 190571.34, 132062.64, 153198.03, 185059.58, 240808.64, 208705.47, 260125.03, 168317.72, 234369.34, 278674.22, 187887.53, 301635.88, 377783.0, 136284.48, 116447.77, 99213.508, 89456.211, 95966.062, 97733.5, 161671.94, 176307.38, 239358.38, 129078.1, 100938.48, 155582.77, 156551.73, 122489.09, 129908.26, 134248.47, 137828.38, 180066.02, 203417.66, 184735.44, 188047.59, 183496.0, 184767.75, 241652.59, 240506.69, 275554.59, 155324.25, 153235.3, 461773.38, 468614.31, 347005.16, 389617.31, 433626.81, 340731.62, 407492.62, 161816.42, 186957.44, 226682.88, 255971.7, 190583.06, 141546.23, 109672.28, 199937.8, 112357.11, 134533.69, 101636.3, 93885.062, 104175.12, 133429.58, 137382.31, 117109.71, 151312.38, 372697.28, 260729.84, 257028.62, 365445.62, 332504.84, 300941.44, 303333.34, 322430.62, 338850.62, 368424.03, 380124.5, 245374.53, 290745.47, 341863.53, 264949.62, 172584.08, 184677.62, 177947.3, 259873.53, 183727.72, 176008.86, 184260.05, 194726.06, 201037.41, 194536.97, 193489.44, 281358.44, 296862.19, 292344.12, 493970.66, 325497.81, 614300.12, 342816.28, 327600.5, 234688.52, 323855.81, 216885.5, 204605.86, 422586.88, 197418.09, 147816.22, 202805.42, 143336.98, 192615.98, 188191.59, 185869.03, 187539.69, 187764.64, 164995.14, 170005.22, 123341.02, 127926.38, 137659.86, 124471.09, 115526.48, 114638.82, 132434.5, 110893.62, 131782.64, 322969.62, 429860.19, 191117.25, 156684.66, 161482.09, 147095.03, 194008.0, 211346.56, 160691.75, 157396.91, 140149.56, 139022.98, 177346.47, 147039.0, 131656.2, 138522.02, 203767.53, 155173.31, 150190.19, 136520.55, 124782.87, 110868.12, 162166.95, 148634.73, 137754.25, 136777.62, 118921.71, 140068.0, 163520.53, 124188.8, 142072.16, 152649.52, 137204.73, 152575.2, 147018.36, 132502.75, 136823.66, 113573.17, 134174.36, 129918.43, 128145.07, 241029.19, 151094.86, 191827.98, 129292.97, 91612.555, 82084.008, 81700.969, 142846.16, 144950.03, 145390.56, 144082.73, 197075.53, 146815.56, 280162.38, 131415.02, 88119.977, 122631.04, 129271.45, 142171.73, 114032.15, 116856.8, 135500.52, 133417.5, 129480.29, 133863.05, 131730.84, 113310.64, 96721.758, 106514.94, 85689.32, 88132.641, 96730.141, 119460.29, 131496.34, 58680.77, 120024.46, 72307.797, 148605.05, 94730.68, 119593.37, 49315.363, 175084.7, 85037.461, 109371.63, 92690.164, 215635.69, 117715.23, 120854.04, 83401.516, 111069.98, 135293.22, 150959.62, 128638.09, 97646.086, 86357.508, 145728.98, 150850.67, 132426.95, 131007.12, 154124.12, 135245.09, 146103.06, 132005.47, 113839.89, 225370.14, 143557.17, 125154.34, 154575.47, 140895.69, 129369.05, 167548.02, 325421.28, 153084.94, 123846.96, 132888.72, 155078.12, 272136.5, 226583.38, 217622.5, 196962.88, 246418.41, 340356.06, 229301.5, 232319.44, 204100.69, 168506.0, 140681.47, 176854.78, 189298.66, 204665.75, 218395.55, 137255.39, 166532.12, 115864.1, 215127.78, 202397.84, 247595.06, 204911.42, 258859.28, 228003.78, 213517.81, 227450.31, 147475.58, 209386.62, 208559.53, 202171.09, 202861.05, 131779.41, 137813.72, 138712.92, 206760.83, 130194.75, 242713.75, 149133.16, 151305.41, 94534.438, 121109.62, 129325.92, 142296.3, 103624.82, 41810.453, 102886.82, 134012.88, 133501.98, 155601.14, 117821.1, 164007.02, 137562.41, 116487.37, 156799.45, 158492.16, 148617.34, 198645.67, 159548.8, 160463.97, 102876.62, 126173.5, 65777.266, 79056.773, 146240.59, 56469.715, 69749.539, 62121.48, 309602.62, 313810.44, 222711.66, 141029.86, 231992.56, 144450.22, 259619.25, 194310.67, 328701.75, 354908.97, 82777.133, 249455.7, 111664.32, 128025.29, 138232.09, 75430.492, 99134.75, 149052.58, 103789.0, 90945.203, 92345.406, 96486.039, 171175.81, 118042.34, 217900.72]\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "# estimators = []\n",
    "# estimators.append(('linear',classifierLinearRegression))\n",
    "# estimators.append(('svr',classifierSVR))\n",
    "# estimators.append(('rf',classifierRandomForestRegressor))\n",
    "\n",
    "# # # create the ensemble model\n",
    "# ensemble = VotingClassifier(estimators,voting='hard')\n",
    "# ensemble = ensemble.fit(x_train,y_train)\n",
    "# pred = []\n",
    "# pred = np.array(pred,dtype='float64')\n",
    "# pred = ensemble.predict(scaler.transform(data_test_less_features))\n",
    "# result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "# result.to_csv('pred_EnsembleVoting.csv', columns=['SalePrice'])\n",
    "\n",
    "\n",
    "#Emsemble via stacking\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "stregr = StackingRegressor(regressors=[classifierLinearRegression,classifierSVR, classifierRandomForestRegressor,classifierGBR], \n",
    "                           meta_regressor=classifierLinearRegression)\n",
    "stregr.fit(x_train,y_train)\n",
    "pred = []\n",
    "pred = np.array(pred)\n",
    "pred = stregr.predict(scaler.transform(data_test_less_features))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_EnsembleStacker.csv', columns=['SalePrice'])\n",
    "\n",
    "#Emsemble via avereging\n",
    "dnn=[]\n",
    "dnn = list(classifierDNN.predict(scaler.transform(data_test_less_features),as_iterable=True))\n",
    "print(dnn)\n",
    "final_labels = (\n",
    "                (classifierLinearRegression.predict(scaler.transform(data_test_less_features))) + \n",
    "                (classifierSVR.predict(scaler.transform(data_test_less_features))) +\n",
    "                (classifierRandomForestRegressor.predict(scaler.transform(data_test_less_features)))+\n",
    "                (classifierGBR.predict(scaler.transform(data_test_less_features))) +\n",
    "                dnn\n",
    "               \n",
    "               ) / 5\n",
    "\n",
    "## Saving to CSV\n",
    "pd.DataFrame({'Id': range(1461,2920), 'SalePrice': final_labels}).to_csv('pred_EnsembleAvereging.csv', index =False)  \n",
    "\n",
    "print(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
