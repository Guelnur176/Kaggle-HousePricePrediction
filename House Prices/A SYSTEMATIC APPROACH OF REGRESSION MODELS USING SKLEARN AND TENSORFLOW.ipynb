{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A SYSTEMATIC APPROACH OF REGRESSION MODELS USING SKLEARN AND TENSORFLOW\n",
    "\n",
    "Christian Masdeval - November 2017\n",
    "\n",
    "\n",
    "In this kernel I will try to bring to you a resume of the analysis I have made in the House Prices dataset. My objective was to explore as many different predective models as possible trying to find the best one. It is divided in the following way:\n",
    "\n",
    "    Understanding and cleaning the data\n",
    "        Linear Regression Models\n",
    "            Linear Regression\n",
    "            Stochastic Gradient Descent\n",
    "            Ridge\n",
    "            Lasso\n",
    "            Elastic Net\n",
    "    Suport Vector Machine\n",
    "    Neural Network\n",
    "    Random Forests\n",
    "    Gradient Boosting\n",
    "    Deep Learning with TensorFlow\n",
    "    Ensemble Technics\n",
    "\n",
    "1 . Understanding and cleaning the data\n",
    "\n",
    "First thing first. As mentioned by several authors, much of the work in make a good model is related to a good feature engineering process. It take a while until we start gain some significant intuition about what is going on but some steps are always present. I will describe what I did trying to stabilize the data and how I got to this decision. Anyway, I think the goals of this step that are always present in any dataset and should guide us are:\n",
    "\n",
    "    Deal with missing values\n",
    "    Select the best features\n",
    "\n",
    "One more thing. As our objective is to analyze the test.csv file, create the submission file and upload it to kaggle, one premisse I have used was not to remove any single line from the test set. We must submit a result file with exactly the number of lines of the original test file.\n",
    "\n",
    "What is the data\n",
    "\n",
    "There are excellent kernels showing how to get a glimpse of the data (like Comprehensive data exploration with Python). Most of them have used programming languages like R ore Python to extract useful visualizations. I would also suggest to use the Weka tool to do that. I could get excellent insights about the data via Weka.\n",
    "\n",
    "We should also look the data dictonary when available. In our case it exists and bring important informations:\n",
    "\n",
    "    *The categorical and numerial fields\n",
    "    *The description of each one\n",
    "    *That there is a category called NA to denote the absence of some characteristic (and Python also use NA)\n",
    "    *That there are some categorical fields that have only numerical values (and Python will interpret them as numbers)\n",
    "\n",
    "\n",
    "Categorical field with numerical values\n",
    "\n",
    "    OverallQual: Rates the overall material and finish of the house\n",
    "\n",
    "       10\tVery Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\n",
    "       5\tAverage\n",
    "       4\tBelow Average\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor    \n",
    "\n",
    "\n",
    "Value NA being used as a category\n",
    "\n",
    "    Alley: Type of alley access to property\n",
    "\n",
    "       Grvl\tGravel\n",
    "       Pave\tPaved\n",
    "       NA \tNo alley access\n",
    "\n",
    "Missing values\n",
    "\n",
    "Maybe the most common problem in real datasets is the presence of null values and most of the implementations of the regression algorithms break in the presence of them. Sometimes, instead of a blank in the position, a wildcard is used like ? or NA. However, as we see above, in the House Prices example the value NA is being used to denote other kind of information, what is a problem. So, our first attempt will be to exchange these NA values to the value 'No', denoting the absence of some characteristic.\n",
    "\n",
    "Fisrt, lets see what are the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.set_printoptions(threshold=np.nan) #to print all the elements of a matrix\n",
    "pd.set_option('display.max_rows', 2000)#to print all the elements of a data frame\n",
    "\n",
    "%matplotlib inline\n",
    "    \n",
    "df = pd.read_csv(\"train.csv\",na_values=['?',''],delimiter=',',delim_whitespace=False)\n",
    "df_test = pd.read_csv(\"test.csv\",na_values=['?',''],delimiter=',',delim_whitespace=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set:  (1460, 81)\n",
      "Shape of test set:  (1459, 80)\n",
      "Missing values before remove NA:  Index(['Alley', 'BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2',\n",
      "       'BsmtFinType1', 'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath',\n",
      "       'BsmtQual', 'BsmtUnfSF', 'Electrical', 'Exterior1st', 'Exterior2nd',\n",
      "       'Fence', 'FireplaceQu', 'Functional', 'GarageArea', 'GarageCars',\n",
      "       'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType', 'GarageYrBlt',\n",
      "       'KitchenQual', 'LotFrontage', 'MSZoning', 'MasVnrArea', 'MasVnrType',\n",
      "       'MiscFeature', 'PoolQC', 'SalePrice', 'SaleType', 'TotalBsmtSF',\n",
      "       'Utilities'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training set: \", df.shape)\n",
    "print(\"Shape of test set: \", df_test.shape)\n",
    "data_aux = df.append(df_test) #merging the two datasets to facilitate the cleaning\n",
    "print(\"Missing values before remove NA: \" , data_aux.columns[data_aux.isnull().any()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets replace NA for No where appropriate and search for the missing values again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after insert No, i.e., real missing values:  Index(['BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF',\n",
      "       'Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'GarageArea',\n",
      "       'GarageCars', 'GarageYrBlt', 'KitchenQual', 'LotFrontage', 'MSZoning',\n",
      "       'MasVnrArea', 'MasVnrType', 'SalePrice', 'SaleType', 'TotalBsmtSF',\n",
      "       'Utilities'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Alley\n",
    "data_aux.Alley.fillna(inplace=True,value='No')\n",
    "\n",
    "#BsmtQual\n",
    "data_aux.BsmtQual.fillna(inplace=True,value='No')\n",
    "\n",
    "#BsmtCond\n",
    "data_aux.BsmtCond.fillna(inplace=True,value='No')\n",
    "\n",
    "#BsmtExposure\n",
    "data_aux.BsmtExposure.fillna(inplace=True,value='No')\n",
    "\n",
    "#BsmtFinType1\n",
    "data_aux.BsmtFinType1.fillna(inplace=True,value='No')\n",
    "\n",
    "#BsmtFinType2\n",
    "data_aux.BsmtFinType2.fillna(inplace=True,value='No')\n",
    "\n",
    "#FireplaceQu\n",
    "data_aux.FireplaceQu.fillna(inplace=True,value='No')    \n",
    "\n",
    "#GarageType\n",
    "data_aux.GarageType.fillna(inplace=True,value='No')\n",
    "\n",
    "#GarageFinish\n",
    "data_aux.GarageFinish.fillna(inplace=True,value='No')\n",
    "\n",
    "#GarageQual \n",
    "data_aux.GarageQual.fillna(inplace=True,value='No')\n",
    "    \n",
    "#GarageCond\n",
    "data_aux.GarageCond.fillna(inplace=True,value='No')\n",
    "\n",
    "#PoolQC\n",
    "data_aux.PoolQC.fillna(inplace=True,value='No')\n",
    "    \n",
    "#Fence\n",
    "data_aux.Fence.fillna(inplace=True,value='No')\n",
    "\n",
    "#MiscFeature\n",
    "data_aux.MiscFeature.fillna(inplace=True,value='No')\n",
    "    \n",
    "print(\"Missing values after insert No, i.e., real missing values: \" , data_aux.columns[data_aux.isnull().any()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the real missing values. Among the columns that were returned, we now going to treat the numeric fields. Note that these numeric fields had NA to denote a missing value. So, the dataset was using the same code to encode two different informations what would cause a lot of confusion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Numeric fields    \n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=1)  #insert the mean   \n",
    "    \n",
    "#BsmtFinSF1\n",
    "#df.dropna(axis=0,subset=['BsmtFinSF1'],inplace=True)\n",
    "#df['BsmtFinSF1'] = imp.fit_transform(df['BsmtFinSF1'].reshape(1,-1)).transpose()    \n",
    "data_aux.BsmtFinSF1.fillna(inplace=True,value=0)\n",
    "    \n",
    "#BsmtFinSF2\n",
    "#df.dropna(axis=0,subset=['BsmtFinSF2'],inplace=True)\n",
    "#df['BsmtFinSF2'] = imp.fit_transform(df['BsmtFinSF2'].reshape(1,-1)).transpose()    \n",
    "data_aux.BsmtFinSF2.fillna(inplace=True,value=0)\n",
    "    \n",
    "#BsmtUnfSF\n",
    "#df.dropna(axis=0,subset=['BsmtUnfSF'],inplace=True)\n",
    "#df.drop('BsmtUnfSF',axis=1,inplace=True)\n",
    "#df['BsmtUnfSF'] = imp.fit_transform(df['BsmtUnfSF'].reshape(1,-1)).transpose()    \n",
    "data_aux.BsmtUnfSF.fillna(inplace=True,value=0)\n",
    "    \n",
    "#TotalBsmtSF\n",
    "#df.dropna(axis=0,subset=['TotalBsmtSF'],inplace=True)\n",
    "#df['TotalBsmtSF'] = imp.fit_transform(df['TotalBsmtSF'].reshape(1,-1)).transpose()    \n",
    "data_aux.TotalBsmtSF.fillna(value=0,inplace=True)\n",
    "    \n",
    "#BsmtFullBath - apenas na base de teste tem NA.Nao posso remover a linha\n",
    "#df['BsmtFullBath'] = imp.fit_transform(df['TotalBsmtSF'].reshape(1,-1)).transpose()    \n",
    "data_aux.BsmtFullBath.fillna(inplace=True,value=0)\n",
    "    \n",
    "#BsmtHalfBath- apenas na base de teste tem NA.Nao posso remover a linha\n",
    "#df['BsmtHalfBath'] = imp.fit_transform(df['BsmtHalfBath'].reshape(1,-1)).transpose()    \n",
    "data_aux.BsmtHalfBath.fillna(inplace=True,value=0)\n",
    "        \n",
    "#GarageCars\n",
    "#df.dropna(axis=0,subset=['GarageCars'],inplace=True)\n",
    "#df['GarageCars'] = imp.fit_transform(df['GarageCars'].reshape(1,-1)).transpose()    \n",
    "data_aux.GarageCars.fillna(value=0,inplace=True)\n",
    "    \n",
    "#GarageArea\n",
    "#df.dropna(axis=0,subset=['GarageArea'],inplace=True)\n",
    "#df['GarageArea'] = imp.fit_transform(df['GarageArea'].reshape(1,-1)).transpose()    \n",
    "data_aux.GarageArea.fillna(value=0,inplace=True)\n",
    "        \n",
    "#LotFrontage \n",
    "data_aux['LotFrontage'].fillna(inplace=True,value=0)\n",
    "    \n",
    "#GarageYrBlt - remove the hole column\n",
    "data_aux.GarageYrBlt.fillna(inplace=True,value=0)\n",
    "   \n",
    "#MasVnrArea \n",
    "data_aux.MasVnrArea.fillna(inplace=True,value=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These eleven fields had null values. My first impulse was to fill these missing values with the mean (average) of the whole values of each column or drop the hole column. Latter, i realized that would be better to set zero instead, as this approch could cause less bias. \n",
    "\n",
    "Next, we have to deal with the categorical values. In such cases i decided to fill with the most common value of each column, once more trying to cause the less bias as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after all:  Index(['SalePrice'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#####Categorial fields\n",
    "\n",
    "\n",
    "#KitchenQual\n",
    "data_aux.KitchenQual = data_aux.KitchenQual.mode()[0]\n",
    "\n",
    "#Functional\n",
    "data_aux.Functional = data_aux.Functional.mode()[0]\n",
    "\n",
    "#Utilities\n",
    "data_aux.Utilities = data_aux.Utilities.mode()[0]  \n",
    "    \n",
    "#SaleType\n",
    "data_aux.SaleType  = data_aux.SaleType.mode()[0]\n",
    "    \n",
    "#Exterior1st- nao posso remover linhas do teste\n",
    "data_aux.Exterior1st = data_aux.Exterior1st.mode()[0]\n",
    "\n",
    "#Exterior2nd\n",
    "data_aux.Exterior2nd = data_aux.Exterior2nd.mode()[0]       \n",
    "\n",
    "#Electrical - remove the records where the value is NA\n",
    "data_aux.Electrical = df['Electrical'].mode()[0]\n",
    "\n",
    "#MSZoning   - tem NA apenas na base de teste. Como nao posso remover linhas removo a coluna   \n",
    "data_aux.MSZoning = data_aux.MSZoning.mode()[0]\n",
    "     \n",
    "#MasVnrType - remove the records where the value is NA \n",
    "data_aux.MasVnrType=df['MasVnrType'].mode()[0]\n",
    "\n",
    "\n",
    "print(\"Missing values after all: \" , data_aux.columns[data_aux.isnull().any()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now that there are no missing value. Of course SalePrice does not count because as we merge train and test, this column became empty for the test records.\n",
    "\n",
    "There is only one more thing to tackle. I have noticed this when i was navigating into Weka and looking the data. Categorical features where the categories are expressed by numbers are treated as quantitative values. This is an undesirable behaviour as we are planning to convert categorical to dummy values and this will not work for this columns. As the sklearn.feature_extraction documentation explain \n",
    "\n",
    "    When feature values are strings, this transformer will do a binary one-hot (aka one-of-K) coding: one boolean-valued feature is constructed for each of the possible string values that the feature can take on. For instance, a feature “f” that can take on the values “ham” and “spam” will become two features in the output, one signifying “f=ham”, the other “f=spam”.\n",
    "\n",
    "    However, note that this transformer will only do a binary one-hot encoding when feature values are of type string. If categorical features are represented as numeric values such as int, the DictVectorizer can be followed by OneHotEncoder to complete binary one-hot encoding.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "#Converting numeric columns to nominal before applying dummy convertion\n",
    "#After converting to String they will be treated as categorical\n",
    "\n",
    "# MSSubClass as str\n",
    "print(data_aux['MSSubClass'].dtype)\n",
    "data_aux['MSSubClass'] = data_aux['MSSubClass'].astype(\"str\")\n",
    "print(data_aux['MSSubClass'].dtype)\n",
    "\n",
    "    \n",
    "# Converting OverallCond to str\n",
    "data_aux.OverallCond = data_aux.OverallCond.astype(\"str\")\n",
    "\n",
    "# KitchenAbvGr to categorical\n",
    "data_aux['KitchenAbvGr'] = data_aux['KitchenAbvGr'].astype(\"str\")\n",
    "    \n",
    "# Year and Month to categorical\n",
    "data_aux['YrSold'] = data_aux['YrSold'].astype(\"str\")\n",
    "data_aux['MoSold'] = data_aux['MoSold'].astype(\"str\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in transforming our dataset in one more suitable for being used by predictive models is to re encode the categorical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape train: (1460, 286)\n",
      "Indice da coluna SalePrice no novo dataset 26\n",
      "New shape test: (1459, 285)\n",
      "Null values train \n",
      " Index([], dtype='object')\n",
      "Null values test \n",
      " Index([], dtype='object')\n",
      "Columns only in test set :  Index([], dtype='object')\n",
      "Columns only in train set :  Index(['SalePrice'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_final = pd.get_dummies(data_aux)\n",
    "\n",
    "data_train = data_final.iloc[:-df_test.shape[0],:]\n",
    "data_train.to_csv('train_no_categorical.csv')\n",
    "print(\"New shape train:\" , np.shape(data_train))\n",
    "print(\"Indice da coluna SalePrice no novo dataset\" , data_train.columns.get_loc('SalePrice'))\n",
    "\n",
    "data_test = data_final.iloc[:df_test.shape[0],:]\n",
    "\n",
    "data_test.to_csv('test_no_categorical.csv')\n",
    "data_test.drop('SalePrice',inplace=True,axis=1)\n",
    "print(\"New shape test:\" , np.shape(data_test))\n",
    "\n",
    "print(\"Null values train \\n\", data_train.columns[data_train.isnull().any()])\n",
    "print(\"Null values test \\n\", data_test.columns[data_test.isnull().any()])\n",
    "\n",
    "\n",
    "print(\"Columns only in test set : \" , data_test.columns.difference(data_train.columns))\n",
    "print(\"Columns only in train set : \" , data_train.columns.difference(data_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà! We now have a new dataset with no null values. I have printed some diagnostic informations for us. \n",
    "\n",
    "    *Note that the difference in the number of columns between the train and test set is only one. Is is the expected if all the transformations we did were well done. This extra column in the train set is due to SalePrice.\n",
    "    *There are the same number of rows as in the begining. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
