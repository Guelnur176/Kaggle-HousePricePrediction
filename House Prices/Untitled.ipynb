{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n",
      "(1459, 80)\n",
      "(1445, 66)\n",
      "(1459, 65)\n",
      "Id                 int64\n",
      "MSSubClass        object\n",
      "LotFrontage      float64\n",
      "LotArea            int64\n",
      "Street            object\n",
      "Alley             object\n",
      "LotShape          object\n",
      "LandContour       object\n",
      "LotConfig         object\n",
      "LandSlope         object\n",
      "Neighborhood      object\n",
      "Condition1        object\n",
      "BldgType          object\n",
      "HouseStyle        object\n",
      "OverallQual        int64\n",
      "OverallCond       object\n",
      "YearBuilt          int64\n",
      "YearRemodAdd       int64\n",
      "RoofStyle         object\n",
      "MasVnrType        object\n",
      "ExterQual         object\n",
      "ExterCond         object\n",
      "Foundation        object\n",
      "BsmtQual          object\n",
      "BsmtCond          object\n",
      "BsmtExposure      object\n",
      "BsmtFinType1      object\n",
      "BsmtFinSF1       float64\n",
      "BsmtFinType2      object\n",
      "BsmtFinSF2       float64\n",
      "                  ...   \n",
      "2ndFlrSF           int64\n",
      "LowQualFinSF       int64\n",
      "GrLivArea          int64\n",
      "BsmtFullBath       int64\n",
      "BsmtHalfBath       int64\n",
      "FullBath           int64\n",
      "HalfBath           int64\n",
      "BedroomAbvGr       int64\n",
      "KitchenAbvGr      object\n",
      "KitchenQual       object\n",
      "TotRmsAbvGrd       int64\n",
      "Fireplaces         int64\n",
      "FireplaceQu       object\n",
      "GarageType        object\n",
      "GarageFinish      object\n",
      "GarageCars         int64\n",
      "GarageArea         int64\n",
      "GarageCond        object\n",
      "PavedDrive        object\n",
      "WoodDeckSF         int64\n",
      "OpenPorchSF        int64\n",
      "EnclosedPorch      int64\n",
      "3SsnPorch          int64\n",
      "ScreenPorch        int64\n",
      "PoolArea           int64\n",
      "Fence             object\n",
      "MoSold            object\n",
      "YrSold            object\n",
      "SaleCondition     object\n",
      "SalePrice          int64\n",
      "dtype: object\n",
      "Null values treino \n",
      " Index([], dtype='object')\n",
      "Null values test \n",
      " Index([], dtype='object')\n",
      "New shape train: (1445, 249)\n",
      "Indice da coluna SalePrice no novo dataset 235\n",
      "New shape test: (1459, 248)\n",
      "Colunas que existem apenas teste :  Index([], dtype='object')\n",
      "Colunas que existem apenas treino :  Index(['SalePrice'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def remove_categorical_columns(df):\n",
    "    df.drop('MSZoning',axis=1,inplace=True)\n",
    "    df.drop('Street',axis=1,inplace=True)\n",
    "    df.drop('Alley',axis=1,inplace=True)\n",
    "    df.drop('LotShape',axis=1,inplace=True)\n",
    "    df.drop('LandContour',axis=1,inplace=True)\n",
    "    df.drop('Utilities',axis=1,inplace=True)\n",
    "    df.drop('LotConfig',axis=1,inplace=True)\n",
    "    df.drop('LandSlope',axis=1,inplace=True)\n",
    "    df.drop('Neighborhood',axis=1,inplace=True)\n",
    "    df.drop('Condition1',axis=1,inplace=True)\n",
    "    df.drop('Condition2',axis=1,inplace=True)\n",
    "    df.drop('BldgType',axis=1,inplace=True)\n",
    "    df.drop('HouseStyle',axis=1,inplace=True)\n",
    "    df.drop('RoofStyle',axis=1,inplace=True)\n",
    "    df.drop('RoofMatl',axis=1,inplace=True)\n",
    "    df.drop('Exterior1st',axis=1,inplace=True)\n",
    "    df.drop('Exterior2nd',axis=1,inplace=True)\n",
    "    df.drop('MasVnrType',axis=1,inplace=True)\n",
    "    df.drop('ExterQual',axis=1,inplace=True)\n",
    "    df.drop('ExterCond',axis=1,inplace=True)\n",
    "    df.drop('Foundation',axis=1,inplace=True)\n",
    "    df.drop('BsmtQual',axis=1,inplace=True)\n",
    "    df.drop('BsmtCond',axis=1,inplace=True)\n",
    "    df.drop('BsmtExposure',axis=1,inplace=True)\n",
    "    df.drop('BsmtFinType1',axis=1,inplace=True)\n",
    "    df.drop('BsmtFinType2',axis=1,inplace=True)\n",
    "    df.drop('Heating',axis=1,inplace=True)\n",
    "    df.drop('HeatingQC',axis=1,inplace=True)\n",
    "    df.drop('CentralAir',axis=1,inplace=True)\n",
    "    df.drop('Electrical',axis=1,inplace=True)\n",
    "    df.drop('KitchenQual',axis=1,inplace=True)\n",
    "    df.drop('Functional',axis=1,inplace=True)\n",
    "    df.drop('FireplaceQu',axis=1,inplace=True)\n",
    "    df.drop('GarageType',axis=1,inplace=True)\n",
    "    df.drop('GarageFinish',axis=1,inplace=True)\n",
    "    df.drop('GarageQual',axis=1,inplace=True)\n",
    "    df.drop('GarageCond',axis=1,inplace=True)\n",
    "    df.drop('PavedDrive',axis=1,inplace=True)\n",
    "    df.drop('PoolQC',axis=1,inplace=True)\n",
    "    df.drop('Fence',axis=1,inplace=True)\n",
    "    df.drop('MiscFeature',axis=1,inplace=True)\n",
    "    df.drop('SaleType',axis=1,inplace=True)\n",
    "    df.drop('SaleCondition',axis=1,inplace=True)\n",
    "\n",
    "def input_missing_value(df):\n",
    "    \n",
    "    # MSSubClass as str\n",
    "    df['MSSubClass'] = df['MSSubClass'].astype(\"str\")\n",
    "    #After converting this column to String, it will be handled as categorical\n",
    "    #There is one value in the test set that there isn't in the training. It is 150\n",
    "    #It will be result in one column for this categorical value that doesn't exist in the training set\n",
    "    #It can't happen\n",
    "    #There is only one value 150 in the row 1358 in the test set\n",
    "    #We also can't remove any single row from the test set as we will need make predictions for all rows \n",
    "    #Let's just pass the value 150 to 40 , as this value exists in booth sets and is is the less common \n",
    "    df['MSSubClass'][df.MSSubClass=='150']='40'\n",
    "    \n",
    "    # Converting OverallCond to str\n",
    "    df.OverallCond = df.OverallCond.astype(\"str\")\n",
    "    \n",
    "    # KitchenAbvGr to categorical\n",
    "    df['KitchenAbvGr'] = df['KitchenAbvGr'].astype(\"str\")\n",
    "    df.drop(df[df.KitchenAbvGr=='3'].index,inplace=True) # apenas no treino\n",
    "    \n",
    "    # Year and Month to categorical\n",
    "    df['YrSold'] = df['YrSold'].astype(\"str\")\n",
    "    df['MoSold'] = df['MoSold'].astype(\"str\")    \n",
    "    \n",
    "    #LotFrontage - insert the mean \n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=1)\n",
    "    #print(np.shape(df['LotFrontage']))\n",
    "    df['LotFrontage'] = imp.fit_transform(df['LotFrontage']).transpose()    \n",
    "   \n",
    "    #Alley\n",
    "    df.Alley.fillna(inplace=True,value='No')\n",
    "\n",
    "    #MasVnrType - remove the records where the value is NA \n",
    "    #print(\"Number of lines where MasVnrType has Nan value\", len(df[df['MasVnrType'].isnull()]))\n",
    "    #df.dropna(axis=0,subset=['MasVnrType'],inplace=True)\n",
    "    #df.drop('MasVnrType',axis=1,inplace=True)\n",
    "    # MasVnrType NA in all. filling with most popular values\n",
    "    df.MasVnrType.fillna(value=df['MasVnrType'].mode()[0],inplace=True)\n",
    "    \n",
    "    #MasVnrArea - remove the hole column\n",
    "    df.drop('MasVnrArea',axis=1,inplace=True)\n",
    "    \n",
    "    #Condition2 - remove the hole column Possui quantidade de tipos diferentes na base de treino e teste e apenas \n",
    "    #um dos tipos é relevante    \n",
    "    df.drop('Condition2',axis=1,inplace=True)\n",
    "    \n",
    "    #RoofMatl - remove the hole column Possui quantidade de tipos diferentes na base de treino e teste e apenas \n",
    "    #um dos tipos é relevante    \n",
    "    df.drop('RoofMatl',axis=1,inplace=True)\n",
    "    \n",
    "\n",
    "    #MSZoning   - tem NA apenas na base de teste. Como nao posso remover linhas removo a coluna   \n",
    "    #df.dropna(axis=0,subset=['MSZoning'],inplace=True)\n",
    "    df.drop('MSZoning',axis=1,inplace=True)\n",
    "    #df.MSZoning.fillna(df['MSZoning'].mode()[0])\n",
    "\n",
    "    \n",
    "    #BsmtQual\n",
    "    df.BsmtQual.fillna(inplace=True,value='No')\n",
    "    \n",
    "    #HouseStyle - Esse valor so existe na base de treino. Ao inves de remover toda coluna removo somente as linhas \n",
    "    df.drop(df[df.HouseStyle=='2.5Fin'].index,inplace=True)\n",
    "    \n",
    "    #BsmtCond\n",
    "    df.BsmtCond.fillna(inplace=True,value='No')\n",
    "\n",
    "    #BsmtExposure\n",
    "    df.BsmtExposure.fillna(inplace=True,value='No')\n",
    "\n",
    "    #BsmtFinType1\n",
    "    df.BsmtFinType1.fillna(inplace=True,value='No')\n",
    "\n",
    "    #BsmtFinType2\n",
    "    df.BsmtFinType2.fillna(inplace=True,value='No')\n",
    "\n",
    "    #Electrical - remove the records where the value is NA\n",
    "    #print(\"Number of lines where Electrical has Nan value\",len(df[df['Electrical'].isnull()]))\n",
    "    df.dropna(axis=0,subset=['Electrical'],inplace=True) # apenas no treino \n",
    "    df.drop(df[df.Electrical=='Mix'].index,inplace=True) # apenas no treino\n",
    "    #print(\"Number of lines where Electrical has Nan value\",len(df[df['Electrical'].isnull()]))\n",
    "\n",
    "    #FireplaceQu\n",
    "    df.FireplaceQu.fillna(inplace=True,value='No')\n",
    "    \n",
    "\n",
    "    #GarageType\n",
    "    df.GarageType.fillna(inplace=True,value='No')\n",
    "\n",
    "    #GarageYrBlt - remove the hole column\n",
    "    df.drop('GarageYrBlt',axis=1,inplace=True)\n",
    "\n",
    "    #GarageFinish\n",
    "    df.GarageFinish.fillna(inplace=True,value='No')\n",
    "\n",
    "    #GarageQual - A base de teste nao tem um dos tipos presente na base de treino. Assim a base de treino terá uma \n",
    "    #feature para esse tipo e a de teste não. Alem disso, apenas um tipo é pertinente\n",
    "    #Achei melhor entao excluir essa coluna    \n",
    "    df.drop('GarageQual',axis=1,inplace=True)\n",
    "    #df.drop(df[df.GarageQual=='Ex'].index,inplace=True)\n",
    "    \n",
    "    #GarageCond\n",
    "    df.GarageCond.fillna(inplace=True,value='No')\n",
    "\n",
    "    #PoolQC\n",
    "    #df.PoolQC.fillna(inplace=True,value='No')\n",
    "    df.drop('PoolQC',axis=1,inplace=True)\n",
    "    \n",
    "    #Fence\n",
    "    df.Fence.fillna(inplace=True,value='No')\n",
    "\n",
    "    #MiscFeature\n",
    "    #df.MiscFeature.fillna(inplace=True,value='No')\n",
    "    df.drop('MiscFeature',axis=1,inplace=True)\n",
    "\n",
    "    #MiscVal\n",
    "    df.drop('MiscVal',axis=1,inplace=True)\n",
    "    \n",
    "    #SaleType\n",
    "    df.drop('SaleType',axis=1,inplace=True)\n",
    "    \n",
    "    #Exterior1st- nao posso remover linhas do teste\n",
    "    #df.dropna(axis=0,subset=['Exterior1st'],inplace=True)     \n",
    "    #df.drop(df[df.Exterior1st=='Stone'].index,inplace=True)\n",
    "    #df.drop(df[df.Exterior1st=='ImStucc'].index,inplace=True)\n",
    "    #df.drop(df[df.Exterior1st=='CBlock'].index,inplace=True)\n",
    "    df.drop('Exterior1st',axis=1,inplace=True)\n",
    "    \n",
    "    #Exterior2nd\n",
    "    #df.dropna(axis=0,subset=['Exterior2nd'],inplace=True)\n",
    "    #df.Exterior2nd.fillna(inplace=True,value= 'Other')\n",
    "    #df.drop(df[df.Exterior2nd=='Other'].index,inplace=True)\n",
    "    #df.drop(df[df.Exterior2nd=='CBlock'].index,inplace=True)\n",
    "    df.drop('Exterior2nd',axis=1,inplace=True)\n",
    "    \n",
    "    #Heating -- esses tipos existem apenas na base de treino\n",
    "    df.drop(df[df.Heating=='OthW'].index,inplace=True)\n",
    "    df.drop(df[df.Heating=='Floor'].index,inplace=True)\n",
    "    \n",
    "    #KitchenQual\n",
    "    #df.dropna(axis=0,subset=['KitchenQual'],inplace=True)\n",
    "    df.KitchenQual.fillna(inplace=True,value='Fa') #- Apenas a base de teste tem NA e como nao posso remover registro\n",
    "    #dessa base setei o valor menos comum\n",
    "    \n",
    "    #Functional\n",
    "    #df.dropna(axis=0,subset=['Functional'],inplace=True)\n",
    "    df.drop('Functional',axis=1,inplace=True)\n",
    "    \n",
    "    #Utilities\n",
    "    df.drop('Utilities',axis=1,inplace=True)\n",
    "    \n",
    "    #BsmtFinSF1\n",
    "    #df.dropna(axis=0,subset=['BsmtFinSF1'],inplace=True)\n",
    "    df['BsmtFinSF1'] = imp.fit_transform(df['BsmtFinSF1']).transpose()    \n",
    "    \n",
    "    #BsmtFinSF2\n",
    "    #df.dropna(axis=0,subset=['BsmtFinSF2'],inplace=True)\n",
    "    df['BsmtFinSF2'] = imp.fit_transform(df['BsmtFinSF2']).transpose()    \n",
    "    \n",
    "    #BsmtUnfSF\n",
    "    #df.dropna(axis=0,subset=['BsmtUnfSF'],inplace=True)\n",
    "    df.drop('BsmtUnfSF',axis=1,inplace=True)\n",
    "    \n",
    "    #TotalBsmtSF\n",
    "    #df.dropna(axis=0,subset=['TotalBsmtSF'],inplace=True)\n",
    "    #df['TotalBsmtSF'] = imp.fit_transform(df['TotalBsmtSF']).transpose()    \n",
    "    df.TotalBsmtSF.fillna(value=0,inplace=True)\n",
    "    \n",
    "    #BsmtFullBath - apenas na base de teste tem NA.Nao posso remover a linha\n",
    "    df.BsmtFullBath.fillna(inplace=True,value=0)\n",
    "    \n",
    "    #BsmtHalfBath- apenas na base de teste tem NA.Nao posso remover a linha\n",
    "    df.BsmtHalfBath.fillna(inplace=True,value=0)\n",
    "    \n",
    "    #GarageCars\n",
    "    #df.dropna(axis=0,subset=['GarageCars'],inplace=True)\n",
    "    df.GarageCars.fillna(value=0,inplace=True)\n",
    "    \n",
    "    #GarageArea\n",
    "    #df.dropna(axis=0,subset=['GarageArea'],inplace=True)\n",
    "    df.GarageArea.fillna(value=0,inplace=True)\n",
    "    \n",
    "df = pd.read_csv(\"train.csv\",na_values=['?','NA'],delimiter=',',delim_whitespace=False)\n",
    "df_test = pd.read_csv(\"test.csv\",na_values=['?','NA'],delimiter=',',delim_whitespace=False)\n",
    "\n",
    "print(df.shape)\n",
    "print(df_test.shape)\n",
    "#print(df.head())\n",
    "#print(df.describe())\n",
    "#print(df.dtypes)\n",
    "#df = df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "########################Dealing with missing values\n",
    "\n",
    "#missing data\n",
    "# total = df.isnull().sum().sort_values(ascending=False)\n",
    "# percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "# print(missing_data.head(20))\n",
    "\n",
    "# \n",
    "#               Total   Percent\n",
    "# PoolQC         1453  0.995205\n",
    "# MiscFeature    1406  0.963014\n",
    "# Alley          1369  0.937671\n",
    "# Fence          1179  0.807534\n",
    "# FireplaceQu     690  0.472603\n",
    "# LotFrontage     259  0.177397\n",
    "# GarageCond       81  0.055479\n",
    "# GarageType       81  0.055479\n",
    "# GarageYrBlt      81  0.055479\n",
    "# GarageFinish     81  0.055479\n",
    "# GarageQual       81  0.055479\n",
    "# BsmtExposure     38  0.026027\n",
    "# BsmtFinType2     38  0.026027\n",
    "# BsmtFinType1     37  0.025342\n",
    "# BsmtCond         37  0.025342\n",
    "# BsmtQual         37  0.025342\n",
    "# MasVnrArea        8  0.005479\n",
    "# MasVnrType        8  0.005479\n",
    "# Electrical        1  0.000685\n",
    "# Utilities         0  0.000000\n",
    "\n",
    "\n",
    "\n",
    "#print(df.columns[df.isnull().any()])\n",
    "#'LotFrontage', 'Alley', 'MasVnrType', 'MasVnrArea', 'BsmtQual',\n",
    "#       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "#       'Electrical', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n",
    "#       'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence',\n",
    "#       'MiscFeature'\n",
    "input_missing_value(df)\n",
    "\n",
    "\n",
    "#print(df_test.columns[df_test.isnull().any()])\n",
    "#Index(['MSZoning', 'LotFrontage', 'Alley', 'Utilities', 'Exterior1st',\n",
    "#       'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond',\n",
    "#       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n",
    "#      'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n",
    "#       'BsmtHalfBath', 'KitchenQual', 'Functional', 'FireplaceQu',\n",
    "#      'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea',\n",
    "#       'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature',\n",
    "#       'SaleType'],\n",
    "\n",
    "input_missing_value(df_test)\n",
    "\n",
    "\n",
    "#Valores numericos que continham NA sao detectados como String. Assim, depois que removemos o NA temos que setar corretamente \n",
    "#o tipo \n",
    "df_test.BsmtFullBath = df_test.BsmtFullBath.astype(\"int64\")\n",
    "df_test.BsmtHalfBath = df_test.BsmtHalfBath.astype(\"int64\")\n",
    "df_test.GarageCars = df_test.GarageCars.astype(\"int64\")\n",
    "df_test.GarageArea = df_test.GarageArea.astype(\"int64\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df_test.shape)\n",
    "print(df.dtypes)\n",
    "print(\"Null values treino \\n\", df.columns[df.isnull().any()])\n",
    "print(\"Null values test \\n\", df_test.columns[df_test.isnull().any()])\n",
    "\n",
    "########################End dealing with missing values\n",
    "\n",
    "\n",
    "# The OneHotEncoder converts features represented as numeric codes (so they are values that can't be ordered)\n",
    "#to their binary representation\n",
    "#enc = preprocessing.OneHotEncoder() \n",
    "#df = enc.fit_transform(df)\n",
    "\n",
    "\n",
    "########################Tratando campos nominais\n",
    "\n",
    "vec = DictVectorizer()\n",
    "aux = np.asmatrix(vec.fit_transform(df.transpose().to_dict().values()).toarray())\n",
    "\n",
    "data_train = pd.DataFrame(aux,columns=vec.feature_names_)\n",
    "#data_train = pd.get_dummies(df)\n",
    "\n",
    "\n",
    "data_train.to_csv('train_no_categorical.csv')\n",
    "\n",
    "print(\"New shape train:\" , np.shape(data_train))\n",
    "print(\"Indice da coluna SalePrice no novo dataset\" , data_train.columns.get_loc('SalePrice'))\n",
    "\n",
    "################################################# Base de teste\n",
    "\n",
    "vec = DictVectorizer()\n",
    "aux_test = vec.fit_transform(df_test.transpose().to_dict().values()).toarray()\n",
    "data_test = pd.DataFrame(aux_test,columns=vec.feature_names_)\n",
    "#data_test = pd.get_dummies(df_test)\n",
    " \n",
    "print(\"New shape test:\" , np.shape(data_test))\n",
    "\n",
    "data_test.to_csv('test_no_categorical.csv')\n",
    "\n",
    "\n",
    "print(\"Colunas que existem apenas teste : \" , data_test.columns.difference(data_train.columns))\n",
    "print(\"Colunas que existem apenas treino : \" , data_train.columns.difference(data_test.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count      1445.000000\n",
      "mean     181043.215225\n",
      "std       79195.218195\n",
      "min       34900.000000\n",
      "25%      130000.000000\n",
      "50%      163000.000000\n",
      "75%      214000.000000\n",
      "max      755000.000000\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAAPUCAYAAABMx1tcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f545c250860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAAFoCAYAAADzZ0kIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiSpEoYOSZJUCUOHJEmqhKFDkiRV4v8DcZ61JHR3uyoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f545c0f59b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhMAAAFoCAYAAADtrnm7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkooYJiRJUpH/D2/J0Sp5ix+jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5428f79128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAAFoCAYAAADzZ0kIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJElSEf8PXu1sVKaiyEEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5428daa2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAFoCAYAAADUycjgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAGWxJREFUeJzt3X+Q3PV93/Hn3YlzBL4z5CBSpsONRx76xpFxfFJkO3Z+\nQmeIPdDEtZNJgWQoQ5mpbWwsGxtSTyBpbVqICRM72BiooTZm7EnpgFEaO4UmqUvwiMpgIaq3qQmR\nbAmQhZQ7iYPjfvSPXaXHsdr77N7e7t7t8zGj0en72d3v+9766valz/ez32/f3NwckiRJi+nvdAGS\nJGllMDRIkqQihgZJklTE0CBJkooYGiRJUhFDgyRJKmJokCRJRQwNkiSpiKFBkiQVMTRIkqQiaxp9\nQkSMAjcBvwS8DPwF8OHMHI+Is4HrgDOBPcB1mfnVec/9EPB+YD3wPeCKzNyx5O9CkiQtu2ZmGr4B\nPA+cDmwGNgJ/FBHrgXuBm4HTgCuAWyNiE0BEnA9cA1wErAPuB+6PiLVL/SYkSdLyayg0RMTrgO3A\n1Zk5mZn7gDupzDpcCGRm3pmZU5n5AHAfcGn16ZcBX8rMRzLzJeAGYA44v0XfiyRJWkYNhYbM/IfM\nvDQzD8zbfDrwIyqzDgtPNewAtlS/fsV4Zs4Bj84blyRJXWxJCyEj4ueADwKfAkaAQwse8jxwavXr\nxcYlSVIXa3gh5DER8U4qpx8+kZkPRsQngL5FnrbYeF1zc3NzfX1LeglJknrVkt9AmwoN1UWNXwY+\nkJl3VTcfoDKbMN8I8Nwi4ztL99vX18f4+CQzM7ONF92DBgb6GR5ea88aVNK3qakpHn+8/qH7pjed\nxeDg4HKU2HU81ppj3xpnz5pzrG9L1cxHLt8B3AG8t7rY8ZhHgIsXPHwL8J1545uphA0ioh/YBNzW\nyP5nZmaZnvZAaYQ9a069vj322GN8/MZ7GBoZrTk+cXAP12+dZWxs83KW2HU81ppj3xpnzzqjodAQ\nEQPArVROSTywYPgu4NqIuKT69TnAu4C3Vcc/D9wdEXdTuUbDlcCLwLbmy5c6Z2hklJPXn9HpMiSp\nbRqdafh5Khdu+pOI+CyVj0z2VX8P4Dzgs8CfAk8DF2bmLoDM/GZEXA18ncp1HLYD765+/FKSJHW5\nhkJDZn4bGKjzkL3AWJ3n3wLc0sg+JUlSd/DeE5IkqYihQZIkFTE0SJKkIoYGSZJUxNAgSZKKGBok\nSVIRQ4MkSSpiaJAkSUUMDZIkqYihQZIkFTE0SJKkIoYGSZJUxNAgSZKKGBokSVIRQ4MkSSpiaJAk\nSUUMDZIkqciaThcgdaOpqSl27PjfzMzM1hzP3N3miiSp8wwNUg2PPfYYH73hzxgaGa05/uxT21m3\nYUubq5KkzjI0SMcxNDLKyevPqDk2cXBvm6uRpM5zTYMkSSpiaJAkSUUMDZIkqYihQZIkFTE0SJKk\nIoYGSZJUxNAgSZKKGBokSVIRQ4MkSSriFSGlZTA7M133/hQbN57F4OBgGyuSpKUzNEjL4Ojh/dy+\nbR9DDx951djEwT1cvxXGxjZ3oDJJap6hQVom9e5dIUkrkWsaJElSEUODJEkqYmiQJElFDA2SJKmI\noUGSJBUxNEiSpCKGBkmSVMTQIEmSihgaJElSEUODJEkqYmiQJElFDA2SJKmIoUGSJBUxNEiSpCKG\nBkmSVMTQIEmSihgaJElSEUODJEkqYmiQJElFDA2SJKmIoUGSJBUxNEiSpCKGBkmSVMTQIEmSihga\nJElSEUODJEkqYmiQJElFDA2SJKmIoUGSJBUxNEiSpCKGBkmSVMTQIEmSihgaJElSEUODJEkqsqbR\nJ0TEucCdwIOZecG87b8M/A/gxeqmPmAO+J3M/C/Vx3wIeD+wHvgecEVm7ljSdyBJktqiodAQEVcC\nlwDfP85Dns7MDcd57vnANcC5wE7gw8D9EfGGzJxspA5JktR+jZ6emATeCvygiX1dBnwpMx/JzJeA\nG6jMRJzfxGtJkqQ2ayg0ZObnMnOizkOGI+KeiDgQEXsj4iPzxjYD/3gqIjPngEeBLQ1VLEmSOqKV\nCyHHqaxTuBH4aSqnMa6JiIur4yPAoQXPeR44tYU1SJKkZdLwQsjjyczvAmfP2/SXEfEF4F8Bd1S3\n9S11PwMDfuCj1LFe2bPGtKNfAwP9rFmzev5ePNaaY98aZ8+a06p+tSw0HMfTwHurXx+gMtsw3wiV\nRZHFhofXLr2qHmPPus/w8FpOOeWkTpfRch5rzbFvjbNnndGy0BAR7wNOzcwvzNv8M8BT1a8fobKu\n4cvVx/cDm4DbGtnP+PgkMzOzSy+4BwwM9DM8vNaeNagd/4MZH5/k0KGjy76fdvFYa459a5w9a86x\nvi1VK2capoA/ioj/C/wV8KvAxcDvVMc/D9wdEXdTWftwJZVrOmxrZCczM7NMT3ugNMKedZfZmWme\neOKJuj/wNm48i8HBwTZW1Roea82xb42zZ53R6HUaJql8TPKE6p/fA8xl5omZeV9EXAF8DjgdeAb4\nUGbeC5CZ34yIq4GvA6cB24F3Vz9+KfWMo4f3c/u2fQw9fKTm+MTBPVy/FcbGNre5Mkmqr6HQkJl1\n5zYy8zbqnG7IzFuAWxrZp7QaDY2McvL6MzpdhiQ1xOWnkiSpiKFBkiQVMTRIkqQihgZJklTE0CBJ\nkooYGiRJUhFDgyRJKmJokCRJRQwNkiSpiKFBkiQVMTRIkqQihgZJklTE0CBJkooYGiRJUhFDgyRJ\nKmJokCRJRQwNkiSpiKFBkiQVMTRIkqQihgZJklTE0CBJkooYGiRJUhFDgyRJKmJokCRJRQwNkiSp\niKFBkiQVMTRIkqQihgZJklTE0CBJkooYGiRJUhFDgyRJKmJokCRJRQwNkiSpiKFBkiQVMTRIkqQi\nhgZJklTE0CBJkooYGiRJUhFDgyRJKmJokCRJRQwNkiSpiKFBkiQVWdPpAqROmJqaYteunTXHBgb6\n+eEP/67NFUlS9zM0qCft2rWTj994D0MjozXHn31qO+s2bGlzVZLU3QwN6llDI6OcvP6MmmMTB/e2\nuRpJ6n6uaZAkSUUMDZIkqYihQZIkFTE0SJKkIoYGSZJUxNAgSZKKGBokSVIRQ4MkSSpiaJAkSUUM\nDZIkqYihQZIkFTE0SJKkIoYGSZJUxNAgSZKKGBokSVIRQ4MkSSpiaJAkSUUMDZIkqYihQZIkFTE0\nSJKkIoYGSZJUxNAgSZKKrGn0CRFxLnAn8GBmXrBg7GzgOuBMYA9wXWZ+dd74h4D3A+uB7wFXZOaO\n5suXJEnt0tBMQ0RcCdwEfL/G2HrgXuBm4DTgCuDWiNhUHT8fuAa4CFgH3A/cHxFrl/INSJKk9mj0\n9MQk8FbgBzXGLgQyM+/MzKnMfAC4D7i0On4Z8KXMfCQzXwJuAOaA85srXZIktVNDoSEzP5eZE8cZ\n3gwsPNWwA9hSazwz54BH541LkqQu1vCahjpGgL0Ltj0PnDpv/FCd8SIDA67dLHWsV/bs1bq9JwMD\n/axZ0901zuex1hz71jh71pxW9auVoQGgb4njixoedglEo+zZq3V7T4aH13LKKSd1uoyGdXtfu5V9\na5w964xWhoYDVGYT5hsBnltkfGcjOxkfn2RmZrapAnvNwEA/w8Nr7VkN4+OTnS6hrvHxSQ4dOtrp\nMop5rDXHvjXOnjXnWN+WqpWh4RHg4gXbtgDfmTe+GfgyQET0A5uA2xrZyczMLNPTHiiNsGev1u0/\nbFbq39lKrbvT7Fvj7FlntDI03AVcGxGXVL8+B3gX8Lbq+OeBuyPibirXaLgSeBHY1sIaJEnSMmn0\nOg2TEfEClWst/Oa8P5OZB4DzgMuBw8BngAszc1d1/JvA1cDXgYNUQsW7qx+/lCRJXa6hmYbMrHtC\nJDO/DYzVGb8FuKWRfUqSpO7gZ1YkSVIRQ4MkSSpiaJAkSUUMDZIkqYihQZIkFTE0SJKkIoYGSZJU\npNU3rJK0RLMz02TurvuYjRvPYnBwsE0VSVKFoUHqMkcP7+f2bfsYevhIzfGJg3u4fiuMjW1uc2WS\nep2hQepCQyOjnLz+jE6XIUmv4JoGSZJUxNAgSZKKeHpCq9LU1BS7du087vhiCw0lSa9maNCqtGvX\nTj5+4z0MjYzWHH/2qe2s27ClzVVJ0spmaNCqVW8x4cTBvW2uRpJWPtc0SJKkIoYGSZJUxNAgSZKK\nGBokSVIRQ4MkSSpiaJAkSUUMDZIkqYihQZIkFTE0SJKkIoYGSZJUxNAgSZKKGBokSVIRQ4MkSSpi\naJAkSUUMDZIkqYihQZIkFTE0SJKkIoYGSZJUxNAgSZKKGBokSVIRQ4MkSSpiaJAkSUUMDZIkqYih\nQZIkFTE0SJKkIms6XYCkxszOTJO5u+5jNm48i8HBwTZVJKlXGBqkFebo4f3cvm0fQw8fqTk+cXAP\n12+FsbHNba5M0mpnaJBWoKGRUU5ef0any5DUY1zTIEmSihgaJElSEUODJEkqYmiQJElFDA2SJKmI\noUGSJBUxNEiSpCKGBkmSVMTQIEmSihgaJElSEUODJEkqYmiQJElFDA2SJKmIoUGSJBUxNEiSpCKG\nBkmSVMTQIEmSihgaJElSEUODJEkqYmiQJElFDA2SJKmIoUGSJBUxNEiSpCKGBkmSVGRNK18sImaB\nl4A5oK/6+62Z+eGIOBu4DjgT2ANcl5lfbeX+JUnS8mlpaKASEv5pZu6dvzEi1gP3Ah8E7gZ+Ebgv\nInZn5o4W1yBJkpZBq0NDX/XXQhcCmZl3Vv/8QETcB1wKvL/FNUiSpGXQ6tAA8B8j4h3AMPA14KPA\nZmDhjMIO4LeWYf+SJGkZtDo0/C3wLeB3gQ1UQsPNwAiwd8FjnwdObXQHAwOu3Sx1rFe92LNe/J7n\nGxjoZ82a9vWgl4+1pbBvjbNnzWlVv1oaGjLznfP/GBFXAd8A/obapy0aNjy8thUv01N6sWe9+D3P\nNzy8llNOOakj+1Xj7Fvj7FlnLMfpifmeBgaAWSqzDfONAM81+oLj45PMzMwuvbIeMDDQz/Dw2p7s\n2fj4ZKdL6Kjx8UkOHTratv318rG2FPatcfasOcf6tlQtCw0R8Rbgosz82LzNPwO8CPw5cPGCp2wB\nvtPofmZmZpme9kBpRC/2rNd/mHTq77wXj7VWsG+Ns2ed0cqZhueAyyLiOeAm4PXAHwK3AF8BromI\nS4C7gHOAdwFva+H+JUnSMmrZSpLM3Ae8G/h14MfAt6nMMHwiMw8A5wGXA4eBzwAXZuauVu1fkiQt\nr1YvhPw28M46Y2Ot3J8kSWofP7MiSZKKGBokSVIRQ4MkSSpiaJAkSUUMDZIkqchyXxFSWhZTU1Ps\n2rXzuOOZu9tYjST1BkODVqRdu3by8RvvYWhktOb4s09tZ92GLW2uSpJWN0ODVqyhkVFOXn9GzbGJ\ngwtvqipJWirXNEiSpCKGBkmSVMTTE+pKLnRs3uzM9KL92bjxLAYHB9tUkaTVwtCgruRCx+YdPbyf\n27ftY+jhIzXHJw7u4fqtMDa2uc2VSVrpDA3qWi50bF693klSs1zTIEmSihgaJElSEUODJEkqYmiQ\nJElFDA2SJKmIoUGSJBUxNEiSpCKGBkmSVMTQIEmSinhFSHWE95aQpJXH0KCO8N4SkrTyGBrUMd5b\nQpJWFtc0SJKkIoYGSZJUxNAgSZKKGBokSVIRQ4MkSSpiaJAkSUUMDZIkqYihQZIkFTE0SJKkIoYG\nSZJUxMtIqymL3XAKYOPGsxgcHGxTRZKk5WZoUFMWu+HUxME9XL8VxsY2t7kySdJyMTSoafVuOCVJ\nWn0MDVKPmZ2ZJnN33cd4aklSLYYGqcccPbyf27ftY+jhIzXHPbUk6XgMDVIP8tSSpGYYGrQsFpsC\nX2x6XJLUfQwNq9jU1BTbtz/B+PgkMzOzrxpfzvPWi02BP/vUdtZt2LIs+5YkLQ9Dwyr2+OM7+egN\nf1bzY5HtOG9dbwp84uDeZduvJGl5GBpWOc9dS5JaxdCgmha74qNrEiSp9xgaVNNiV3x0TYIk9R5D\ng47LNQmqpdYs1MBAP8PDa/9x0a0Xh5JWJ0ODpIZ43xGpdxkaJDXMBbZSb+rvdAGSJGllMDRIkqQi\nnp7oYot97BG8G6EkqX0MDXV0+k3bBWeSpG5iaKijG960XXAmSeoWhoZF+KYtSVKFoaGDvFSzJGkl\nMTR0kJdqliStJIaGDvNSzeo2szPTdWe5nAGTepehYQkW++H68ssvA3DCCSfUHPeHr7rR0cP7uX3b\nPoYePlJz3BkwqXcZGpag5Ifria9b5+kHrTjOgEmqxdCwRIv9cB0aOb0rf/g6Ba3lstQZOPCiZVK3\nMjT0KKegtVyWOgPnRcuk7tXToaHXP/LoFLSWy1Jm4CR1r54ODX7kUVp9On35d2k16+nQAP5vW1pt\nuuHy79Jq1dbQEBGjwM3A24EJ4GuZeVU7a1hNFltw9uST2cZqpPYoOa3Yqcu/O8uh1a7dMw33ANuB\n3wbWAX8eEc9k5k1trmNVcDGjVqOST/bcvu2Jpk8rLvb69d7USwJLvdqc5Wjesd4PDPQzPLyW8fFJ\nZmZmX/EYA9nya1toiIifA94MnJ2ZR4AjEXEj8GHA0NAkT69otSkNw80e9/Vef7E39dJ1UC7ybD1P\nO3WHds40bAKezszxedt2ABERJ2Xm0VbvcHp6muv/+E+YneurOb5/3x7gzFbvVtISLXcYPt7rl8xy\nLKW2ha+/8H/Ni13DotPXuOj06ZelnHbqdO2rRTtDwwhwaMG256u/nwoUhYaBgf7iHU5OvsSjf/cC\nrx19R83x/ROTTE/tOe7zX/iHZ4C5VTnezbWt9vFurq3Xxw/8/Xe56QdTnDj83Zrjz+9PTht9c9P7\nLnn9nzjpFE4c/qmmxl8Yf46tF/8aZ575xuPWsBS7d/8fbrzjLzqy/yefTCYOHv/n9cTBPTz55NBx\n3yNKav/if/gImzatzpmKRt476+mbmzv+Ad5KEXE18J7MfOu8bW8Avg9syMy/b0shkiSpKa2JHmUO\nUJltmG+ESiw/0MY6JElSE9oZGh4BRiPiJ+dteyvwRGa+0MY6JElSE9p2egIgIh4CHgc+CvwTYBtw\nQ2Z+oW1FSJKkprRzpgHgfVTCwjPAg8AdBgZJklaGts40SJKklavdMw2SJGmFMjRIkqQihgZJklTE\n0CBJkooYGiRJUhFDgyRJKtLOG1Y1JSJ+EfgWr7wLTD9wQmYOdKaq7hcRbwE+Q+XuopPAA8BHMvPH\nHS2sy0XEZuB6YDMwAdyUmZ/pbFXdJyLOBe4EHszMCxaMnQ1cR+UWsnuA6zLzq+2vsrvU61l1/GPA\np4DLM/OL7a6vWy1yrP0ylWNtI/Bj4D9l5qfaX2V3WaRnvwl8EthApWdfA34vM2dLXrvrZxoy839m\n5trMPPHYL+APqHyjqiEiBqhcbfMh4DQq/6B+CvjTTtbV7SLiFOC/AX8LrAfOBT4QEe/taGFdJiKu\nBG6icrO5hWPrgXuBm6kce1cAt0bEprYW2WXq9aw6fj/wK/z/O/+KRY+104H7gS8BPwn8NvCxiHhV\nIOsli/RsE3AHcGVmDgHnARcDHyh9/a4PDQtFxCiwFbiy07V0sZ+u/vpKZk5n5iHgHmCss2V1vZ8H\nXpuZn8zMFzPzCeAG4NIO19VtJqncN+YHNcYuBDIz78zMqcx8ALgPe1ivZwAPZeZ5wIvtK2lFqNe3\ndcCtmXlrZs5k5nbgvwO/1M4Cu1C9nr0A/MvM/BZAZu4C/hfwptIX7/rTEzX8IXBbZv6o04V0sR8B\n3wUui4jfB04C3gt8o6NVrQxzEdGXmcdOhx0G3tLJgrpNZn4OICJqDW8GdizYtgP4rWUuq6st0jMy\n89NtLWiFqNe3zHyEyo0Q5zsd+N7yV9a9FunZbmB3dbwf+FXgF4CLSl9/Rc00RMTrgfcAf9zhUrpa\n9Q3vfcBvAOPAfmAA+L1O1rUCPEQlif+7iFgbEW8A/g2VqU+VGQEOLdj2PHBqB2pRD4mIy6mcp/d+\nRouIiIuAl6jMQP/bzPzL0ueuqNBA5bzLPZn5XKcL6WYRMUhlVuFrwOuo3CRsHOj5xWj1ZOZh4NeB\nf0YlaP3n6q/pTta1AvV1ugD1loj4IJW1bv88Mw90up5ul5lfAV4DvAv4/Yj416XPXWmnJ95HZT2D\n6jsHeH1mHptZOBIR1wCPRsTJ1TdH1ZCZDwFvP/bniPgXVE73qMwBKrMN840ABn0ti4j491QW8/1K\nZvb0qYlGVD8t8VBE3AxcDtxa8rwVM9MQET8LjALF0yg9bADor56zOuYneOXHVrVARLwmIn43Il47\nb/O5VE5bqMwjVNY1zLcF+E4HatEqFxFbqXxq4u0GhsVFxFUR8eUFm2eBl0tfYyXNNIwBBzPzSKcL\nWQEeAo4AfxARnwZOpLKe4a+dZahrCrgGeGNEfJLKjM0FVBYKqcxdwLURcUn163OoTIG+raNVadWJ\niA3AtVQCww87XM5K8ddU/n3+VyofjT6TyrqtO0pfoG9ubmX85zMirgIuyMw3d7qWlSAixqhc3Oln\nqSx4+Stga2Y+08m6ul31c8xfpPKPaS/wicy8r7NVdZeImKQya3VCddM0MFe9hgoR8QvAZ6n08Gng\nqsy8twOldo16PVtwAbvXVMdmgL/JzF/rRL3dYpG+fZJKaJia95Q+4OnMfGNbC+0iBf8+fwP4NPB6\n4Fkqa92uzcyi2YYVExokSVJnrZg1DZIkqbMMDZIkqYihQZIkFTE0SJKkIoYGSZJUxNAgSZKKGBok\nSVIRQ4MkSSpiaJAkSUUMDZIkqYihQZIkFfl/lZqD6E6milcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5428e5d908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg8AAAFoCAYAAADQPBjdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5467468c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "print(data_train.SalePrice.describe())\n",
    "\n",
    "saleprice_scaled = preprocessing.StandardScaler().fit_transform((data_train['SalePrice'][:,np.newaxis]));\n",
    "fig = plt.figure(1, figsize=(6, 12))\n",
    "#ax = fig.add_subplot(111)\n",
    "#ax.boxplot(saleprice_scaled)\n",
    "plt.boxplot(saleprice_scaled)\n",
    "\n",
    "plt.figure()\n",
    "x = plt.hist(data_train['SalePrice'],bins=50)\n",
    "\n",
    "plt.figure()\n",
    "saleprice_log = np.log(data_train['SalePrice'])\n",
    "x = plt.hist(saleprice_log,bins=50)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "x = plt.hist(data_train['LotArea'],bins=50)\n",
    "\n",
    "plt.figure()\n",
    "saleprice_log = np.log(data_train['LotArea'])\n",
    "x = plt.hist(saleprice_log,bins=50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "x = plt.hist(data_train['GarageCars'],bins=50)\n",
    "\n",
    "# plt.figure()\n",
    "# saleprice_log = np.log(data_train['GarageArea'])\n",
    "# x = plt.hist(saleprice_log,bins=50)\n",
    "\n",
    "\n",
    "#data_train['SalePrice'] = np.log(data_train['SalePrice'])\n",
    "data_train['OverallQual'] = np.log(data_train['OverallQual'])\n",
    "data_train['LotArea'] = np.log(data_train['LotArea'])\n",
    "\n",
    "\n",
    "data_test['OverallQual'] = np.log(data_test['OverallQual'])\n",
    "data_test['LotArea'] = np.log(data_test['LotArea'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
    "def encode_text_dummy(df,name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name,x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tentativa de selecionar melhores features \n",
      "\n",
      "\n",
      "As features selecionadas com Tree-based feature selection foram: \n",
      "\n",
      "['ExterQual=TA' 'OverallQual' 'GarageCars' 'BsmtQual=Ex' 'GrLivArea'\n",
      " 'TotalBsmtSF' 'Neighborhood=NoRidge' 'FireplaceQu=No' '1stFlrSF'\n",
      " '2ndFlrSF' 'BsmtFinSF1' 'TotRmsAbvGrd' 'FullBath' 'LotArea'\n",
      " 'MSSubClass=60' 'BedroomAbvGr' 'YearRemodAdd' 'GarageType=Attchd'\n",
      " 'YearBuilt' 'BsmtQual=Gd' 'Fireplaces' 'GarageArea' 'BsmtExposure=Gd'\n",
      " 'KitchenQual=TA' 'KitchenQual=Ex' 'ExterQual=Fa' 'BsmtFullBath']\n",
      "[[ 0.24506076  0.17339574  0.10220639  0.08477404  0.06769452  0.02122161\n",
      "   0.02114328  0.01799789  0.01759197  0.01331804  0.01144122  0.01097446\n",
      "   0.00868255  0.00749924  0.00676658  0.00647457  0.0064394   0.00586092\n",
      "   0.00583216  0.00502513  0.00501331  0.0049933   0.00462131  0.00456835\n",
      "   0.00449937  0.0044832   0.00376397]]\n",
      "\n",
      " New shape train apos Tree-based feature selection: (1445, 26)\n",
      "\n",
      " Fim tentativa selecionar melhores features \n",
      "\n",
      "\n",
      " New shape test apos Tree-based feature selection: (1459, 26)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Tentativa de selecionar melhores features \\n\")\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "#Removing features with low variance\n",
    "#print(\"Original shape: {}\".format(np.shape(df.iloc[:,0:-1])))\n",
    "#sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "#features = sel.fit_transform(df.iloc[:,0:-1])\n",
    "#print(\"Shape apos Removing features with low variance {}\".format(np.shape(features))) #nenhuma foi selecionada \n",
    "#print(\"\\n\")\n",
    "\n",
    "#Tree-based feature selection\n",
    "y_train = (data_train['SalePrice'])\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "\n",
    "print()\n",
    "\n",
    "clf = ExtraTreesRegressor(n_estimators=20)\n",
    "clf = clf.fit(x_train,y_train)\n",
    "data = np.zeros((1,x_train.shape[1])) \n",
    "data = pd.DataFrame(data, columns=x_train.columns)\n",
    "data.iloc[0] = clf.feature_importances_\n",
    "data = data.T.sort_values(df.index[0], ascending=False).T\n",
    "\n",
    "\n",
    "print(\"As features selecionadas com Tree-based feature selection foram: \\n\")\n",
    "yyy = np.asarray((data.columns[0:27]))\n",
    "xxx = np.asarray((data.iloc[:,0:27]))\n",
    "print(yyy)\n",
    "print(xxx)\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "aux = model.transform(x_train)\n",
    "\n",
    "print(\"\\n New shape train apos Tree-based feature selection: {}\".format(aux.shape))\n",
    "\n",
    "print(\"\\n Fim tentativa selecionar melhores features \\n\")\n",
    "\n",
    "\n",
    "data_train_less_features = pd.concat([pd.DataFrame(aux),pd.DataFrame(y_train)],axis=1)\n",
    "data_train_less_features.to_csv('data_train_less_features.csv')\n",
    "\n",
    "\n",
    "aux = model.transform((data_test))\n",
    "data_test_less_features = pd.DataFrame(aux)\n",
    "print(\"\\n New shape test apos Tree-based feature selection: {}\".format(aux.shape))\n",
    "data_test_less_features.to_csv('data_test_less_features.csv')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn   import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caso 1 - Linear Regression \n",
      "Fold #1\n",
      "Fold score (RMSE): 622243187960918912.00\n",
      "Accuracy: -62765513129602993115627520.000\n",
      "Fold #2\n",
      "Fold score (RMSE): 905009122196841856.00\n",
      "Accuracy: -131289954335369637232902144.000\n",
      "Fold #3\n",
      "Fold score (RMSE): 117636420432799248.00\n",
      "Accuracy: -1935378154193937026252800.000\n",
      "Fold #4\n",
      "Fold score (RMSE): 201586936866963200.00\n",
      "Accuracy: -7499709332072572561391616.000\n",
      "Fold #5\n",
      "Fold score (RMSE): 24891.00\n",
      "Accuracy: 0.901\n",
      "\n",
      " Average RMSE: 5.021361800848278e+17\n",
      "\n",
      "\n",
      "\n",
      "SGDRegressor \n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 46234.39\n",
      "Accuracy: 0.653\n",
      "Fold #2\n",
      "Fold score (RMSE): 3193689.37\n",
      "Accuracy: -1633.974\n",
      "Fold #3\n",
      "Fold score (RMSE): 1041515.60\n",
      "Accuracy: -150.710\n",
      "Fold #4\n",
      "Fold score (RMSE): 958658.73\n",
      "Accuracy: -168.609\n",
      "Fold #5\n",
      "Fold score (RMSE): 307935.01\n",
      "Accuracy: -14.108\n",
      "\n",
      " Average RMSE: 1568463.8772219159\n",
      "\n",
      "\n",
      " Less Features\n",
      "Fold #1\n",
      "Fold score (RMSE): 25195.78\n",
      "Accuracy: 0.879\n",
      "Fold #2\n",
      "Fold score (RMSE): 34522.20\n",
      "Accuracy: 0.818\n",
      "Fold #3\n",
      "Fold score (RMSE): 34675.22\n",
      "Accuracy: 0.841\n",
      "Fold #4\n",
      "Fold score (RMSE): 27179.47\n",
      "Accuracy: 0.856\n",
      "Fold #5\n",
      "Fold score (RMSE): 45643.30\n",
      "Accuracy: 0.690\n",
      "\n",
      " Average RMSE: 34208.23123874215\n",
      "\n",
      "\n",
      "\n",
      "SGDRegressor \n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 25378.30\n",
      "Accuracy: 0.878\n",
      "Fold #2\n",
      "Fold score (RMSE): 36372.82\n",
      "Accuracy: 0.798\n",
      "Fold #3\n",
      "Fold score (RMSE): 33540.48\n",
      "Accuracy: 0.852\n",
      "Fold #4\n",
      "Fold score (RMSE): 28021.28\n",
      "Accuracy: 0.847\n",
      "Fold #5\n",
      "Fold score (RMSE): 44504.18\n",
      "Accuracy: 0.705\n",
      "\n",
      " Average RMSE: 34228.11254807786\n"
     ]
    }
   ],
   "source": [
    "#Starting making predictors\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "\n",
    "#Caso 1 - Linear Regression \n",
    "print(\"Caso 1 - Linear Regression \")\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "data_train = data_train.reindex(np.random.permutation(data_train.index))\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "#Normalization\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "classifier = LinearRegression()\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "# The mean squared error\n",
    "#pred = classifier.predict(x_test_scaled)\n",
    "#score = metrics.mean_squared_error(y_test, pred)\n",
    "#print(\"Final, out of sample score (RMSE): {}\".format(score))    \n",
    "\n",
    "# Evaluate success using accuracy\n",
    "#print(\"Final Accuracy: %.3f\" % classifier.score(X=x_test_scaled,y=y_test))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"SGDRegressor \\n\\n\")\n",
    "\n",
    "\n",
    "classifier = SGDRegressor()\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "###########Less features\n",
    "\n",
    "print(\"\\n\\n Less Features\")\n",
    "\n",
    "#Normalization\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "classifierLinearRegression = LinearRegression()\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "    pred = []    \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifierLinearRegression = classifierLinearRegression.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifierLinearRegression.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifierLinearRegression.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "  \n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "# Write the cross-validated prediction\n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifierLinearRegression.predict(scaler.transform(data_test_less_features))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_LinearRegression.csv', columns=['SalePrice'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"SGDRegressor \\n\\n\")\n",
    "\n",
    "classifier = SGDRegressor()\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "Fold #1\n",
      "Fold score (RMSE): 39525.09\n",
      "Accuracy: 0.707\n",
      "Fold #2\n",
      "Fold score (RMSE): 23699.40\n",
      "Accuracy: 0.878\n",
      "Fold #3\n",
      "Fold score (RMSE): 39725.28\n",
      "Accuracy: 0.820\n",
      "Fold #4\n",
      "Fold score (RMSE): 23568.49\n",
      "Accuracy: 0.910\n",
      "Fold #5\n",
      "Fold score (RMSE): 25820.08\n",
      "Accuracy: 0.894\n",
      "\n",
      " Average RMSE: 31381.979449950468\n",
      "\n",
      "\n",
      " Less features \n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 24490.56\n",
      "Accuracy: 0.886\n",
      "Fold #2\n",
      "Fold score (RMSE): 33605.32\n",
      "Accuracy: 0.827\n",
      "Fold #3\n",
      "Fold score (RMSE): 34913.01\n",
      "Accuracy: 0.839\n",
      "Fold #4\n",
      "Fold score (RMSE): 25681.50\n",
      "Accuracy: 0.872\n",
      "Fold #5\n",
      "Fold score (RMSE): 44985.00\n",
      "Accuracy: 0.699\n",
      "\n",
      " Average RMSE: 33559.53943694814\n"
     ]
    }
   ],
   "source": [
    "#Caso 3 - SVM\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import NuSVR\n",
    "\n",
    "print(\"SVM\")\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "data_train = data_train.reindex(np.random.permutation(data_train.index))\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#Normalization\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "x_train = np.ascontiguousarray(x_train)\n",
    "\n",
    "\n",
    "classifier = NuSVR(kernel='linear', C=1e3) #34761.27693615821\n",
    "kf = KFold(5)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "\n",
    "# The mean squared error\n",
    "#pred = classifier.predict(x_test_scaled)\n",
    "#score = metrics.mean_squared_error(y_test, pred)\n",
    "#print(\"Final, out of sample score (RMSE): {}\".format(score))    \n",
    "\n",
    "# Evaluate success using accuracy\n",
    "#print(\"Final Accuracy: %.3f\" % classifier.score(X=x_test_scaled,y=y_test))\n",
    "\n",
    "###########Less features\n",
    "print(\"\\n\\n Less features \\n\")\n",
    "#Normalization\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "x_train = np.ascontiguousarray(x_train)\n",
    "\n",
    "\n",
    "#classifier = SVR(kernel='rbf', C=1e3, gamma=0.1) #66483.84692815947\n",
    "classifierSVR = SVR(kernel='linear', C=1e3) #34761.27693615821\n",
    "#classifier = SVR(kernel='poly', C=1e3, degree=3) #86747.4465877091\n",
    "#classifier = NuSVR(C=1e3) #57249.1589623674\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifierSVR = classifierSVR.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifierSVR.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifierSVR.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "    \n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "# Write the cross-validated prediction\n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifierSVR.predict(scaler.transform(data_test_less_features))\n",
    "\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "\n",
    "\n",
    "result.to_csv('pred_SVR.csv', columns=['SalePrice'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN MLPRegressor\n",
      "Fold #1\n",
      "Fold score (RMSE): 36219.85\n",
      "Accuracy: 0.740\n",
      "Fold #2\n",
      "Fold score (RMSE): 49890.26\n",
      "Accuracy: 0.594\n",
      "Fold #3\n",
      "Fold score (RMSE): 37256.48\n",
      "Accuracy: 0.792\n",
      "Fold #4\n",
      "Fold score (RMSE): 57485.03\n",
      "Accuracy: 0.533\n",
      "Fold #5\n",
      "Fold score (RMSE): 38793.61\n",
      "Accuracy: 0.762\n",
      "\n",
      " Average RMSE: 44717.857481245104\n",
      "\n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 27091.87\n",
      "Accuracy: 0.861\n",
      "Fold #2\n",
      "Fold score (RMSE): 39778.14\n",
      "Accuracy: 0.758\n",
      "Fold #3\n",
      "Fold score (RMSE): 38260.95\n",
      "Accuracy: 0.807\n",
      "Fold #4\n",
      "Fold score (RMSE): 25655.44\n",
      "Accuracy: 0.872\n",
      "Fold #5\n",
      "Fold score (RMSE): 48460.65\n",
      "Accuracy: 0.651\n",
      "\n",
      " Average RMSE: 36842.38250360394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Shuffle\n",
    "print(\"NN MLPRegressor\")\n",
    "np.random.seed(42)\n",
    "data_train = data_train.reindex(np.random.permutation(data_train.index))\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "#Normalization\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "classifier = MLPRegressor(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10,4,2), random_state=1)\n",
    "kf = KFold(5)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "\n",
    "# The mean squared error\n",
    "#pred = classifier.predict(x_test_scaled)\n",
    "#score = metrics.mean_squared_error(y_test, pred)\n",
    "#print(\"Final, out of sample score (RMSE): {}\".format(score))    \n",
    "\n",
    "# Evaluate success using accuracy\n",
    "#print(\"Final Accuracy: %.3f\" % classifier.score(X=x_test_scaled,y=y_test))\n",
    "\n",
    "###########Less features\n",
    "print(\"\\n\\n\")\n",
    "#Normalization\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "classifier = MLPRegressor(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(8,2), random_state=1)\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests\n",
      "Mean squared error: 29597.05109582514\n",
      "Accuracy: 0.852\n",
      "\n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 28951.730162219337\n",
      "Accuracy: 0.886\n",
      "Fold #2\n",
      "Fold score (RMSE): 25924.39997455174\n",
      "Accuracy: 0.897\n",
      "Fold #3\n",
      "Fold score (RMSE): 32718.1452740622\n",
      "Accuracy: 0.820\n",
      "Fold #4\n",
      "Fold score (RMSE): 30757.611548626795\n",
      "Accuracy: 0.822\n",
      "Fold #5\n",
      "Fold score (RMSE): 27059.08361496614\n",
      "Accuracy: 0.880\n",
      "\n",
      " Average RMSE: 29185.540590707198\n",
      "\n",
      " oob score : 0.8655222916876083\n",
      "\n",
      "\n",
      " Less features \n",
      "\n",
      "\n",
      "Mean squared error: 29747.78358186385\n",
      "Accuracy: 0.857\n",
      "\n",
      "\n",
      "\n",
      "Fold #1\n",
      "Fold score (RMSE): 36294.32835986868\n",
      "Accuracy: 0.750\n",
      "Fold #2\n",
      "Fold score (RMSE): 39355.108645266264\n",
      "Accuracy: 0.763\n",
      "Fold #3\n",
      "Fold score (RMSE): 41209.549168746766\n",
      "Accuracy: 0.776\n",
      "Fold #4\n",
      "Fold score (RMSE): 36140.77541894397\n",
      "Accuracy: 0.746\n",
      "Fold #5\n",
      "Fold score (RMSE): 39628.79999529705\n",
      "Accuracy: 0.766\n",
      "\n",
      " Average RMSE: 38577.006693666204\n",
      "\n",
      " oob score : 0.7588977163258779\n"
     ]
    }
   ],
   "source": [
    "##Random Forests\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Shuffle\n",
    "print(\"Random Forests\")\n",
    "np.random.seed(42)\n",
    "data_train = data_train.reindex(np.random.permutation(data_train.index))\n",
    "data_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_train.drop('SalePrice',axis=1), data_train['SalePrice'], \n",
    "                                                    test_size=0.20, random_state=42)\n",
    "\n",
    "classifier = RandomForestRegressor(n_estimators=20,oob_score=True)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# The mean squared error\n",
    "pred = classifier.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "print(\"Mean squared error: {}\".format(score))\n",
    "# Evaluate success using accuracy\n",
    "print(\"Accuracy: %.3f\" % classifier.score(X=x_test,y=y_test))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "y_train = ((data_train['SalePrice']))\n",
    "x_train = (data_train.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "classifier = RandomForestRegressor(n_estimators=1000,oob_score=True)\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifier.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifier.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifier.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "print(\"\\n oob score : {}\".format(classifier.oob_score_))    \n",
    "\n",
    "# Write the cross-validated prediction\n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifier.predict(scaler.transform(data_test))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_RF_full_features.csv', columns=['SalePrice'])\n",
    "\n",
    "###########Less features\n",
    "print(\"\\n\\n Less features \\n\\n\")\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_train_less_features.drop('SalePrice',axis=1), \n",
    "                                    data_train_less_features['SalePrice'], test_size=0.20, random_state=42)\n",
    "\n",
    "classifier = RandomForestRegressor(n_estimators=20,oob_score=True)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# The mean squared error\n",
    "pred = classifier.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "print(\"Mean squared error: {}\".format(score))\n",
    "# Evaluate success using accuracy\n",
    "print(\"Accuracy: %.3f\" % classifier.score(X=x_test,y=y_test))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=7)    \n",
    "\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "pred = []\n",
    "\n",
    "classifierRandomForestRegressor = RandomForestRegressor(n_estimators=1000,oob_score=True,max_depth=3)\n",
    "\n",
    "for training, test in kf.split(x_train):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    classifierRandomForestRegressor = classifierRandomForestRegressor.fit(x_train_fold, y_train_fold)\n",
    "    pred = classifierRandomForestRegressor.predict(x_test_fold)\n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n",
    "\n",
    "    # Evaluate success using accuracy\n",
    "    print(\"Accuracy: %.3f\" % classifierRandomForestRegressor.score(X=x_test_fold,y=y_test_fold))\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "print(\"\\n oob score : {}\".format(classifierRandomForestRegressor.oob_score_))    \n",
    "\n",
    "\n",
    "# Write the cross-validated prediction\n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifierRandomForestRegressor.predict(scaler.transform(data_test_less_features))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_RF_less_features.csv', columns=['SalePrice'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 28955.769604822068\n",
      "Accuracy: 0.886\n",
      "\n",
      "\n",
      "\n",
      "Mean squared error: 31991.973659829757\n",
      "Accuracy: 0.859\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "data_train_less_features = data_train_less_features.reindex(np.random.permutation(data_train_less_features.index))\n",
    "data_train_less_features.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train,y_train,test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "classifierGBR = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, \n",
    "                                               min_samples_leaf=25, min_samples_split=20, loss='huber').fit(x_train, y_train)\n",
    "\n",
    "# The mean squared error\n",
    "pred = classifierGBR.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "print(\"Mean squared error: {}\".format(score))\n",
    "# Evaluate success using accuracy\n",
    "print(\"Accuracy: %.3f\" % classifierGBR.score(X=x_test,y=y_test))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle\n",
    "np.random.seed(42)\n",
    "data_train_less_features = data_train_less_features.reindex(np.random.permutation(data_train_less_features.index))\n",
    "data_train_less_features.reset_index(inplace=True, drop=True)\n",
    "\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train,y_train,test_size=0.20, random_state=43)\n",
    "\n",
    "classifierGBR.fit(x_train, y_train)\n",
    "\n",
    "# The mean squared error\n",
    "pred = classifierGBR.predict(x_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n",
    "print(\"Mean squared error: {}\".format(score))\n",
    "# Evaluate success using accuracy\n",
    "print(\"Accuracy: %.3f\" % classifierGBR.score(X=x_test,y=y_test))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "#save the contest result \n",
    "pred = []\n",
    "pred = np.array(pred,dtype='int64')\n",
    "pred = classifierGBR.predict(scaler.transform(data_test_less_features))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_GBR.csv', columns=['SalePrice'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 0.12.1\n",
      "Fold #1\n",
      "Fold score (RMSE): 26694.64\n",
      "Fold #2\n",
      "Fold score (RMSE): 24574.89\n",
      "Fold #3\n",
      "Fold score (RMSE): 33134.05\n",
      "Fold #4\n",
      "Fold score (RMSE): 29328.47\n",
      "Fold #5\n",
      "Fold score (RMSE): 25758.76\n",
      "\n",
      " Average RMSE: 28064.452980077105\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensor Flow Version: {}\".format(tf.__version__))\n",
    "import tensorflow.contrib.learn as learn\n",
    "import shutil \n",
    "import os\n",
    "\n",
    "# Set the desired TensorFlow output level for this example\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df,target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "\n",
    "    # find out the type of the target column.  Is it really this hard? :(\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    \n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.int32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "\n",
    "    # Get a new directory to hold checkpoints from a neural network.  This allows the neural network to be\n",
    "# loaded later.  If the erase param is set to true, the contents of the directory will be cleared.\n",
    "def get_model_dir(name,erase=False):\n",
    "    base_path = os.path.join(\".\",\"dnn\")\n",
    "    model_dir = os.path.join(base_path,name)\n",
    "    os.makedirs(model_dir,exist_ok=True)\n",
    "    if erase and len(model_dir)>4 and os.path.isdir(model_dir):\n",
    "        shutil.rmtree(model_dir,ignore_errors=True) # be careful, this deletes everything below the specified path\n",
    "    return model_dir\n",
    "\n",
    "\n",
    "#Normalization\n",
    "# y_train = ((data_train['SalePrice']))\n",
    "# x_train = (data_train.drop('SalePrice',axis=1))\n",
    "# scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "# x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "\n",
    "#Normalization\n",
    "#x_train,y_train = to_xy(data_train_less_features,\"SalePrice\")\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train_scaled = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "#Choose an optimizer\n",
    "#opt=tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "#opt=tf.train.MomentumOptimizer(learning_rate=0.001,momentum=0.9)\n",
    "\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x_train.shape[1])]\n",
    "classifierDNN = learn.DNNRegressor(hidden_units=[20, 10, 10, 5, 2], \n",
    "                                   feature_columns=feature_columns,\n",
    "                                   model_dir=get_model_dir(\"dnn\",False)\n",
    "                                   #optimizer=opt\n",
    "                                  )\n",
    "\n",
    "\n",
    "kf = KFold(5, random_state=7)    \n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "for training, test in kf.split(x_train_scaled):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train_fold = x_train_scaled[training]\n",
    "    y_train_fold = y_train[training]\n",
    "    x_test_fold = x_train_scaled[test]\n",
    "    y_test_fold = y_train[test]\n",
    "    \n",
    "    # Early stopping\n",
    "    validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    x_test_fold,\n",
    "    y_test_fold,\n",
    "    every_n_steps=500,\n",
    "    #metrics=validation_metrics,\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=400)\n",
    "        \n",
    "    classifierDNN.fit(x_train_fold, y_train_fold, monitors=[validation_monitor] ,steps=1000)\n",
    "    pred = (list(classifierDNN.predict(x_test_fold, as_iterable=True)))\n",
    "    \n",
    "    oos_y.append(y_test_fold)\n",
    "    oos_pred.append(pred)        \n",
    "\n",
    "    # Measure accuracy    \n",
    "    score = np.sqrt(metrics.mean_squared_error(y_test_fold,pred))\n",
    "    print(\"Fold score (RMSE): %.2f\" %(score))\n",
    "\n",
    "\n",
    "# Build the oos prediction list and calculate the error.\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(oos_y,oos_pred))\n",
    "print(\"\\n Average RMSE: {}\".format(score))    \n",
    "print(\"\\n\\n\")\n",
    "\n",
    "#Save the contest result\n",
    "pred = (list(classifierDNN.predict(scaler.transform(data_test_less_features),as_iterable=True)))\n",
    "#pred = list(classifierDNN.predict(scaler.transform(data_test),as_iterable=True))\n",
    "\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_DNN.csv', columns=['SalePrice'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122330.37, 156075.0, 181284.42, 183389.73, 183891.97, 185776.25, 173206.55, 166986.78, 201896.38, 117399.3, 203114.27, 91679.148, 98148.781, 124665.77, 116012.41, 361092.06, 240285.47, 325906.53, 331564.88, 409927.56, 276896.31, 207531.41, 183356.31, 160188.03, 198207.62, 203331.8, 381087.91, 239524.38, 198056.78, 243307.7, 196925.94, 96982.75, 199927.91, 310600.38, 287966.88, 234842.12, 199422.53, 157661.52, 159082.42, 152206.02, 159278.61, 148117.72, 289667.66, 226104.08, 205672.75, 161768.47, 246789.92, 214979.84, 159347.89, 141483.92, 147113.64, 163806.11, 138440.64, 160254.8, 197752.06, 146881.38, 183840.19, 128102.05, 221958.58, 128821.94, 136811.12, 161443.59, 111872.76, 110116.31, 125348.77, 127969.77, 104787.83, 133784.95, 134526.73, 190473.33, 147284.05, 113610.95, 134131.03, 122650.28, 155230.81, 110691.74, 96636.641, 146071.67, 229563.53, 133776.16, 140290.36, 139898.56, 209665.28, 75725.984, 101226.78, 143002.69, 137467.05, 116651.86, 114337.44, 117544.02, 131614.44, 153788.73, 121861.16, 104003.77, 155692.66, 100081.39, 107678.92, 101974.53, 112520.45, 138993.67, 128166.33, 130034.36, 110753.02, 140826.83, 150673.97, 245174.88, 86772.531, 208235.98, 116895.34, 138333.55, 126435.72, 148764.0, 276865.88, 134426.56, 249299.95, 244874.47, 189724.7, 127166.14, 141856.17, 193397.31, 120873.91, 126960.69, 288311.59, 226431.59, 150017.81, 80567.227, 106071.7, 144040.77, 103872.74, 125675.81, 100367.28, 102368.01, 112111.47, 193649.92, 130661.18, 228363.66, 150923.66, 194574.75, 198638.66, 170273.31, 58374.773, 117582.85, 81943.828, 298018.22, 239307.16, 156052.47, 162293.42, 215641.42, 190454.78, 175251.0, 148490.64, 185516.39, 160801.61, 133011.7, 81299.008, 81921.414, 99236.883, 127121.03, 128566.34, 225699.62, 180631.5, 140595.09, 253571.55, 195879.75, 129697.8, 152153.59, 179559.66, 311100.47, 180123.28, 341884.84, 207270.12, 232650.36, 176171.89, 182087.25, 175417.72, 152860.08, 203646.75, 204745.0, 171427.61, 245778.44, 177582.03, 225640.97, 222383.42, 222447.23, 177681.0, 145429.09, 157200.42, 128947.22, 127453.91, 123064.24, 129561.16, 94008.0, 92083.562, 151418.44, 137652.5, 142324.44, 152533.34, 138147.94, 126847.99, 138557.48, 409701.69, 360749.66, 342047.59, 427699.5, 322042.62, 336070.03, 345957.34, 342847.78, 344364.56, 340788.75, 247476.2, 400836.44, 309875.5, 230345.91, 192023.77, 194848.22, 224955.62, 420168.81, 386227.62, 304127.62, 293897.22, 358434.28, 180229.58, 183566.75, 177381.88, 163415.08, 184854.56, 191665.64, 195517.17, 187769.94, 176799.38, 272746.53, 170434.67, 174206.02, 172848.0, 256682.64, 159706.64, 320533.75, 287353.34, 271382.41, 289212.5, 244854.0, 246680.97, 264091.5, 239276.8, 429745.72, 217790.73, 192221.09, 255778.81, 215818.91, 267447.59, 244584.33, 289974.09, 213540.14, 209824.97, 171656.89, 162012.19, 141259.25, 240630.34, 241531.98, 169804.73, 132883.33, 169286.44, 211676.5, 240965.92, 190425.75, 165195.11, 184914.02, 177126.7, 174165.34, 119649.88, 132374.09, 124511.27, 135634.81, 126101.24, 111370.48, 305170.47, 268116.06, 290127.44, 193785.06, 200502.3, 159650.66, 164852.66, 256430.0, 211242.17, 224211.06, 215495.64, 254606.95, 144838.09, 150195.3, 234725.62, 122351.98, 146517.17, 206189.72, 166181.81, 112105.03, 109794.17, 135192.95, 142936.05, 159756.28, 148769.72, 209786.88, 161521.88, 110481.81, 153412.48, 155752.36, 230129.92, 122057.41, 162659.88, 131547.48, 123684.88, 137458.83, 135559.48, 132943.91, 126338.04, 119478.55, 103846.29, 142043.69, 113351.44, 170621.72, 119909.22, 96036.031, 142473.39, 78984.102, 97290.617, 126482.23, 134816.28, 63744.43, 101637.68, 74295.258, 185458.14, 143154.77, 121055.09, 145070.34, 128350.82, 147840.7, 139267.52, 114020.27, 98035.008, 115347.13, 128607.55, 134951.33, 154053.55, 133418.88, 149409.53, 117107.55, 162597.09, 120368.97, 109573.74, 137541.95, 70906.633, 93900.672, 115647.21, 86042.273, 51251.32, 76775.055, 94150.07, 157966.81, 129067.78, 84240.797, 95846.773, 147246.7, 58816.66, 126187.85, 115934.43, 97869.328, 106323.8, 118253.36, 138769.34, 129520.97, 146612.05, 127575.15, 141114.78, 121379.01, 133177.55, 128388.94, 94015.469, 123906.98, 92220.25, 153899.58, 164659.77, 96597.812, 144675.22, 145202.7, 131925.25, 156082.55, 162369.34, 47509.887, 108602.45, 117619.53, 163999.62, 110748.14, 130010.16, 181329.84, 181178.88, 206617.02, 172871.28, 151266.92, 115288.52, 155624.33, 114930.47, 330143.69, 326362.0, 326405.88, 367826.03, 382741.94, 222044.09, 272055.19, 207339.81, 236040.06, 311420.88, 175280.23, 222319.56, 146261.94, 217340.09, 195150.77, 219262.2, 200258.69, 131028.24, 131214.11, 253737.09, 239634.47, 205126.45, 217585.66, 264123.97, 327149.75, 207094.81, 261626.33, 178414.39, 128281.61, 145807.31, 101216.46, 144062.36, 133641.78, 155375.84, 131528.97, 129043.06, 118263.3, 155401.7, 138082.33, 157721.39, 144162.3, 224517.95, 134027.72, 157091.2, 161240.0, 168450.62, 110836.26, 142799.05, 139625.98, 178286.33, 302866.31, 135927.83, 73926.586, 314152.41, 43289.25, 257766.12, 147242.39, 147780.69, 178911.75, 409141.5, 356521.81, 218922.69, 223107.88, 216374.33, 387757.94, 129899.68, 150347.34, 111898.64, 129796.51, 126229.47, 172310.36, 188110.59, 177786.97, 177421.83, 189923.39, 179016.33, 169235.48, 250759.72, 178290.83, 177223.0, 183285.16, 221545.55, 394465.03, 396433.69, 182120.22, 338968.19, 180845.8, 258622.38, 170627.56, 265835.12, 223690.19, 171323.39, 199157.06, 134648.06, 310926.81, 157780.39, 267747.34, 142805.17, 104282.3, 123444.18, 96874.258, 99495.148, 116307.97, 133079.02, 155657.16, 275820.59, 389001.53, 395241.34, 357413.78, 414705.38, 350138.53, 251360.48, 344390.28, 384682.25, 266304.09, 301169.19, 309017.19, 314484.38, 206361.39, 332293.81, 221420.88, 203840.16, 173052.91, 245885.73, 222212.78, 199643.44, 171989.61, 199668.97, 215716.81, 214118.8, 214650.12, 182560.75, 243628.06, 192237.34, 298159.81, 487330.41, 293245.0, 388460.66, 315606.75, 295061.84, 271357.66, 253342.34, 250729.41, 247179.31, 200458.47, 248762.92, 202415.84, 194116.48, 196926.91, 142326.06, 173753.73, 190057.25, 186533.53, 198471.89, 200813.52, 199765.77, 124765.59, 130170.49, 122338.77, 111527.69, 187099.08, 157332.83, 283895.59, 318402.91, 178109.08, 152091.5, 164232.52, 161688.12, 260347.95, 243244.52, 256131.86, 256770.19, 157477.41, 215980.73, 177887.89, 184588.91, 297629.84, 238318.83, 322333.28, 301484.91, 210213.19, 150126.42, 183851.09, 202017.03, 122565.38, 156606.84, 120470.52, 128763.12, 178186.11, 103545.16, 127856.55, 143602.03, 73840.781, 95874.812, 145182.94, 117354.05, 207098.59, 146799.03, 165272.47, 163798.98, 131395.8, 115290.13, 135864.52, 122279.03, 167080.47, 115124.78, 129507.3, 77155.492, 98907.414, 76844.703, 146325.38, 139693.59, 178109.25, 141954.62, 114447.28, 130862.12, 120582.86, 129751.1, 106146.09, 130557.93, 141144.16, 133004.92, 102730.96, 120007.75, 142576.69, 127706.76, 101723.41, 81836.875, 112084.37, 112466.2, 116971.24, 59042.734, 134640.66, 158181.84, 82586.68, 88842.469, 152777.58, 51784.367, 112441.41, 131195.59, 121387.57, 98761.578, 133269.83, 92402.625, 68600.664, 217460.67, 108215.8, 113585.19, 114718.86, 129283.0, 122194.84, 121180.35, 110221.58, 139013.52, 113278.69, 150141.14, 114597.8, 103300.12, 122084.97, 79544.938, 86732.102, 120867.48, 154155.61, 115289.3, 132813.56, 152542.66, 115127.63, 89215.789, 135171.59, 135949.55, 116961.09, 118818.66, 118404.16, 110442.95, 70877.742, 103917.59, 139668.03, 151684.03, 145084.31, 149019.28, 122479.95, 131827.53, 139738.17, 139132.62, 167714.91, 152925.86, 127371.31, 153345.88, 210353.8, 120019.16, 184245.14, 159865.16, 114985.59, 129585.54, 282195.19, 223010.59, 231908.56, 225545.27, 187831.08, 238641.73, 282329.97, 353649.09, 255728.95, 193100.53, 147352.27, 196943.0, 211537.06, 205860.22, 221607.59, 144587.34, 133107.09, 151566.27, 232620.33, 251473.84, 338041.5, 236062.92, 222075.86, 145151.61, 238785.47, 188844.75, 218672.77, 190575.27, 128425.07, 147882.95, 141276.88, 142450.55, 160937.09, 341338.47, 98692.375, 98828.18, 95514.562, 109250.51, 103731.7, 108439.38, 85818.367, 109293.31, 134948.44, 177042.66, 122307.37, 144684.05, 176427.09, 156099.27, 151834.08, 108587.36, 139425.53, 208051.62, 243649.69, 190400.97, 126302.3, 120131.72, 118031.83, 97392.281, 106961.67, 102482.47, 159968.66, 51784.531, 82431.695, 82159.156, 81061.859, 312458.16, 301562.19, 309784.56, 223213.25, 137021.38, 161582.14, 187395.25, 345104.5, 242992.14, 150570.78, 212261.61, 182271.16, 189503.12, 258765.91, 230298.47, 252219.03, 298139.41, 257182.67, 127206.94, 162475.12, 157659.44, 139651.56, 123879.93, 102261.2, 125776.86, 149519.44, 119445.42, 128340.27, 136568.11, 143609.97, 140700.16, 169846.47, 157634.45, 173159.06, 189999.19, 191570.39, 267699.5, 168900.28, 174180.55, 161712.84, 181584.17, 220203.89, 411295.28, 344527.0, 199577.17, 338152.69, 341759.84, 407795.09, 168050.91, 192024.27, 225428.5, 189684.62, 173150.91, 187701.3, 174480.25, 180757.36, 180099.03, 148463.06, 103454.95, 106565.06, 160810.23, 201524.33, 107273.68, 99071.172, 146434.78, 122149.26, 372951.53, 264514.53, 313107.06, 348864.69, 380257.62, 398788.0, 386295.81, 357744.31, 377731.03, 249096.73, 365774.88, 357149.28, 321429.56, 291836.38, 329251.44, 244369.7, 223735.34, 226251.78, 200155.38, 204560.31, 188480.12, 212496.84, 304932.5, 202313.73, 194424.14, 219427.38, 186173.17, 201475.72, 179728.59, 211806.39, 199344.41, 179624.98, 180784.92, 174636.55, 269743.19, 173227.62, 233655.88, 202769.23, 203458.33, 195111.62, 210935.66, 219534.52, 185787.98, 177236.44, 293589.22, 455791.94, 315406.22, 297636.66, 379655.0, 305190.59, 194136.69, 254540.45, 223180.19, 375843.75, 206403.75, 224132.75, 213080.17, 229882.23, 222214.3, 198268.64, 185950.69, 221947.14, 218431.17, 327791.22, 262814.91, 245988.52, 249727.39, 135837.31, 124639.59, 153140.31, 190483.62, 198284.89, 146834.62, 113300.16, 139170.92, 280940.78, 146841.41, 187591.61, 211931.06, 175976.48, 201460.25, 212395.88, 209040.48, 164624.91, 159680.97, 167619.69, 238300.45, 263198.84, 246191.94, 268969.91, 313371.09, 134894.86, 215070.5, 148023.06, 156953.44, 201652.09, 208088.58, 227678.27, 142307.67, 137317.33, 137590.02, 135617.28, 122655.84, 142482.45, 136261.2, 108868.74, 148325.39, 149175.41, 222654.83, 151656.19, 228998.89, 125383.66, 76542.969, 57624.543, 112168.97, 122397.09, 119349.71, 121577.15, 148868.39, 153119.56, 122200.78, 134919.45, 131948.83, 196268.28, 119638.71, 160797.77, 126883.48, 127668.26, 113386.77, 123790.35, 116937.8, 121981.61, 135286.59, 134670.84, 139233.0, 116715.0, 99119.695, 150558.98, 271237.62, 146084.44, 108605.66, 175737.11, 113532.52, 139523.09, 129744.11, 137434.12, 139189.47, 130564.75, 155566.78, 99476.898, 103187.34, 134843.72, 97331.273, 124777.54, 112198.87, 101246.15, 111605.19, 139293.34, 83852.938, 92964.383, 173368.27, 160213.89, 103055.6, 153586.8, 123809.98, 220990.91, 88789.844, 111286.97, 119467.05, 125573.52, 114407.37, 147636.41, 78093.094, 138991.91, 111219.14, 120145.48, 109440.44, 159211.97, 123633.07, 109386.66, 158268.5, 89162.906, 88612.977, 191314.03, 197821.78, 197149.91, 113477.61, 68912.211, 233693.77, 105287.92, 130182.61, 144171.08, 125470.42, 162525.83, 122765.45, 144943.23, 115980.51, 118562.62, 116212.66, 156037.73, 251347.89, 174751.77, 178790.86, 154896.55, 66466.641, 168475.17, 155335.42, 143983.31, 114177.56, 227897.95, 140585.56, 114735.47, 106409.08, 121681.48, 132771.19, 158650.8, 85797.617, 197794.0, 222258.55, 239732.22, 266376.22, 242997.89, 221836.09, 216073.05, 185076.22, 228372.94, 215120.39, 284899.91, 159711.88, 176313.17, 132557.72, 143251.33, 253197.25, 215564.09, 197395.33, 216051.47, 133540.34, 125499.16, 127743.7, 142719.19, 121246.15, 127436.77, 127198.01, 124501.55, 262234.44, 228941.59, 201842.16, 250913.08, 273808.97, 227219.11, 273353.38, 189896.16, 200760.06, 170574.7, 190950.05, 171257.17, 143162.14, 114918.45, 143105.72, 131536.75, 141664.19, 149397.19, 189232.0, 570935.44, 145641.7, 122868.47, 91636.289, 105995.45, 119378.85, 103416.22, 116269.31, 147686.17, 117134.62, 140925.8, 122285.33, 125537.16, 141862.72, 174751.08, 131222.8, 122759.02, 119028.4, 185247.77, 197289.28, 126262.05, 180434.12, 157042.34, 220527.52, 276690.09, 131965.73, 103822.61, 132446.67, 82745.82, 53115.977, 125771.4, 138729.89, 119508.7, 248949.34, 169516.09, 177635.39, 264632.66, 200976.47, 137731.83, 136384.17, 192346.09, 222782.7, 202561.89, 265863.34, 176889.06, 228033.64, 286300.75, 175532.95, 317855.41, 376773.53, 174746.14, 143932.86, 96645.531, 89345.906, 82451.508, 97170.562, 133415.52, 192908.92, 224019.12, 165823.19, 110358.56, 155193.7, 159645.84, 125811.63, 132791.7, 142665.56, 150689.05, 184622.12, 198067.42, 199540.14, 178530.55, 186426.03, 195064.34, 246442.14, 231517.17, 260371.7, 155107.06, 164132.23, 465793.94, 448460.59, 359198.06, 370055.38, 432914.0, 346085.5, 397036.53, 141643.33, 195649.47, 233235.2, 267323.81, 206480.89, 143861.8, 115510.63, 206878.62, 113069.65, 123023.71, 103434.43, 94559.516, 104616.7, 124281.52, 136145.7, 145837.17, 138575.36, 359610.81, 254958.52, 241282.61, 380799.94, 320918.88, 308518.78, 315599.38, 333758.56, 343595.72, 356922.5, 391167.28, 243504.8, 294995.09, 349750.22, 264241.84, 172930.66, 186047.67, 172623.22, 268601.12, 185705.94, 180475.8, 193929.19, 204161.28, 184151.08, 195155.03, 195545.28, 290024.44, 296794.22, 298795.84, 518470.19, 320102.69, 598167.75, 328692.47, 330187.38, 237700.55, 307786.94, 210311.62, 202586.56, 405072.03, 198546.44, 146746.67, 194049.88, 139728.11, 194143.52, 187711.92, 207452.38, 194111.61, 189207.02, 162858.88, 169655.3, 128341.42, 140568.45, 133711.66, 125307.28, 116905.29, 116604.98, 136914.3, 110948.68, 139438.7, 320276.97, 428979.84, 179266.28, 152692.8, 156752.11, 148409.62, 187889.69, 206735.31, 155643.67, 148898.09, 139474.61, 172683.66, 158436.95, 134291.41, 132596.62, 137168.08, 193597.28, 152217.75, 153482.0, 132825.25, 134079.59, 114120.02, 160370.61, 147023.45, 136606.06, 163445.31, 127061.79, 120507.78, 166542.2, 122420.35, 143410.84, 147250.08, 133337.62, 160282.16, 151766.36, 147254.86, 142082.45, 134452.06, 142646.53, 118569.93, 144520.45, 246017.61, 156563.14, 237166.81, 128815.8, 87404.68, 83925.758, 90780.727, 139859.5, 148033.94, 150645.72, 147141.08, 191993.58, 169816.91, 268732.97, 131110.2, 88991.273, 132166.83, 126804.95, 148750.27, 114383.78, 117879.86, 132618.75, 128333.42, 144615.58, 136735.84, 122238.97, 104527.85, 100739.79, 102580.63, 87512.078, 89319.922, 79431.773, 122452.57, 106628.61, 62880.262, 119469.64, 76227.086, 149489.72, 114192.16, 123036.74, 83505.578, 171317.05, 72179.711, 110491.11, 91491.664, 215093.09, 98073.422, 92597.219, 87282.055, 113198.68, 130493.8, 150748.14, 146505.5, 102217.83, 85914.969, 159802.34, 145874.12, 132335.73, 130291.74, 147233.22, 152578.81, 152861.48, 162379.55, 122459.91, 220173.02, 120915.56, 123873.49, 155987.94, 134576.67, 125315.56, 195692.45, 340989.66, 167452.98, 134666.5, 129316.75, 151983.28, 253692.3, 224964.56, 218297.91, 193943.92, 243170.22, 347324.59, 239247.38, 230247.88, 201532.88, 169775.12, 144178.73, 177127.47, 207061.41, 207031.53, 218324.47, 156787.33, 136286.89, 115627.95, 214203.69, 200193.83, 257860.06, 207791.48, 266366.59, 225649.58, 229789.81, 226330.73, 150713.02, 207774.64, 207766.95, 200715.42, 204347.88, 123338.04, 121935.37, 138403.7, 208295.84, 131573.5, 257416.05, 145807.31, 151998.69, 90893.688, 115253.62, 116387.42, 126246.97, 117421.01, 45377.832, 108828.34, 133808.91, 124982.02, 175381.98, 107214.76, 168840.94, 129094.28, 114559.66, 143269.97, 135659.44, 137470.06, 224880.48, 168723.19, 192498.89, 99255.938, 126112.27, 69131.812, 71894.773, 147365.58, 65655.023, 87212.148, 58395.551, 321767.69, 301845.47, 214657.86, 138985.98, 228165.81, 145811.92, 240922.75, 201525.41, 309915.25, 334080.38, 94016.242, 252824.78, 127446.45, 134381.84, 132422.0, 76691.359, 96675.523, 154747.84, 93229.391, 82179.586, 82030.938, 91033.453, 169580.39, 121068.43, 231063.25]\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Voting Ensemble for Classification\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "y_train = ((data_train_less_features['SalePrice']))\n",
    "x_train = (data_train_less_features.drop('SalePrice',axis=1))\n",
    "scaler = preprocessing.StandardScaler().fit((x_train))\n",
    "x_train = scaler.transform((x_train))\n",
    "\n",
    "\n",
    "# estimators = []\n",
    "# estimators.append(('linear',classifierLinearRegression))\n",
    "# estimators.append(('svr',classifierSVR))\n",
    "# estimators.append(('rf',classifierRandomForestRegressor))\n",
    "\n",
    "# # # create the ensemble model\n",
    "# ensemble = VotingClassifier(estimators,voting='hard')\n",
    "# ensemble = ensemble.fit(x_train,y_train)\n",
    "# pred = []\n",
    "# pred = np.array(pred,dtype='float64')\n",
    "# pred = ensemble.predict(scaler.transform(data_test_less_features))\n",
    "# result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "# result.to_csv('pred_EnsembleVoting.csv', columns=['SalePrice'])\n",
    "\n",
    "\n",
    "#Emsemble via stacking\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "stregr = StackingRegressor(regressors=[classifierLinearRegression,classifierSVR, classifierRandomForestRegressor,classifierGBR], \n",
    "                           meta_regressor=classifierLinearRegression)\n",
    "stregr.fit(x_train,y_train)\n",
    "pred = []\n",
    "pred = np.array(pred)\n",
    "pred = stregr.predict(scaler.transform(data_test_less_features))\n",
    "result = pd.DataFrame(pred,columns=['SalePrice'], index=range(1461,2920))\n",
    "result.to_csv('pred_EnsembleStacker.csv', columns=['SalePrice'])\n",
    "\n",
    "#Emsemble via avereging\n",
    "dnn=[]\n",
    "dnn = list(classifierDNN.predict(scaler.transform(data_test_less_features),as_iterable=True))\n",
    "print(dnn)\n",
    "final_labels = (\n",
    "                (classifierLinearRegression.predict(scaler.transform(data_test_less_features))) + \n",
    "                (classifierSVR.predict(scaler.transform(data_test_less_features))) +\n",
    "                (classifierRandomForestRegressor.predict(scaler.transform(data_test_less_features)))+\n",
    "                (classifierGBR.predict(scaler.transform(data_test_less_features))) +\n",
    "                dnn\n",
    "               \n",
    "               ) / 5\n",
    "\n",
    "## Saving to CSV\n",
    "pd.DataFrame({'Id': range(1461,2920), 'SalePrice': final_labels}).to_csv('pred_EnsembleAvereging.csv', index =False)  \n",
    "\n",
    "print(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
